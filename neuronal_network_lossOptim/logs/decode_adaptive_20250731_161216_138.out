==================== ä½œä¸šä¿¡æ¯ ====================
ä½œä¸šID: 138
ä½œä¸šåç§°: decode_adaptive_20250731_161216
èŠ‚ç‚¹: master
GPU: 0
CPUæ ¸å¿ƒæ•°: 16
å†…å­˜: 32768 MB
å¼€å§‹æ—¶é—´: 2025å¹´ 07æœˆ 31æ—¥ æ˜ŸæœŸå›› 16:12:16 CST
å·¥ä½œç›®å½•: /home/guest/Others/DECODE_rewrite/neuronal_network_lossOptim
é…ç½®æ–‡ä»¶: training/configs/train_config_adaptive.json
è®¾å¤‡: cuda
æ¢å¤è®­ç»ƒ: false
ä»…ç›‘æ§æ¨¡å¼: false
=================================================
\n==================== ç³»ç»Ÿæ£€æŸ¥ ====================
ä¸»æœºå: master
æ“ä½œç³»ç»Ÿ: Linux master 5.15.0-107-generic #117~20.04.1-Ubuntu SMP Tue Apr 30 10:35:57 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
ç£ç›˜ç©ºé—´:
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda4       409G  140G  249G  36% /
\nGPUçŠ¶æ€:
0, NVIDIA GeForce RTX 3090, 24576, 12, 0
1, NVIDIA GeForce RTX 3090, 24576, 11, 0
2, NVIDIA GeForce RTX 3090, 24576, 11, 0
3, NVIDIA GeForce RTX 3090, 24576, 11, 0
\nPythonç¯å¢ƒ:
Pythonç‰ˆæœ¬: Python 3.11.7
PyTorchç‰ˆæœ¬: 2.7.1+cu118
CUDAå¯ç”¨æ€§: True
GPUæ•°é‡: 1
=================================================
\n==================== å¼€å§‹è®­ç»ƒ ====================
æ‰§è¡Œå‘½ä»¤: python start_adaptive_training.py --config training/configs/train_config_adaptive.json
=================================================
ğŸ¯ DECODEè‡ªé€‚åº”è®­ç»ƒå¯åŠ¨å™¨
============================================================
å¯åŠ¨æ—¶é—´: 2025-07-31 16:12:23
ğŸ” æ£€æŸ¥è¿è¡Œç¯å¢ƒ...
  Pythonç‰ˆæœ¬: 3.11.7
  PyTorchç‰ˆæœ¬: 2.7.1+cu118
  CUDAå¯ç”¨: True
  GPUæ•°é‡: 1
  å½“å‰GPU: NVIDIA GeForce RTX 3090
  âœ… numpy
  âœ… matplotlib

ğŸ“ æ£€æŸ¥æ•°æ®å¯ç”¨æ€§...
  æ•°æ®ç›®å½•: /home/guest/Others/DECODE_rewrite/simulation_zmap2tiff/outputs_100samples_40
  æ ·æœ¬æ•°é‡: 100
  âœ… æ•°æ®å……è¶³

ğŸ“‹ ä½¿ç”¨é…ç½®: training/configs/train_config_adaptive.json

ğŸ“‚ è¾“å‡ºç›®å½•: outputs/training_results_adaptive

ğŸš€ å¯åŠ¨è‡ªé€‚åº”è®­ç»ƒ...
============================================================
ä½¿ç”¨è®¾å¤‡: cuda
æ¨¡å‹æ€»å‚æ•°é‡: 26,273,203
æŸå¤±å‡½æ•°åˆå§‹åŒ–å®Œæˆ: unified_decode
ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆï¼Œåˆå§‹å­¦ä¹ ç‡: 0.0005
è®­ç»ƒæ ·æœ¬: 80, éªŒè¯æ ·æœ¬: 20
æ‰¾åˆ° 80 ä¸ªæœ‰æ•ˆæ ·æœ¬
æ€»å…± 80 ä¸ªè®­ç»ƒæ ·æœ¬
æ‰¾åˆ° 20 ä¸ªæœ‰æ•ˆæ ·æœ¬
æ€»å…± 20 ä¸ªè®­ç»ƒæ ·æœ¬
æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ
è®­ç»ƒæ ·æœ¬: 80, éªŒè¯æ ·æœ¬: 20
TensorBoardæ—¥å¿—ç›®å½•: outputs/training_results_adaptive/tensorboard
å¼€å§‹è‡ªé€‚åº”è®­ç»ƒ...
Epoch 0, Batch 0/5, Loss: 48018.128906, LR: 5.00e-04
Warmupé˜¶æ®µ - Epoch 0: LR 5.00e-04 -> 5.90e-05
ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: 51035.052734
Epoch    0/1000: Train Loss: 52114.857031, Val Loss: 51035.052734, LR: 1.00e-05, Best: 51035.052734
Epoch 1, Batch 0/5, Loss: 48536.054688, LR: 1.00e-05
Warmupé˜¶æ®µ - Epoch 1: LR 1.00e-05 -> 1.08e-04
Epoch    1/1000: Train Loss: 51977.471094, Val Loss: 51047.724609, LR: 5.90e-05, Best: 51035.052734
Epoch 2, Batch 0/5, Loss: 43038.675781, LR: 5.90e-05
Warmupé˜¶æ®µ - Epoch 2: LR 5.90e-05 -> 1.57e-04
Epoch    2/1000: Train Loss: 51939.800781, Val Loss: 51044.371094, LR: 1.08e-04, Best: 51035.052734
Epoch 3, Batch 0/5, Loss: 57298.183594, LR: 1.08e-04
Warmupé˜¶æ®µ - Epoch 3: LR 1.08e-04 -> 2.06e-04
Epoch    3/1000: Train Loss: 51829.121094, Val Loss: 51041.759766, LR: 1.57e-04, Best: 51035.052734
Epoch 4, Batch 0/5, Loss: 41684.691406, LR: 1.57e-04
Warmupé˜¶æ®µ - Epoch 4: LR 1.57e-04 -> 2.55e-04
ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: 50990.025391
Epoch    4/1000: Train Loss: 51710.948438, Val Loss: 50990.025391, LR: 2.06e-04, Best: 50990.025391
Epoch 5, Batch 0/5, Loss: 42345.691406, LR: 2.06e-04
Warmupé˜¶æ®µ - Epoch 5: LR 2.06e-04 -> 3.04e-04
ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: 50482.486328
Epoch    5/1000: Train Loss: 51406.996875, Val Loss: 50482.486328, LR: 2.55e-04, Best: 50482.486328
Epoch 6, Batch 0/5, Loss: 48258.625000, LR: 2.55e-04
Warmupé˜¶æ®µ - Epoch 6: LR 2.55e-04 -> 3.53e-04
ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: 49012.398438
Epoch    6/1000: Train Loss: 50797.963281, Val Loss: 49012.398438, LR: 3.04e-04, Best: 49012.398438
Epoch 7, Batch 0/5, Loss: 52114.007812, LR: 3.04e-04
Warmupé˜¶æ®µ - Epoch 7: LR 3.04e-04 -> 4.02e-04
ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: 47324.867188
Epoch    7/1000: Train Loss: 49904.230469, Val Loss: 47324.867188, LR: 3.53e-04, Best: 47324.867188
Epoch 8, Batch 0/5, Loss: 66256.398438, LR: 3.53e-04
Warmupé˜¶æ®µ - Epoch 8: LR 3.53e-04 -> 4.51e-04
ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: 45687.806641
Epoch    8/1000: Train Loss: 48814.280469, Val Loss: 45687.806641, LR: 4.02e-04, Best: 45687.806641
Epoch 9, Batch 0/5, Loss: 45779.523438, LR: 4.02e-04
Warmupé˜¶æ®µ - Epoch 9: LR 4.02e-04 -> 5.00e-04
ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: 45571.269531
Epoch    9/1000: Train Loss: 47883.637500, Val Loss: 45571.269531, LR: 4.51e-04, Best: 45571.269531
Epoch 10, Batch 0/5, Loss: 52700.464844, LR: 4.51e-04
Epoch   10/1000: Train Loss: 47283.744531, Val Loss: 47440.195312, LR: 4.51e-04, Best: 45571.269531
Epoch 11, Batch 0/5, Loss: 56919.324219, LR: 4.51e-04
Epoch   11/1000: Train Loss: 46802.597656, Val Loss: 48358.228516, LR: 4.51e-04, Best: 45571.269531
Epoch 12, Batch 0/5, Loss: 44796.941406, LR: 4.51e-04
Epoch   12/1000: Train Loss: 46233.661719, Val Loss: 47793.996094, LR: 4.51e-04, Best: 45571.269531
Epoch 13, Batch 0/5, Loss: 28197.068359, LR: 4.51e-04
Epoch   13/1000: Train Loss: 45813.324609, Val Loss: 49594.458984, LR: 4.51e-04, Best: 45571.269531
Epoch 14, Batch 0/5, Loss: 48377.929688, LR: 4.51e-04
Epoch   14/1000: Train Loss: 45330.748828, Val Loss: 48666.541016, LR: 4.51e-04, Best: 45571.269531
Epoch 15, Batch 0/5, Loss: 46332.492188, LR: 4.51e-04
Epoch   15/1000: Train Loss: 45123.929688, Val Loss: 47497.406250, LR: 4.51e-04, Best: 45571.269531
Epoch 16, Batch 0/5, Loss: 38957.632812, LR: 4.51e-04
Epoch   16/1000: Train Loss: 45013.625781, Val Loss: 48864.781250, LR: 4.51e-04, Best: 45571.269531
Epoch 17, Batch 0/5, Loss: 48363.648438, LR: 4.51e-04
Epoch   17/1000: Train Loss: 44856.936328, Val Loss: 48846.335938, LR: 4.51e-04, Best: 45571.269531
Epoch 18, Batch 0/5, Loss: 37226.015625, LR: 4.51e-04
Epoch   18/1000: Train Loss: 44289.292188, Val Loss: 50047.343750, LR: 4.51e-04, Best: 45571.269531
Epoch 19, Batch 0/5, Loss: 35893.789062, LR: 4.51e-04
Epoch   19/1000: Train Loss: 44255.391406, Val Loss: 49381.992188, LR: 4.51e-04, Best: 45571.269531
Epoch 20, Batch 0/5, Loss: 34773.781250, LR: 4.51e-04
Epoch   20/1000: Train Loss: 43822.135156, Val Loss: 48640.091797, LR: 4.51e-04, Best: 45571.269531
Epoch 21, Batch 0/5, Loss: 49620.742188, LR: 4.51e-04
Epoch   21/1000: Train Loss: 43311.677344, Val Loss: 49315.619141, LR: 4.51e-04, Best: 45571.269531
Epoch 22, Batch 0/5, Loss: 32745.341797, LR: 4.51e-04
Epoch   22/1000: Train Loss: 43121.139453, Val Loss: 48455.386719, LR: 4.51e-04, Best: 45571.269531
Epoch 23, Batch 0/5, Loss: 42050.808594, LR: 4.51e-04
Epoch   23/1000: Train Loss: 42871.470703, Val Loss: 48454.554688, LR: 4.51e-04, Best: 45571.269531
Epoch 24, Batch 0/5, Loss: 48176.531250, LR: 4.51e-04
Epoch   24/1000: Train Loss: 42546.069531, Val Loss: 48712.308594, LR: 4.51e-04, Best: 45571.269531
Epoch 25, Batch 0/5, Loss: 44874.609375, LR: 4.51e-04
Epoch   25/1000: Train Loss: 42360.415234, Val Loss: 47610.048828, LR: 4.51e-04, Best: 45571.269531
Epoch 26, Batch 0/5, Loss: 42861.203125, LR: 4.51e-04
Epoch   26/1000: Train Loss: 42081.664453, Val Loss: 48860.167969, LR: 4.51e-04, Best: 45571.269531
Epoch 27, Batch 0/5, Loss: 47316.136719, LR: 4.51e-04
Epoch   27/1000: Train Loss: 41980.201172, Val Loss: 48829.539062, LR: 4.51e-04, Best: 45571.269531
Epoch 28, Batch 0/5, Loss: 44799.734375, LR: 4.51e-04
Epoch   28/1000: Train Loss: 41395.685156, Val Loss: 49409.224609, LR: 4.51e-04, Best: 45571.269531
Epoch 29, Batch 0/5, Loss: 37293.199219, LR: 4.51e-04
Epoch   29/1000: Train Loss: 41214.605469, Val Loss: 48893.443359, LR: 4.51e-04, Best: 45571.269531
Epoch 30, Batch 0/5, Loss: 35289.906250, LR: 4.51e-04
Epoch   30/1000: Train Loss: 40804.704688, Val Loss: 49798.205078, LR: 4.51e-04, Best: 45571.269531
Epoch 31, Batch 0/5, Loss: 46661.511719, LR: 4.51e-04
Epoch   31/1000: Train Loss: 40364.759766, Val Loss: 49332.660156, LR: 4.51e-04, Best: 45571.269531
Epoch 32, Batch 0/5, Loss: 49077.824219, LR: 4.51e-04
Epoch   32/1000: Train Loss: 39894.763281, Val Loss: 49857.916016, LR: 4.51e-04, Best: 45571.269531
Epoch 33, Batch 0/5, Loss: 23715.019531, LR: 4.51e-04
Epoch   33/1000: Train Loss: 39316.399219, Val Loss: 49077.652344, LR: 4.51e-04, Best: 45571.269531
Epoch 34, Batch 0/5, Loss: 60210.417969, LR: 4.51e-04
Epoch   34/1000: Train Loss: 38820.630078, Val Loss: 49036.626953, LR: 4.51e-04, Best: 45571.269531
Epoch 35, Batch 0/5, Loss: 21933.488281, LR: 4.51e-04
Epoch   35/1000: Train Loss: 38558.286328, Val Loss: 48895.722656, LR: 4.51e-04, Best: 45571.269531
Epoch 36, Batch 0/5, Loss: 41111.406250, LR: 4.51e-04
Epoch   36/1000: Train Loss: 38166.560937, Val Loss: 48896.496094, LR: 4.51e-04, Best: 45571.269531
Epoch 37, Batch 0/5, Loss: 51399.144531, LR: 4.51e-04
Epoch   37/1000: Train Loss: 38176.494531, Val Loss: 48597.634766, LR: 4.51e-04, Best: 45571.269531
Epoch 38, Batch 0/5, Loss: 38781.449219, LR: 4.51e-04
Epoch   38/1000: Train Loss: 37755.623437, Val Loss: 48847.824219, LR: 4.51e-04, Best: 45571.269531
Epoch 39, Batch 0/5, Loss: 31458.939453, LR: 4.51e-04
Epoch   39/1000: Train Loss: 37606.071484, Val Loss: 49153.199219, LR: 4.51e-04, Best: 45571.269531
Epoch 40, Batch 0/5, Loss: 49304.011719, LR: 4.51e-04
Epoch   40/1000: Train Loss: 37452.911719, Val Loss: 48606.451172, LR: 4.51e-04, Best: 45571.269531
Epoch 41, Batch 0/5, Loss: 41001.031250, LR: 4.51e-04
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 4.51e-04 -> 3.61e-04 (factor: 0.8)
Epoch   41/1000: Train Loss: 37417.042969, Val Loss: 49051.044922, LR: 3.61e-04, Best: 45571.269531
Epoch 42, Batch 0/5, Loss: 36727.390625, LR: 3.61e-04
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 3.61e-04 -> 2.89e-04 (factor: 0.8)
Epoch   42/1000: Train Loss: 37329.428906, Val Loss: 48683.380859, LR: 2.89e-04, Best: 45571.269531
Epoch 43, Batch 0/5, Loss: 36394.617188, LR: 2.89e-04
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 2.89e-04 -> 2.31e-04 (factor: 0.8)
Epoch   43/1000: Train Loss: 37036.528125, Val Loss: 49113.353516, LR: 2.31e-04, Best: 45571.269531
Epoch 44, Batch 0/5, Loss: 30841.496094, LR: 2.31e-04
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 2.31e-04 -> 1.85e-04 (factor: 0.8)
Epoch   44/1000: Train Loss: 36965.998047, Val Loss: 48590.960938, LR: 1.85e-04, Best: 45571.269531
Epoch 45, Batch 0/5, Loss: 43830.558594, LR: 1.85e-04
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.85e-04 -> 1.48e-04 (factor: 0.8)
Epoch   45/1000: Train Loss: 36849.736328, Val Loss: 48692.751953, LR: 1.48e-04, Best: 45571.269531
Epoch 46, Batch 0/5, Loss: 24295.800781, LR: 1.48e-04
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.48e-04 -> 1.18e-04 (factor: 0.8)
Epoch   46/1000: Train Loss: 36745.223828, Val Loss: 49004.078125, LR: 1.18e-04, Best: 45571.269531
Epoch 47, Batch 0/5, Loss: 30369.203125, LR: 1.18e-04
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.18e-04 -> 9.46e-05 (factor: 0.8)
Epoch   47/1000: Train Loss: 36689.719531, Val Loss: 48732.488281, LR: 9.46e-05, Best: 45571.269531
Epoch 48, Batch 0/5, Loss: 33439.304688, LR: 9.46e-05
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 9.46e-05 -> 7.57e-05 (factor: 0.8)
Epoch   48/1000: Train Loss: 36674.578906, Val Loss: 48771.964844, LR: 7.57e-05, Best: 45571.269531
Epoch 49, Batch 0/5, Loss: 21786.039062, LR: 7.57e-05
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 7.57e-05 -> 6.05e-05 (factor: 0.8)
Epoch   49/1000: Train Loss: 36639.789453, Val Loss: 48803.369141, LR: 6.05e-05, Best: 45571.269531
Epoch 50, Batch 0/5, Loss: 44611.046875, LR: 6.05e-05
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 6.05e-05 -> 4.84e-05 (factor: 0.8)
Epoch   50/1000: Train Loss: 36583.615625, Val Loss: 48827.525391, LR: 4.84e-05, Best: 45571.269531
Epoch 51, Batch 0/5, Loss: 49759.039062, LR: 4.84e-05
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 4.84e-05 -> 3.87e-05 (factor: 0.8)
Epoch   51/1000: Train Loss: 36654.821094, Val Loss: 48793.003906, LR: 3.87e-05, Best: 45571.269531
Epoch 52, Batch 0/5, Loss: 41559.726562, LR: 3.87e-05
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 3.87e-05 -> 3.10e-05 (factor: 0.8)
Epoch   52/1000: Train Loss: 36580.328516, Val Loss: 48790.515625, LR: 3.10e-05, Best: 45571.269531
Epoch 53, Batch 0/5, Loss: 55026.199219, LR: 3.10e-05
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 3.10e-05 -> 2.48e-05 (factor: 0.8)
Epoch   53/1000: Train Loss: 36591.365234, Val Loss: 48830.570312, LR: 2.48e-05, Best: 45571.269531
Epoch 54, Batch 0/5, Loss: 45315.226562, LR: 2.48e-05
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 2.48e-05 -> 1.98e-05 (factor: 0.8)
Epoch   54/1000: Train Loss: 36526.698047, Val Loss: 48825.474609, LR: 1.98e-05, Best: 45571.269531
Epoch 55, Batch 0/5, Loss: 42348.925781, LR: 1.98e-05
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.98e-05 -> 1.59e-05 (factor: 0.8)
Epoch   55/1000: Train Loss: 36538.716797, Val Loss: 48831.439453, LR: 1.59e-05, Best: 45571.269531
Epoch 56, Batch 0/5, Loss: 43067.476562, LR: 1.59e-05
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.59e-05 -> 1.27e-05 (factor: 0.8)
Epoch   56/1000: Train Loss: 36499.704688, Val Loss: 48843.876953, LR: 1.27e-05, Best: 45571.269531
Epoch 57, Batch 0/5, Loss: 25558.916016, LR: 1.27e-05
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.27e-05 -> 1.02e-05 (factor: 0.8)
Epoch   57/1000: Train Loss: 36484.113672, Val Loss: 48850.939453, LR: 1.02e-05, Best: 45571.269531
Epoch 58, Batch 0/5, Loss: 40171.191406, LR: 1.02e-05
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.02e-05 -> 8.12e-06 (factor: 0.8)
Epoch   58/1000: Train Loss: 36531.603516, Val Loss: 48823.054688, LR: 8.12e-06, Best: 45571.269531
Epoch 59, Batch 0/5, Loss: 37852.449219, LR: 8.12e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 8.12e-06 -> 6.50e-06 (factor: 0.8)
Epoch   59/1000: Train Loss: 36480.257031, Val Loss: 48834.822266, LR: 6.50e-06, Best: 45571.269531
Epoch 60, Batch 0/5, Loss: 33952.164062, LR: 6.50e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 6.50e-06 -> 5.20e-06 (factor: 0.8)
Epoch   60/1000: Train Loss: 36416.825000, Val Loss: 48841.363281, LR: 5.20e-06, Best: 45571.269531
Epoch 61, Batch 0/5, Loss: 39916.851562, LR: 5.20e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 5.20e-06 -> 4.16e-06 (factor: 0.8)
Epoch   61/1000: Train Loss: 36451.424219, Val Loss: 48843.716797, LR: 4.16e-06, Best: 45571.269531
Epoch 62, Batch 0/5, Loss: 36402.894531, LR: 4.16e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 4.16e-06 -> 3.33e-06 (factor: 0.8)
Epoch   62/1000: Train Loss: 36500.432422, Val Loss: 48830.822266, LR: 3.33e-06, Best: 45571.269531
Epoch 63, Batch 0/5, Loss: 35696.449219, LR: 3.33e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 3.33e-06 -> 2.66e-06 (factor: 0.8)
Epoch   63/1000: Train Loss: 36425.697266, Val Loss: 48832.148438, LR: 2.66e-06, Best: 45571.269531
Epoch 64, Batch 0/5, Loss: 31988.841797, LR: 2.66e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 2.66e-06 -> 2.13e-06 (factor: 0.8)
Epoch   64/1000: Train Loss: 36417.658984, Val Loss: 48835.777344, LR: 2.13e-06, Best: 45571.269531
Epoch 65, Batch 0/5, Loss: 33725.652344, LR: 2.13e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 2.13e-06 -> 1.70e-06 (factor: 0.8)
Epoch   65/1000: Train Loss: 36450.965234, Val Loss: 48830.367188, LR: 1.70e-06, Best: 45571.269531
Epoch 66, Batch 0/5, Loss: 38921.457031, LR: 1.70e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.70e-06 -> 1.36e-06 (factor: 0.8)
Epoch   66/1000: Train Loss: 36435.562500, Val Loss: 48834.234375, LR: 1.36e-06, Best: 45571.269531
Epoch 67, Batch 0/5, Loss: 28424.533203, LR: 1.36e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.36e-06 -> 1.09e-06 (factor: 0.8)
Epoch   67/1000: Train Loss: 36462.701953, Val Loss: 48849.109375, LR: 1.09e-06, Best: 45571.269531
Epoch 68, Batch 0/5, Loss: 45020.042969, LR: 1.09e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.09e-06 -> 1.00e-06 (factor: 0.8)
Epoch   68/1000: Train Loss: 36498.702344, Val Loss: 48841.636719, LR: 1.00e-06, Best: 45571.269531
Epoch 69, Batch 0/5, Loss: 21085.421875, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   69/1000: Train Loss: 36549.881250, Val Loss: 48829.330078, LR: 1.00e-06, Best: 45571.269531
Epoch 70, Batch 0/5, Loss: 21807.242188, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   70/1000: Train Loss: 36480.537500, Val Loss: 48847.085938, LR: 1.00e-06, Best: 45571.269531
Epoch 71, Batch 0/5, Loss: 32480.167969, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   71/1000: Train Loss: 36557.100781, Val Loss: 48838.394531, LR: 1.00e-06, Best: 45571.269531
Epoch 72, Batch 0/5, Loss: 30589.162109, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   72/1000: Train Loss: 36529.211719, Val Loss: 48839.738281, LR: 1.00e-06, Best: 45571.269531
Epoch 73, Batch 0/5, Loss: 43066.703125, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   73/1000: Train Loss: 36511.087891, Val Loss: 48832.974609, LR: 1.00e-06, Best: 45571.269531
Epoch 74, Batch 0/5, Loss: 32252.062500, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   74/1000: Train Loss: 36401.690234, Val Loss: 48843.453125, LR: 1.00e-06, Best: 45571.269531
Epoch 75, Batch 0/5, Loss: 45189.632812, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   75/1000: Train Loss: 36441.988281, Val Loss: 48837.642578, LR: 1.00e-06, Best: 45571.269531
Epoch 76, Batch 0/5, Loss: 32742.486328, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   76/1000: Train Loss: 36535.155469, Val Loss: 48830.775391, LR: 1.00e-06, Best: 45571.269531
Epoch 77, Batch 0/5, Loss: 36225.394531, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   77/1000: Train Loss: 36446.751172, Val Loss: 48837.318359, LR: 1.00e-06, Best: 45571.269531
Epoch 78, Batch 0/5, Loss: 35309.570312, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   78/1000: Train Loss: 36412.315625, Val Loss: 48838.373047, LR: 1.00e-06, Best: 45571.269531
Epoch 79, Batch 0/5, Loss: 34295.917969, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   79/1000: Train Loss: 36450.671875, Val Loss: 48850.437500, LR: 1.00e-06, Best: 45571.269531
Epoch 80, Batch 0/5, Loss: 35266.796875, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   80/1000: Train Loss: 36461.596094, Val Loss: 48843.515625, LR: 1.00e-06, Best: 45571.269531
Epoch 81, Batch 0/5, Loss: 38078.308594, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   81/1000: Train Loss: 36552.009375, Val Loss: 48851.673828, LR: 1.00e-06, Best: 45571.269531
Epoch 82, Batch 0/5, Loss: 23751.427734, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   82/1000: Train Loss: 36500.370313, Val Loss: 48846.029297, LR: 1.00e-06, Best: 45571.269531
Epoch 83, Batch 0/5, Loss: 36305.011719, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   83/1000: Train Loss: 36440.144141, Val Loss: 48848.013672, LR: 1.00e-06, Best: 45571.269531
Epoch 84, Batch 0/5, Loss: 30634.134766, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   84/1000: Train Loss: 36502.678906, Val Loss: 48856.589844, LR: 1.00e-06, Best: 45571.269531
Epoch 85, Batch 0/5, Loss: 31716.818359, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   85/1000: Train Loss: 36489.998828, Val Loss: 48841.857422, LR: 1.00e-06, Best: 45571.269531
Epoch 86, Batch 0/5, Loss: 36793.691406, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   86/1000: Train Loss: 36443.596094, Val Loss: 48844.806641, LR: 1.00e-06, Best: 45571.269531
Epoch 87, Batch 0/5, Loss: 25540.462891, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   87/1000: Train Loss: 36433.723047, Val Loss: 48851.648438, LR: 1.00e-06, Best: 45571.269531
Epoch 88, Batch 0/5, Loss: 28699.380859, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   88/1000: Train Loss: 36483.812891, Val Loss: 48836.736328, LR: 1.00e-06, Best: 45571.269531
Epoch 89, Batch 0/5, Loss: 29091.878906, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   89/1000: Train Loss: 36416.525781, Val Loss: 48850.826172, LR: 1.00e-06, Best: 45571.269531
Epoch 90, Batch 0/5, Loss: 31971.507812, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   90/1000: Train Loss: 36590.702344, Val Loss: 48817.365234, LR: 1.00e-06, Best: 45571.269531
Epoch 91, Batch 0/5, Loss: 54104.375000, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   91/1000: Train Loss: 36545.455469, Val Loss: 48821.476562, LR: 1.00e-06, Best: 45571.269531
Epoch 92, Batch 0/5, Loss: 26257.324219, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   92/1000: Train Loss: 36451.907813, Val Loss: 48816.449219, LR: 1.00e-06, Best: 45571.269531
Epoch 93, Batch 0/5, Loss: 36154.898438, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   93/1000: Train Loss: 36470.727734, Val Loss: 48832.267578, LR: 1.00e-06, Best: 45571.269531
Epoch 94, Batch 0/5, Loss: 48249.652344, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   94/1000: Train Loss: 36491.863672, Val Loss: 48823.097656, LR: 1.00e-06, Best: 45571.269531
Epoch 95, Batch 0/5, Loss: 34144.632812, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   95/1000: Train Loss: 36440.285547, Val Loss: 48817.109375, LR: 1.00e-06, Best: 45571.269531
Epoch 96, Batch 0/5, Loss: 40236.691406, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   96/1000: Train Loss: 36466.283594, Val Loss: 48830.173828, LR: 1.00e-06, Best: 45571.269531
Epoch 97, Batch 0/5, Loss: 52413.812500, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   97/1000: Train Loss: 36485.578906, Val Loss: 48836.626953, LR: 1.00e-06, Best: 45571.269531
Epoch 98, Batch 0/5, Loss: 39172.789062, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   98/1000: Train Loss: 36415.633203, Val Loss: 48847.068359, LR: 1.00e-06, Best: 45571.269531
Epoch 99, Batch 0/5, Loss: 26371.181641, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch   99/1000: Train Loss: 36491.382031, Val Loss: 48854.292969, LR: 1.00e-06, Best: 45571.269531
Epoch 100, Batch 0/5, Loss: 27958.316406, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch  100/1000: Train Loss: 36463.004687, Val Loss: 48861.787109, LR: 1.00e-06, Best: 45571.269531
Epoch 101, Batch 0/5, Loss: 37286.570312, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch  101/1000: Train Loss: 36410.084766, Val Loss: 48852.980469, LR: 1.00e-06, Best: 45571.269531
Epoch 102, Batch 0/5, Loss: 25150.476562, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch  102/1000: Train Loss: 36478.980078, Val Loss: 48821.306641, LR: 1.00e-06, Best: 45571.269531
Epoch 103, Batch 0/5, Loss: 28066.138672, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch  103/1000: Train Loss: 36476.611719, Val Loss: 48835.138672, LR: 1.00e-06, Best: 45571.269531
Epoch 104, Batch 0/5, Loss: 32387.123047, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch  104/1000: Train Loss: 36473.355078, Val Loss: 48839.412109, LR: 1.00e-06, Best: 45571.269531
Epoch 105, Batch 0/5, Loss: 39832.117188, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch  105/1000: Train Loss: 36440.088672, Val Loss: 48831.970703, LR: 1.00e-06, Best: 45571.269531
Epoch 106, Batch 0/5, Loss: 39048.632812, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch  106/1000: Train Loss: 36425.978516, Val Loss: 48825.457031, LR: 1.00e-06, Best: 45571.269531
Epoch 107, Batch 0/5, Loss: 36101.835938, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch  107/1000: Train Loss: 36477.908203, Val Loss: 48834.267578, LR: 1.00e-06, Best: 45571.269531
Epoch 108, Batch 0/5, Loss: 34268.773438, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch  108/1000: Train Loss: 36479.894531, Val Loss: 48829.941406, LR: 1.00e-06, Best: 45571.269531
Epoch 109, Batch 0/5, Loss: 36077.675781, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch  109/1000: Train Loss: 36411.355078, Val Loss: 48835.183594, LR: 1.00e-06, Best: 45571.269531
Epoch 110, Batch 0/5, Loss: 29226.585938, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch  110/1000: Train Loss: 36427.623047, Val Loss: 48843.554688, LR: 1.00e-06, Best: 45571.269531
Epoch 111, Batch 0/5, Loss: 43440.363281, LR: 1.00e-06
è‡ªé€‚åº”è°ƒæ•´ - decay: LR 1.00e-06 -> 1.00e-06 (factor: 0.8)
Epoch  111/1000: Train Loss: 36452.093750, Val Loss: 48821.011719, LR: 1.00e-06, Best: 45571.269531
æ—©åœï¼šå­¦ä¹ ç‡è¿‡å°ä¸”101ä¸ªepochæ— æ”¹å–„

è®­ç»ƒå®Œæˆï¼
æ€»æ—¶é—´: 414.13ç§’
æœ€ä½³éªŒè¯æŸå¤±: 45571.269531
æœ€ç»ˆå­¦ä¹ ç‡: 1.00e-06

ğŸ‰ è®­ç»ƒå®Œæˆï¼
æœ€ä½³éªŒè¯æŸå¤±: 45571.269531
è®­ç»ƒæ—¶é—´: 414.13ç§’

ğŸ“Š è®­ç»ƒå®Œæˆï¼Œç”Ÿæˆç›‘æ§æŠ¥å‘Š...

ğŸ“Š å¯åŠ¨è®­ç»ƒç›‘æ§...
============================================================
è‡ªé€‚åº”è®­ç»ƒåˆ†ææŠ¥å‘Š
============================================================
ç”Ÿæˆæ—¶é—´: 2025-07-31 16:19:22

ğŸ“Š è®­ç»ƒåŸºæœ¬ä¿¡æ¯:
  æ€»è®­ç»ƒè½®æ•°: 112
  å½“å‰epoch: 111

ğŸ“ˆ æŸå¤±åˆ†æ:
  è®­ç»ƒæŸå¤±: 52114.857031 â†’ 36452.093750
  è®­ç»ƒæ”¹å–„: 30.05%
  éªŒè¯æŸå¤±: 51035.052734 â†’ 48821.011719
  éªŒè¯æ”¹å–„: 4.34%
  æœ€ä½³éªŒè¯æŸå¤±: 45571.269531

ğŸ¯ å­¦ä¹ ç‡åˆ†æ:
  å­¦ä¹ ç‡: 4.51e-04 â†’ 1.00e-06
  å­¦ä¹ ç‡è°ƒæ•´æ¬¡æ•°: 28

ğŸ” æ”¶æ•›çŠ¶æ€:
  å½“å‰çŠ¶æ€: æ­£å¸¸
  è¿‘æœŸæŸå¤±æ–¹å·®: 702.681284

ğŸ’¡ ä¼˜åŒ–å»ºè®®:

æŠ¥å‘Šå·²ä¿å­˜: monitoring_outputs/training_report_20250731_161922.txt
ç›‘æ§å¯åŠ¨å¤±è´¥: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

ğŸ¯ åç»­å»ºè®®:
1. æŸ¥çœ‹TensorBoard: tensorboard --logdir outputs/training_results_adaptive/tensorboard
2. è¿è¡Œç›‘æ§è„šæœ¬: python training/monitor_adaptive_training.py
3. æ£€æŸ¥æœ€ä½³æ¨¡å‹: outputs/training_results_adaptive/best_model.pth
\n==================== è®­ç»ƒç»“æœ ====================
âœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼
ğŸ“ ç»“æœä¿å­˜åœ¨: outputs/training_results_adaptive/
ğŸ“Š TensorBoardæ—¥å¿—: outputs/training_results_adaptive/tensorboard/
ğŸ’¾ æ£€æŸ¥ç‚¹æ–‡ä»¶: outputs/training_results_adaptive/checkpoints/
\nğŸ“‚ è¾“å‡ºæ–‡ä»¶åˆ—è¡¨:
total 617092
drwxrwxr-x 3 guest guest      4096 7æœˆ  31 15:54 .
drwxrwxr-x 4 guest guest      4096 7æœˆ  31 15:51 ..
-rw-rw-r-- 1 guest guest 315933707 7æœˆ  31 16:13 best_model.pth
-rw-rw-r-- 1 guest guest 315949164 7æœˆ  31 16:19 latest_checkpoint.pth
drwxrwxr-x 2 guest guest      4096 7æœˆ  31 16:12 tensorboard
\nç»“æŸæ—¶é—´: 2025å¹´ 07æœˆ 31æ—¥ æ˜ŸæœŸå›› 16:19:23 CST
æ€»è¿è¡Œæ—¶é—´: 427 ç§’
=================================================
