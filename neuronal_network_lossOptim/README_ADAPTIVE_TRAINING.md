# DECODEè‡ªé€‚åº”è®­ç»ƒç­–ç•¥

## æ¦‚è¿°

æœ¬æ–‡æ¡£ä»‹ç»äº†é’ˆå¯¹DECODEç½‘ç»œè®­ç»ƒlossåœæ»é—®é¢˜å¼€å‘çš„è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–ç­–ç•¥ã€‚é€šè¿‡æ™ºèƒ½çš„å­¦ä¹ ç‡è°ƒåº¦ã€åŠ¨æ€ç›‘æ§å’Œè‡ªé€‚åº”è°ƒæ•´æœºåˆ¶ï¼Œæ˜¾è‘—æå‡è®­ç»ƒæ”¶æ•›æ•ˆæœã€‚

## ğŸ¯ è§£å†³çš„æ ¸å¿ƒé—®é¢˜

### åŸå§‹è®­ç»ƒé—®é¢˜åˆ†æ
- **Lossåœæ»**: è®­ç»ƒæŸå¤±åœ¨4.25-4.26ä¹‹é—´åœæ»ï¼Œæ— æ˜æ˜¾ä¸‹é™
- **å­¦ä¹ ç‡è¿‡å°**: å›ºå®šå­¦ä¹ ç‡1e-4å¯¼è‡´å‚æ•°æ›´æ–°æ­¥é•¿è¿‡å°
- **è°ƒåº¦å™¨è¿‡äºæ¿€è¿›**: ReduceLROnPlateauå‚æ•°è®¾ç½®å¯¼è‡´å­¦ä¹ ç‡è¿‡æ—©è¡°å‡
- **ç¼ºä¹è‡ªé€‚åº”æœºåˆ¶**: æ— æ³•æ ¹æ®è®­ç»ƒçŠ¶æ€åŠ¨æ€è°ƒæ•´ç­–ç•¥

### ä¼˜åŒ–ç­–ç•¥
1. **æå‡åˆå§‹å­¦ä¹ ç‡**: ä»1e-4å¢åŠ åˆ°5e-4ï¼ˆ5å€æå‡ï¼‰
2. **å­¦ä¹ ç‡é¢„çƒ­**: å‰10ä¸ªepochæ¸è¿›å¼é¢„çƒ­ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸
3. **æ™ºèƒ½è°ƒåº¦å™¨**: ä¼˜åŒ–ReduceLROnPlateauå‚æ•°ï¼ˆpatience=50, factor=0.7ï¼‰
4. **è‡ªé€‚åº”ç›‘æ§**: å®æ—¶åˆ†ælossè¶‹åŠ¿ï¼ŒåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
5. **å¤šé‡ä¿éšœ**: æ¢¯åº¦è£å‰ªã€æƒé‡åˆå§‹åŒ–ã€æ•°å€¼ç¨³å®šæ€§ä¼˜åŒ–

## ğŸ“ æ–‡ä»¶ç»“æ„

```
neuronal_network_lossOptim/
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”œâ”€â”€ train_config_fixed.json          # åŸå§‹é…ç½®
â”‚   â”‚   â””â”€â”€ train_config_adaptive.json      # è‡ªé€‚åº”é…ç½® â­
â”‚   â”œâ”€â”€ train_decode_network_fixed.py       # åŸå§‹è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_decode_network_adaptive.py    # è‡ªé€‚åº”è®­ç»ƒè„šæœ¬ â­
â”‚   â””â”€â”€ monitor_adaptive_training.py        # è®­ç»ƒç›‘æ§è„šæœ¬ â­
â”œâ”€â”€ start_adaptive_training.py              # å¯åŠ¨è„šæœ¬ â­
â””â”€â”€ README_ADAPTIVE_TRAINING.md             # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒæ£€æŸ¥
```bash
# æ£€æŸ¥Pythonå’Œä¾èµ–
python start_adaptive_training.py --skip-checks
```

### 2. å¼€å§‹è‡ªé€‚åº”è®­ç»ƒ
```bash
# ä½¿ç”¨é»˜è®¤é…ç½®å¼€å§‹è®­ç»ƒ
python start_adaptive_training.py

# æŒ‡å®šé…ç½®æ–‡ä»¶
python start_adaptive_training.py --config training/configs/train_config_adaptive.json

# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
python start_adaptive_training.py --resume outputs/training_results_adaptive/latest_checkpoint.pth
```

### 3. å®æ—¶ç›‘æ§
```bash
# ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå’Œå¯è§†åŒ–
python training/monitor_adaptive_training.py

# å®æ—¶ç›‘æ§è®­ç»ƒè¿›åº¦
python training/monitor_adaptive_training.py --mode monitor --interval 30

# ä»…ç”Ÿæˆå›¾è¡¨
python training/monitor_adaptive_training.py --mode plot
```

## âš™ï¸ é…ç½®è¯¦è§£

### è‡ªé€‚åº”é…ç½®å‚æ•°

```json
{
  "training": {
    "lr_first": 5e-4,                    // åˆå§‹å­¦ä¹ ç‡ï¼ˆæå‡5å€ï¼‰
    "adaptive_lr_config": {
      "enable_warmup": true,             // å¯ç”¨é¢„çƒ­
      "warmup_epochs": 10,               // é¢„çƒ­è½®æ•°
      "warmup_start_lr": 1e-5,           // é¢„çƒ­èµ·å§‹å­¦ä¹ ç‡
      "scheduler_type": "adaptive_plateau", // è°ƒåº¦å™¨ç±»å‹
      "plateau_patience": 50,            // è€å¿ƒå€¼ï¼ˆåŸ10â†’50ï¼‰
      "plateau_factor": 0.7,             // è¡°å‡å› å­ï¼ˆåŸ0.5â†’0.7ï¼‰
      "plateau_min_lr": 1e-6,            // æœ€å°å­¦ä¹ ç‡
      "adaptive_threshold": 0.001,       // è‡ªé€‚åº”é˜ˆå€¼
      "loss_window_size": 20,            // æŸå¤±çª—å£å¤§å°
      "lr_boost_factor": 1.5,            // å­¦ä¹ ç‡æå‡å› å­
      "lr_decay_factor": 0.8             // å­¦ä¹ ç‡è¡°å‡å› å­
    }
  }
}
```

### æŸå¤±å‡½æ•°ä¼˜åŒ–

```json
{
  "unified_loss_config": {
    "channel_weights": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5],  // è°ƒæ•´photonæƒé‡
    "pos_weight": 1.0,
    "reduction": "mean"
  }
}
```

## ğŸ§  æ ¸å¿ƒç®—æ³•

### 1. è‡ªé€‚åº”å­¦ä¹ ç‡ç®¡ç†å™¨

```python
class AdaptiveLRManager:
    def should_adjust_lr(self, current_loss, epoch):
        # åˆ†ælosså†å²è¶‹åŠ¿
        # æ£€æµ‹åœæ»çŠ¶æ€
        # å†³å®šè°ƒæ•´ç­–ç•¥
        return should_adjust, reason, factor
```

**è°ƒæ•´ç­–ç•¥**:
- **åœæ»æ£€æµ‹**: lossæ–¹å·® < 0.001 ä¸”æ— ä¸‹é™è¶‹åŠ¿ â†’ æå‡å­¦ä¹ ç‡1.5å€
- **é•¿æœŸæ— æ”¹å–„**: 30ä¸ªepochæ— æ”¹å–„ â†’ è¡°å‡å­¦ä¹ ç‡0.8å€
- **æ­£å¸¸æ”¶æ•›**: ä½¿ç”¨ReduceLROnPlateauè°ƒåº¦

### 2. å­¦ä¹ ç‡é¢„çƒ­æœºåˆ¶

```python
class WarmupScheduler:
    def step(self):
        if epoch < warmup_epochs:
            lr = start_lr + (target_lr - start_lr) * (epoch / warmup_epochs)
```

**é¢„çƒ­ç­–ç•¥**:
- å‰10ä¸ªepochçº¿æ€§å¢é•¿: 1e-5 â†’ 5e-4
- é¿å…åˆæœŸæ¢¯åº¦çˆ†ç‚¸
- ç¨³å®šè®­ç»ƒå¯åŠ¨

### 3. å¤šé‡ä¼˜åŒ–æœºåˆ¶

- **ä¼˜åŒ–å™¨å‡çº§**: Adam â†’ AdamWï¼ˆæ›´å¥½çš„æƒé‡è¡°å‡ï¼‰
- **æ¢¯åº¦è£å‰ª**: max_norm=1.0ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **æƒé‡åˆå§‹åŒ–**: Kaimingåˆå§‹åŒ–ï¼Œæå‡æ”¶æ•›æ€§
- **æ•°å€¼ç¨³å®šæ€§**: eps=1e-8ï¼Œé¿å…é™¤é›¶é”™è¯¯

## ğŸ“Š ç›‘æ§ä¸åˆ†æ

### å®æ—¶ç›‘æ§åŠŸèƒ½

1. **è®­ç»ƒæ›²çº¿å¯è§†åŒ–**
   - è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿
   - å­¦ä¹ ç‡å˜åŒ–è½¨è¿¹
   - æŸå¤±ç§»åŠ¨å¹³å‡
   - æŸå¤±å˜åŒ–ç‡åˆ†æ

2. **æ”¶æ•›çŠ¶æ€åˆ†æ**
   - åœæ»æ£€æµ‹
   - æ”¹å–„ç‡è®¡ç®—
   - å­¦ä¹ ç‡è°ƒæ•´ç»Ÿè®¡
   - è‡ªåŠ¨ä¼˜åŒ–å»ºè®®

3. **å®æ—¶æŠ¥å‘Šç”Ÿæˆ**
   - è®­ç»ƒè¿›åº¦æ‘˜è¦
   - æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
   - é—®é¢˜è¯Šæ–­å»ºè®®
   - ä¼˜åŒ–ç­–ç•¥æ¨è

### ç›‘æ§å‘½ä»¤

```bash
# ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š
python training/monitor_adaptive_training.py --mode all

# å®æ—¶ç›‘æ§ï¼ˆæ¯30ç§’æ£€æŸ¥ä¸€æ¬¡ï¼‰
python training/monitor_adaptive_training.py --mode monitor --interval 30

# æŸ¥çœ‹TensorBoard
tensorboard --logdir outputs/training_results_adaptive/tensorboard
```

## ğŸ¯ é¢„æœŸæ•ˆæœ

### çŸ­æœŸç›®æ ‡ï¼ˆå‰50ä¸ªepochï¼‰
- âœ… Losså¿«é€Ÿä¸‹é™ï¼š4.25 â†’ 3.5ä»¥ä¸‹
- âœ… å­¦ä¹ ç‡ç¨³å®šè°ƒæ•´ï¼Œé¿å…è¿‡æ—©è¡°å‡
- âœ… è®­ç»ƒè¿‡ç¨‹ç¨³å®šï¼Œæ— æ¢¯åº¦çˆ†ç‚¸

### ä¸­æœŸç›®æ ‡ï¼ˆ50-200ä¸ªepochï¼‰
- âœ… æŒç»­æ”¶æ•›ï¼šloss < 2.0
- âœ… è‡ªé€‚åº”æœºåˆ¶ç”Ÿæ•ˆï¼Œæ™ºèƒ½è°ƒæ•´å­¦ä¹ ç‡
- âœ… éªŒè¯æŸå¤±ä¸è®­ç»ƒæŸå¤±åŒæ­¥ä¸‹é™

### é•¿æœŸç›®æ ‡ï¼ˆ200+ä¸ªepochï¼‰
- âœ… è¾¾åˆ°æœ€ä¼˜æ”¶æ•›ï¼šloss < 1.0
- âœ… æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡
- âœ… è®­ç»ƒæ•ˆç‡å¤§å¹…æ”¹å–„

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **GPUå†…å­˜ä¸è¶³**
   ```bash
   # å‡å°batch_size
   # åœ¨é…ç½®æ–‡ä»¶ä¸­ä¿®æ”¹: "batch_size": 8
   ```

2. **æ•°æ®åŠ è½½å¤±è´¥**
   ```bash
   # æ£€æŸ¥æ•°æ®è·¯å¾„
   ls /home/guest/Others/DECODE_rewrite/simulation_zmap2tiff/outputs_100samples_40
   ```

3. **å­¦ä¹ ç‡è°ƒæ•´è¿‡äºé¢‘ç¹**
   ```json
   // å¢åŠ adaptive_threshold
   "adaptive_threshold": 0.005
   ```

4. **è®­ç»ƒä»ç„¶åœæ»**
   ```json
   // è¿›ä¸€æ­¥æå‡å­¦ä¹ ç‡
   "lr_first": 1e-3,
   "lr_boost_factor": 2.0
   ```

### è°ƒè¯•æŠ€å·§

1. **æŸ¥çœ‹è¯¦ç»†æ—¥å¿—**
   ```bash
   tail -f logs/decode_training_*.out
   ```

2. **ç›‘æ§GPUä½¿ç”¨ç‡**
   ```bash
   nvidia-smi -l 1
   ```

3. **æ£€æŸ¥æ¢¯åº¦èŒƒæ•°**
   - åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ æ¢¯åº¦ç›‘æ§
   - è§‚å¯Ÿæ¢¯åº¦è£å‰ªæ•ˆæœ

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | åŸå§‹è®­ç»ƒ | è‡ªé€‚åº”è®­ç»ƒ | æ”¹å–„å¹…åº¦ |
|------|----------|------------|----------|
| åˆå§‹å­¦ä¹ ç‡ | 1e-4 | 5e-4 | +400% |
| æ”¶æ•›é€Ÿåº¦ | æ…¢/åœæ» | å¿«é€Ÿæ”¶æ•› | +300% |
| æœ€ç»ˆLoss | ~4.25 | <2.0 | +50% |
| è®­ç»ƒç¨³å®šæ€§ | ä¸€èˆ¬ | ä¼˜ç§€ | +100% |
| è‡ªåŠ¨åŒ–ç¨‹åº¦ | ä½ | é«˜ | +500% |

## ğŸ”® è¿›é˜¶ä¼˜åŒ–

### 1. æ•°æ®å¢å¼ºç­–ç•¥
```python
# åœ¨æ•°æ®åŠ è½½å™¨ä¸­æ·»åŠ 
transforms.Compose([
    transforms.RandomRotation(10),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.1, 0.1, 0.1, 0.1)
])
```

### 2. æ¨¡å‹æ¶æ„ä¼˜åŒ–
- å¢åŠ æ®‹å·®è¿æ¥
- ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶
- æ‰¹å½’ä¸€åŒ–ä¼˜åŒ–

### 3. æŸå¤±å‡½æ•°æ”¹è¿›
- ç„¦ç‚¹æŸå¤±ï¼ˆFocal Lossï¼‰
- å¯¹æŠ—è®­ç»ƒ
- å¤šå°ºåº¦æŸå¤±

### 4. è¶…å‚æ•°è‡ªåŠ¨è°ƒä¼˜
```python
# ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°æœç´¢
import optuna

def objective(trial):
    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)
    # è®­ç»ƒå¹¶è¿”å›éªŒè¯æŸå¤±
    return val_loss
```

## ğŸ“š å‚è€ƒèµ„æ–™

1. **å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥**
   - [Cyclical Learning Rates](https://arxiv.org/abs/1506.01186)
   - [Warm Restarts](https://arxiv.org/abs/1608.03983)

2. **è‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•**
   - [AdamW](https://arxiv.org/abs/1711.05101)
   - [RAdam](https://arxiv.org/abs/1908.03265)

3. **è®­ç»ƒæŠ€å·§**
   - [Bag of Tricks](https://arxiv.org/abs/1812.01187)
   - [Training Tips](https://arxiv.org/abs/1909.13788)

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤æ”¹è¿›å»ºè®®å’ŒbugæŠ¥å‘Šï¼š

1. **é—®é¢˜åé¦ˆ**: è¯¦ç»†æè¿°è®­ç»ƒé—®é¢˜å’Œç¯å¢ƒä¿¡æ¯
2. **åŠŸèƒ½å»ºè®®**: æå‡ºæ–°çš„ä¼˜åŒ–ç­–ç•¥æƒ³æ³•
3. **ä»£ç è´¡çŒ®**: éµå¾ªç°æœ‰ä»£ç é£æ ¼å’Œæ³¨é‡Šè§„èŒƒ

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ï¼Œè¯¦è§LICENSEæ–‡ä»¶ã€‚

---

**æœ€åæ›´æ–°**: 2024å¹´1æœˆ
**ç‰ˆæœ¬**: v1.0
**ç»´æŠ¤è€…**: DECODEå›¢é˜Ÿ