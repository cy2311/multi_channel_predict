#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„æŸå¤±å‡½æ•°

éªŒè¯ï¼š
1. æŸå¤±å€¼æ•°é‡çº§æ˜¯å¦åˆç†ï¼ˆ0.1-10èŒƒå›´ï¼‰
2. æ•°å€¼ç¨³å®šæ€§
3. æ¢¯åº¦æ˜¯å¦æ­£å¸¸
4. ä¸åŸå§‹å¤æ‚æŸå¤±å‡½æ•°çš„å¯¹æ¯”

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025å¹´
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List
import time

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æŸå¤±å‡½æ•°
from loss.unified_decode_loss import (
    UnifiedDECODELoss,
    SimpleCountLoss,
    SimpleLocLoss,
    SimpleCombinedLoss
)

# å¯¼å…¥åŸå§‹å¤æ‚æŸå¤±å‡½æ•°è¿›è¡Œå¯¹æ¯”
try:
    from loss.count_loss import CountLoss
    from loss.loc_loss import LocLoss
    ORIGINAL_AVAILABLE = True
except ImportError:
    print("åŸå§‹æŸå¤±å‡½æ•°ä¸å¯ç”¨ï¼Œè·³è¿‡å¯¹æ¯”æµ‹è¯•")
    ORIGINAL_AVAILABLE = False


class LossTestSuite:
    """
    æŸå¤±å‡½æ•°æµ‹è¯•å¥—ä»¶
    """
    
    def __init__(self, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def create_test_data(self, batch_size: int = 8, image_size: int = 40) -> Dict[str, torch.Tensor]:
        """
        åˆ›å»ºæµ‹è¯•æ•°æ®
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            image_size: å›¾åƒå¤§å°
            
        Returns:
            åŒ…å«æµ‹è¯•æ•°æ®çš„å­—å…¸
        """
        # æ¨¡æ‹Ÿç½‘ç»œè¾“å‡ºï¼ˆ6é€šé“ï¼‰
        output = torch.randn(batch_size, 6, image_size, image_size, device=self.device)
        
        # æ¨¡æ‹Ÿç›®æ ‡æ•°æ®
        target_unified = torch.randn(batch_size, 6, image_size, image_size, device=self.device)
        target_unified[:, 0] = torch.sigmoid(target_unified[:, 0])  # æ¦‚ç‡é€šé“
        
        # åˆ†ç¦»çš„ç›®æ ‡æ•°æ®
        targets = {
            'unified_target': target_unified,
            'count_maps': torch.sigmoid(torch.randn(batch_size, 1, image_size, image_size, device=self.device)),
            'loc_maps': torch.randn(batch_size, 3, image_size, image_size, device=self.device),
            'photon_maps': torch.abs(torch.randn(batch_size, 1, image_size, image_size, device=self.device)),
            'background_maps': torch.abs(torch.randn(batch_size, 1, image_size, image_size, device=self.device))
        }
        
        return {
            'output': output,
            'targets': targets
        }
    
    def test_unified_loss(self) -> Dict[str, float]:
        """
        æµ‹è¯•ç»Ÿä¸€æŸå¤±å‡½æ•°
        """
        print("\n=== æµ‹è¯•ç»Ÿä¸€DECODEæŸå¤±å‡½æ•° ===")
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        loss_fn = UnifiedDECODELoss(
            channel_weights=[1.0, 1.0, 1.0, 1.0, 0.5, 0.1],
            pos_weight=1.0
        ).to(self.device)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        data = self.create_test_data()
        output = data['output']
        target = data['targets']['unified_target']
        
        # è®¡ç®—æŸå¤±
        losses = loss_fn(output, target)
        
        # æ‰“å°ç»“æœ
        print(f"æ€»æŸå¤±: {losses['total'].item():.6f}")
        for key, value in losses.items():
            if key != 'total':
                print(f"  {key}: {value.item():.6f}")
        
        # æµ‹è¯•æ¢¯åº¦
        output_for_grad = data['output'].clone().requires_grad_(True)
        losses_for_grad = loss_fn(output_for_grad, target)
        losses_for_grad['total'].backward()
        
        grad_norm = torch.norm(output_for_grad.grad).item()
        print(f"æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        is_stable = (
            not torch.isnan(losses['total']) and
            not torch.isinf(losses['total']) and
            0.001 <= losses['total'].item() <= 100.0
        )
        
        print(f"æ•°å€¼ç¨³å®šæ€§: {'âœ“' if is_stable else 'âœ—'}")
        
        return {
            'total_loss': losses['total'].item(),
            'grad_norm': grad_norm,
            'is_stable': is_stable
        }
    
    def test_simple_count_loss(self) -> Dict[str, float]:
        """
        æµ‹è¯•ç®€åŒ–è®¡æ•°æŸå¤±
        """
        print("\n=== æµ‹è¯•ç®€åŒ–è®¡æ•°æŸå¤± ===")
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        loss_fn = SimpleCountLoss(pos_weight=1.0).to(self.device)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        data = self.create_test_data()
        pred_logits = data['output'][:, 0:1]  # [B, 1, H, W]
        target_prob = data['targets']['count_maps']  # [B, 1, H, W]
        
        # è®¡ç®—æŸå¤±
        loss = loss_fn(pred_logits, target_prob)
        
        print(f"è®¡æ•°æŸå¤±: {loss.item():.6f}")
        
        # æµ‹è¯•æ¢¯åº¦
        pred_logits_for_grad = data['output'][:, 0:1].clone().requires_grad_(True)
        loss_for_grad = loss_fn(pred_logits_for_grad, target_prob)
        loss_for_grad.backward()
        
        grad_norm = torch.norm(pred_logits_for_grad.grad).item()
        print(f"æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        is_stable = (
            not torch.isnan(loss) and
            not torch.isinf(loss) and
            0.001 <= loss.item() <= 10.0
        )
        
        print(f"æ•°å€¼ç¨³å®šæ€§: {'âœ“' if is_stable else 'âœ—'}")
        
        return {
            'loss': loss.item(),
            'grad_norm': grad_norm,
            'is_stable': is_stable
        }
    
    def test_simple_loc_loss(self) -> Dict[str, float]:
        """
        æµ‹è¯•ç®€åŒ–å®šä½æŸå¤±
        """
        print("\n=== æµ‹è¯•ç®€åŒ–å®šä½æŸå¤± ===")
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        loss_fn = SimpleLocLoss().to(self.device)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        data = self.create_test_data()
        pred_coords = data['output'][:, 1:4]  # [B, 3, H, W]
        target_coords = data['targets']['loc_maps']  # [B, 3, H, W]
        
        # åˆ›å»ºæ©ç ï¼ˆåªåœ¨éƒ¨åˆ†ä½ç½®è®¡ç®—æŸå¤±ï¼‰
        mask = torch.rand_like(data['targets']['count_maps']) > 0.7  # [B, 1, H, W]
        
        # è®¡ç®—æŸå¤±
        loss = loss_fn(pred_coords, target_coords, mask)
        
        print(f"å®šä½æŸå¤±: {loss.item():.6f}")
        
        # æµ‹è¯•æ¢¯åº¦
        pred_coords_for_grad = data['output'][:, 1:4].clone().requires_grad_(True)
        loss_for_grad = loss_fn(pred_coords_for_grad, target_coords, mask)
        loss_for_grad.backward()
        
        grad_norm = torch.norm(pred_coords_for_grad.grad).item()
        print(f"æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        is_stable = (
            not torch.isnan(loss) and
            not torch.isinf(loss) and
            0.001 <= loss.item() <= 10.0
        )
        
        print(f"æ•°å€¼ç¨³å®šæ€§: {'âœ“' if is_stable else 'âœ—'}")
        
        return {
            'loss': loss.item(),
            'grad_norm': grad_norm,
            'is_stable': is_stable
        }
    
    def test_simple_combined_loss(self) -> Dict[str, float]:
        """
        æµ‹è¯•ç®€åŒ–ç»„åˆæŸå¤±
        """
        print("\n=== æµ‹è¯•ç®€åŒ–ç»„åˆæŸå¤± ===")
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        loss_fn = SimpleCombinedLoss(
            count_weight=1.0,
            loc_weight=1.0,
            photon_weight=0.5,
            bg_weight=0.1,
            pos_weight=1.0
        ).to(self.device)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        data = self.create_test_data()
        output = data['output']
        
        # æ„å»ºè¾“å‡ºå­—å…¸
        outputs = {
            'prob': output[:, 0:1],      # [B, 1, H, W]
            'offset': output[:, 1:4],    # [B, 3, H, W]
            'photon': output[:, 4:5],    # [B, 1, H, W]
            'background': output[:, 5:6] # [B, 1, H, W]
        }
        
        # è®¡ç®—æŸå¤±
        losses = loss_fn(outputs, data['targets'])
        
        print(f"æ€»æŸå¤±: {losses['total'].item():.6f}")
        for key, value in losses.items():
            if key != 'total':
                print(f"  {key}: {value.item():.6f}")
        
        # æµ‹è¯•æ¢¯åº¦
        output_for_grad = data['output'].clone().requires_grad_(True)
        outputs_for_grad = {
            'prob': output_for_grad[:, 0:1],
            'offset': output_for_grad[:, 1:4],
            'photon': output_for_grad[:, 4:5],
            'background': output_for_grad[:, 5:6]
        }
        losses_for_grad = loss_fn(outputs_for_grad, data['targets'])
        losses_for_grad['total'].backward()
        
        grad_norm = torch.norm(output_for_grad.grad).item()
        print(f"æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        is_stable = (
            not torch.isnan(losses['total']) and
            not torch.isinf(losses['total']) and
            0.001 <= losses['total'].item() <= 100.0
        )
        
        print(f"æ•°å€¼ç¨³å®šæ€§: {'âœ“' if is_stable else 'âœ—'}")
        
        return {
            'total_loss': losses['total'].item(),
            'grad_norm': grad_norm,
            'is_stable': is_stable
        }
    
    def compare_with_original(self) -> Dict[str, Dict[str, float]]:
        """
        ä¸åŸå§‹å¤æ‚æŸå¤±å‡½æ•°å¯¹æ¯”
        """
        if not ORIGINAL_AVAILABLE:
            print("\n=== åŸå§‹æŸå¤±å‡½æ•°ä¸å¯ç”¨ï¼Œè·³è¿‡å¯¹æ¯” ===")
            return {}
        
        print("\n=== ä¸åŸå§‹æŸå¤±å‡½æ•°å¯¹æ¯” ===")
        
        results = {}
        
        # æµ‹è¯•æ•°æ®
        data = self.create_test_data(batch_size=4, image_size=32)  # è¾ƒå°çš„æ•°æ®é¿å…å†…å­˜é—®é¢˜
        
        try:
            # åŸå§‹CountLoss
            print("\n--- åŸå§‹CountLoss ---")
            original_count_loss = CountLoss().to(self.device)
            
            prob_map = torch.sigmoid(data['output'][:, 0:1])  # [B, 1, H, W]
            true_count = torch.randint(0, 10, (data['output'].size(0),), device=self.device).float()
            
            start_time = time.time()
            original_loss = original_count_loss(prob_map, true_count)
            original_time = time.time() - start_time
            
            print(f"åŸå§‹CountLoss: {original_loss.item():.6f} (è€—æ—¶: {original_time:.4f}s)")
            
            # ç®€åŒ–CountLoss
            print("\n--- ç®€åŒ–CountLoss ---")
            simple_count_loss = SimpleCountLoss().to(self.device)
            
            pred_logits = data['output'][:, 0:1]
            target_prob = data['targets']['count_maps']
            
            start_time = time.time()
            simple_loss = simple_count_loss(pred_logits, target_prob)
            simple_time = time.time() - start_time
            
            print(f"ç®€åŒ–CountLoss: {simple_loss.item():.6f} (è€—æ—¶: {simple_time:.4f}s)")
            
            results['count_loss'] = {
                'original': original_loss.item(),
                'simple': simple_loss.item(),
                'original_time': original_time,
                'simple_time': simple_time,
                'speedup': original_time / simple_time if simple_time > 0 else float('inf')
            }
            
            print(f"åŠ é€Ÿæ¯”: {results['count_loss']['speedup']:.2f}x")
            
        except Exception as e:
            print(f"CountLosså¯¹æ¯”å¤±è´¥: {e}")
        
        try:
            # åŸå§‹LocLossï¼ˆè¿™ä¸ªå¯èƒ½ä¼šå¾ˆå¤æ‚ï¼Œæˆ‘ä»¬ç®€åŒ–æµ‹è¯•ï¼‰
            print("\n--- LocLosså¯¹æ¯”ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰ ---")
            
            # ç®€åŒ–LocLoss
            simple_loc_loss = SimpleLocLoss().to(self.device)
            
            pred_coords = data['output'][:, 1:4]
            target_coords = data['targets']['loc_maps']
            mask = data['targets']['count_maps'] > 0.5
            
            start_time = time.time()
            simple_loc_result = simple_loc_loss(pred_coords, target_coords, mask)
            simple_loc_time = time.time() - start_time
            
            print(f"ç®€åŒ–LocLoss: {simple_loc_result.item():.6f} (è€—æ—¶: {simple_loc_time:.4f}s)")
            
            results['loc_loss'] = {
                'simple': simple_loc_result.item(),
                'simple_time': simple_loc_time
            }
            
        except Exception as e:
            print(f"LocLossæµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def stress_test(self, num_iterations: int = 100) -> Dict[str, List[float]]:
        """
        å‹åŠ›æµ‹è¯•ï¼šå¤šæ¬¡è¿è¡Œæ£€æŸ¥ç¨³å®šæ€§
        """
        print(f"\n=== å‹åŠ›æµ‹è¯• ({num_iterations}æ¬¡è¿­ä»£) ===")
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        unified_loss = UnifiedDECODELoss().to(self.device)
        simple_count_loss = SimpleCountLoss().to(self.device)
        
        results = {
            'unified_losses': [],
            'count_losses': [],
            'grad_norms': [],
            'computation_times': []
        }
        
        for i in range(num_iterations):
            # åˆ›å»ºéšæœºæµ‹è¯•æ•°æ®
            data = self.create_test_data(batch_size=np.random.randint(2, 16))
            
            try:
                # æµ‹è¯•ç»Ÿä¸€æŸå¤±
                start_time = time.time()
                
                output = data['output'].clone().requires_grad_(True)
                
                unified_result = unified_loss(output, data['targets']['unified_target'])
                unified_result['total'].backward()
                
                grad_norm = torch.norm(output.grad).item()
                computation_time = time.time() - start_time
                
                # è®°å½•ç»“æœ
                results['unified_losses'].append(unified_result['total'].item())
                results['grad_norms'].append(grad_norm)
                results['computation_times'].append(computation_time)
                
                # æµ‹è¯•ç®€åŒ–è®¡æ•°æŸå¤±
                count_result = simple_count_loss(
                    data['output'][:, 0:1], 
                    data['targets']['count_maps']
                )
                results['count_losses'].append(count_result.item())
                
                if (i + 1) % 20 == 0:
                    print(f"å®Œæˆ {i+1}/{num_iterations} æ¬¡è¿­ä»£")
                    
            except Exception as e:
                print(f"è¿­ä»£ {i+1} å¤±è´¥: {e}")
                continue
        
        # ç»Ÿè®¡ç»“æœ
        print("\n--- å‹åŠ›æµ‹è¯•ç»“æœ ---")
        print(f"ç»Ÿä¸€æŸå¤± - å‡å€¼: {np.mean(results['unified_losses']):.6f}, "
              f"æ ‡å‡†å·®: {np.std(results['unified_losses']):.6f}")
        print(f"è®¡æ•°æŸå¤± - å‡å€¼: {np.mean(results['count_losses']):.6f}, "
              f"æ ‡å‡†å·®: {np.std(results['count_losses']):.6f}")
        print(f"æ¢¯åº¦èŒƒæ•° - å‡å€¼: {np.mean(results['grad_norms']):.6f}, "
              f"æ ‡å‡†å·®: {np.std(results['grad_norms']):.6f}")
        print(f"è®¡ç®—æ—¶é—´ - å‡å€¼: {np.mean(results['computation_times']):.4f}s, "
              f"æ ‡å‡†å·®: {np.std(results['computation_times']):.4f}s")
        
        # æ£€æŸ¥å¼‚å¸¸å€¼
        unified_losses = np.array(results['unified_losses'])
        anomalies = np.sum((unified_losses < 0.001) | (unified_losses > 100.0))
        print(f"å¼‚å¸¸æŸå¤±å€¼æ•°é‡: {anomalies}/{len(unified_losses)}")
        
        return results
    
    def run_all_tests(self):
        """
        è¿è¡Œæ‰€æœ‰æµ‹è¯•
        """
        print("å¼€å§‹æŸå¤±å‡½æ•°æµ‹è¯•å¥—ä»¶...")
        
        # åŸºç¡€åŠŸèƒ½æµ‹è¯•
        unified_result = self.test_unified_loss()
        count_result = self.test_simple_count_loss()
        loc_result = self.test_simple_loc_loss()
        combined_result = self.test_simple_combined_loss()
        
        # å¯¹æ¯”æµ‹è¯•
        comparison_result = self.compare_with_original()
        
        # å‹åŠ›æµ‹è¯•
        stress_result = self.stress_test(num_iterations=50)
        
        # æ€»ç»“
        print("\n" + "="*50)
        print("æµ‹è¯•æ€»ç»“")
        print("="*50)
        
        all_stable = (
            unified_result['is_stable'] and
            count_result['is_stable'] and
            loc_result['is_stable'] and
            combined_result['is_stable']
        )
        
        print(f"æ‰€æœ‰æŸå¤±å‡½æ•°æ•°å€¼ç¨³å®š: {'âœ“' if all_stable else 'âœ—'}")
        
        # æŸå¤±å€¼èŒƒå›´æ£€æŸ¥
        loss_values = [
            unified_result['total_loss'],
            count_result['loss'],
            loc_result['loss'],
            combined_result['total_loss']
        ]
        
        reasonable_range = all(0.001 <= loss <= 100.0 for loss in loss_values)
        print(f"æŸå¤±å€¼åœ¨åˆç†èŒƒå›´å†…: {'âœ“' if reasonable_range else 'âœ—'}")
        
        # æ¢¯åº¦æ£€æŸ¥
        grad_norms = [
            unified_result['grad_norm'],
            count_result['grad_norm'],
            loc_result['grad_norm'],
            combined_result['grad_norm']
        ]
        
        healthy_gradients = all(0.0001 <= grad <= 10.0 for grad in grad_norms)
        print(f"æ¢¯åº¦å¥åº·: {'âœ“' if healthy_gradients else 'âœ—'}")
        
        if all_stable and reasonable_range and healthy_gradients:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æŸå¤±å‡½æ•°ä¿®å¤æˆåŠŸï¼")
        else:
            print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´")
        
        return {
            'unified': unified_result,
            'count': count_result,
            'loc': loc_result,
            'combined': combined_result,
            'comparison': comparison_result,
            'stress': stress_result
        }


def main():
    """ä¸»å‡½æ•°"""
    print("DECODEæŸå¤±å‡½æ•°ä¿®å¤éªŒè¯")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = LossTestSuite()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results = test_suite.run_all_tests()
    
    # å¯é€‰ï¼šä¿å­˜ç»“æœ
    import json
    
    # è½¬æ¢ç»“æœä¸ºå¯åºåˆ—åŒ–æ ¼å¼
    serializable_results = {}
    for key, value in results.items():
        if isinstance(value, dict):
            serializable_results[key] = {k: float(v) if isinstance(v, (int, float, torch.Tensor)) else v 
                                       for k, v in value.items() if not isinstance(v, list)}
    
    with open('loss_test_results.json', 'w') as f:
        json.dump(serializable_results, f, indent=2)
    
    print("\næµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° loss_test_results.json")


if __name__ == '__main__':
    main()