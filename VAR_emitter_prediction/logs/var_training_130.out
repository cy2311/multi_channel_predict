======================================
ğŸš€ VAR Emitterè®­ç»ƒä½œä¸šå¼€å§‹
ä½œä¸šID: 130
èŠ‚ç‚¹: master
å¼€å§‹æ—¶é—´: 2025å¹´ 07æœˆ 30æ—¥ æ˜ŸæœŸä¸‰ 14:11:31 CST
======================================
ğŸ“Š ç³»ç»Ÿä¿¡æ¯:
èŠ‚ç‚¹ä¿¡æ¯: master
GPUä¿¡æ¯:
Wed Jul 30 14:11:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.52.04              Driver Version: 555.52.04      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090        Off |   00000000:31:00.0 Off |                  N/A |
| 30%   32C    P8             17W /  350W |      12MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA GeForce RTX 3090        Off |   00000000:4B:00.0 Off |                  N/A |
| 30%   29C    P8             26W /  350W |      11MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA GeForce RTX 3090        Off |   00000000:B1:00.0 Off |                  N/A |
| 30%   31C    P8             17W /  350W |      11MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA GeForce RTX 3090        Off |   00000000:CA:00.0 Off |                  N/A |
| 30%   29C    P8             18W /  350W |      11MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      2071      G   /usr/lib/xorg/Xorg                              5MiB |
|    1   N/A  N/A      2071      G   /usr/lib/xorg/Xorg                              4MiB |
|    2   N/A  N/A      2071      G   /usr/lib/xorg/Xorg                              4MiB |
|    3   N/A  N/A      2071      G   /usr/lib/xorg/Xorg                              4MiB |
+-----------------------------------------------------------------------------------------+
å†…å­˜ä¿¡æ¯:
              total        used        free      shared  buff/cache   available
Mem:          251Gi        11Gi       158Gi       5.0Mi        82Gi       238Gi
Swap:          30Gi       246Mi        30Gi
CPUä¿¡æ¯:
Model name:                         Intel(R) Xeon(R) Platinum 8358P CPU @ 2.60GHz
Pythonç‰ˆæœ¬:
Python 3.11.7
PyTorchç‰ˆæœ¬:
PyTorch: 2.7.1+cu118
CUDA available: True
CUDA version: 11.8
======================================
âœ… æ‰€æœ‰å¿…è¦æ–‡ä»¶æ£€æŸ¥å®Œæˆ
======================================
ğŸ¯ å¼€å§‹VAR Emitteræ¨¡å‹è®­ç»ƒ...
é…ç½®æ–‡ä»¶: configs/config_true_var.json
è®­ç»ƒè„šæœ¬: train_true_var.py
======================================
INFO:__main__:Wandb logging disabled
INFO:__main__:Total parameters: 14,875,916
INFO:__main__:Trainable parameters: 14,875,916
INFO:__main__:Training samples: 1000
INFO:__main__:Validation samples: 1000
INFO:__main__:Starting VAR training...
Using device: cuda (NVIDIA GeForce RTX 3090)
GPU memory: 23.6 GB
Current batch size: 8
If you encounter CUDA out of memory errors, reduce batch_size in config file
No sample directories found at data/train, using 1000 synthetic samples
No sample directories found at data/val, using 1000 synthetic samples
Epoch 0:   0%|          | 0/125 [00:00<?, ?it/s]ERROR:__main__:CUDA out of memory error: CUDA out of memory. Tried to allocate 9.77 GiB. GPU 0 has a total capacity of 23.60 GiB of which 7.40 GiB is free. Including non-PyTorch memory, this process has 16.18 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 611.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR:__main__:Try reducing batch_size in config file
Epoch 0:   0%|          | 0/125 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/guest/Others/DECODE_rewrite/VAR_emitter_prediction/train_true_var.py", line 623, in <module>
    trainer.train()
  File "/home/guest/Others/DECODE_rewrite/VAR_emitter_prediction/train_true_var.py", line 531, in train
    losses = self.train_step(batch)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guest/Others/DECODE_rewrite/VAR_emitter_prediction/train_true_var.py", line 467, in train_step
    raise e
  File "/home/guest/Others/DECODE_rewrite/VAR_emitter_prediction/train_true_var.py", line 452, in train_step
    predictions = self.model(images)
                  ^^^^^^^^^^^^^^^^^^
  File "/home/guest/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guest/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guest/Others/DECODE_rewrite/VAR_emitter_prediction/var_emitter_model_true.py", line 350, in forward
    tokens = block(tokens, scale_cond)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guest/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guest/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guest/Others/DECODE_rewrite/VAR_emitter_prediction/var_emitter_model_true.py", line 165, in forward
    attn_out, _ = self.attn(x_norm, x_norm, x_norm)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guest/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guest/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guest/anaconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/guest/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py", line 6373, in multi_head_attention_forward
    attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.77 GiB. GPU 0 has a total capacity of 23.60 GiB of which 7.40 GiB is free. Including non-PyTorch memory, this process has 16.18 GiB memory in use. Of the allocated memory 15.28 GiB is allocated by PyTorch, and 611.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
======================================
âœ… è®­ç»ƒæˆåŠŸå®Œæˆ!
å®Œæˆæ—¶é—´: 2025å¹´ 07æœˆ 30æ—¥ æ˜ŸæœŸä¸‰ 14:11:38 CST
ğŸ“Š è®­ç»ƒç»“æœç»Ÿè®¡:
è¾“å‡ºç›®å½•å¤§å°: 4.0K	outputs
æ¨¡å‹æ–‡ä»¶æ•°é‡: 0
TensorBoardæ—¥å¿—: 544K	logs/tensorboard
======================================
ğŸ ä½œä¸šç»“æŸæ—¶é—´: 2025å¹´ 07æœˆ 30æ—¥ æ˜ŸæœŸä¸‰ 14:11:38 CST
æ€»è¿è¡Œæ—¶é—´: 7 ç§’
======================================
