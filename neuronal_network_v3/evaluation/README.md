# Evaluation è¯„ä¼°æ¨¡å—

æœ¬æ¨¡å—åŒ…å«DECODEç¥žç»ç½‘ç»œv3çš„å…¨é¢è¯„ä¼°ç³»ç»Ÿï¼Œæ”¯æŒå•é€šé“å’Œå¤šé€šé“SMLMæ•°æ®çš„æ€§èƒ½è¯„ä¼°ã€ä¸ç¡®å®šæ€§åˆ†æžå’Œå¯è§†åŒ–ã€‚

## ðŸ“‹ æ¨¡å—æ¦‚è§ˆ

### æ ¸å¿ƒç»„ä»¶

#### ðŸ”¹ MultiChannelEvaluation (`multi_channel_evaluation.py`)
- **åŠŸèƒ½**: å¤šé€šé“ç³»ç»Ÿç»¼åˆè¯„ä¼°
- **ç‰¹ç‚¹**:
  - åŒé€šé“æ€§èƒ½è¯„ä¼°
  - æ¯”ä¾‹é¢„æµ‹è¯„ä¼°
  - ç‰©ç†çº¦æŸéªŒè¯
  - ä¸ç¡®å®šæ€§æ ¡å‡†åˆ†æž
- **ç”¨é€”**: å¤šé€šé“æ¨¡åž‹çš„å…¨é¢æ€§èƒ½è¯„ä¼°

#### ðŸ”¹ ModelEvaluator (`evaluator.py`)
- **åŠŸèƒ½**: åŸºç¡€æ¨¡åž‹è¯„ä¼°å™¨
- **ç‰¹ç‚¹**:
  - æ ‡å‡†è¯„ä¼°æŒ‡æ ‡
  - æ‰¹é‡è¯„ä¼°å¤„ç†
  - å¯é…ç½®è¯„ä¼°æµç¨‹
  - ç»“æžœç»Ÿè®¡åˆ†æž
- **ç”¨é€”**: å•é€šé“æ¨¡åž‹å’ŒåŸºç¡€è¯„ä¼°ä»»åŠ¡

#### ðŸ”¹ ResultAnalyzer (`analyzer.py`)
- **åŠŸèƒ½**: ç»“æžœæ·±åº¦åˆ†æž
- **ç‰¹ç‚¹**:
  - è¯¯å·®åˆ†æž
  - æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
  - æ•°æ®åˆ†å¸ƒåˆ†æž
  - ç›¸å…³æ€§åˆ†æž
- **ç”¨é€”**: æ¨¡åž‹æ€§èƒ½çš„æ·±å…¥ç†è§£å’Œä¼˜åŒ–æŒ‡å¯¼

#### ðŸ”¹ BenchmarkEvaluator (`benchmark.py`)
- **åŠŸèƒ½**: åŸºå‡†æµ‹è¯•å’Œå¯¹æ¯”
- **ç‰¹ç‚¹**:
  - å¤šæ¨¡åž‹å¯¹æ¯”
  - æ ‡å‡†æ•°æ®é›†è¯„ä¼°
  - æ€§èƒ½åŸºå‡†å»ºç«‹
  - å›žå½’æµ‹è¯•
- **ç”¨é€”**: æ¨¡åž‹æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œç‰ˆæœ¬å¯¹æ¯”

#### ðŸ”¹ EvaluationMetrics (`metrics.py`)
- **åŠŸèƒ½**: è¯„ä¼°æŒ‡æ ‡è®¡ç®—
- **ç‰¹ç‚¹**:
  - ä¸°å¯Œçš„è¯„ä¼°æŒ‡æ ‡
  - è‡ªå®šä¹‰æŒ‡æ ‡æ”¯æŒ
  - ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•
  - ç½®ä¿¡åŒºé—´è®¡ç®—
- **ç”¨é€”**: æ ‡å‡†åŒ–çš„æ€§èƒ½æŒ‡æ ‡è®¡ç®—

#### ðŸ”¹ ResultVisualizer (`visualizer.py`)
- **åŠŸèƒ½**: ç»“æžœå¯è§†åŒ–
- **ç‰¹ç‚¹**:
  - å¤šç»´åº¦å¯è§†åŒ–
  - äº¤äº’å¼å›¾è¡¨
  - è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆ
  - è‡ªå®šä¹‰å¯è§†åŒ–
- **ç”¨é€”**: è¯„ä¼°ç»“æžœçš„ç›´è§‚å±•ç¤ºå’Œåˆ†æž

## ðŸš€ ä½¿ç”¨ç¤ºä¾‹

### å¤šé€šé“è¯„ä¼°

```python
from neuronal_network_v3.evaluation import MultiChannelEvaluation
from neuronal_network_v3.inference import MultiChannelInfer

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = MultiChannelEvaluation(
    device='cuda',
    save_plots=True,
    plot_dir='evaluation_plots/'
)

# åŠ è½½æµ‹è¯•æ•°æ®å’Œæ¨¡åž‹é¢„æµ‹
test_data = load_test_data('data/test_multi_channel.h5')
predictions = load_predictions('results/multi_channel_predictions.h5')

# æ‰§è¡Œè¯„ä¼°
metrics = evaluator.evaluate(
    predictions=predictions,
    ground_truth=test_data,
    compute_uncertainty_metrics=True,
    validate_physics_constraints=True
)

# æ‰“å°ä¸»è¦æŒ‡æ ‡
print("=== å¤šé€šé“è¯„ä¼°ç»“æžœ ===")
print(f"é€šé“1 RMSE: {metrics['channel1']['rmse']:.4f}")
print(f"é€šé“2 RMSE: {metrics['channel2']['rmse']:.4f}")
print(f"æ¯”ä¾‹é¢„æµ‹ MAE: {metrics['ratio']['mae']:.4f}")
print(f"å…‰å­æ•°å®ˆæ’è¯¯å·®: {metrics['conservation']['error']:.4f}")
print(f"ä¸ç¡®å®šæ€§æ ¡å‡†è¯¯å·®: {metrics['uncertainty']['calibration_error']:.4f}")

# ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
report = evaluator.generate_report(metrics, save_path='evaluation_report.html')
```

### å•é€šé“è¯„ä¼°

```python
from neuronal_network_v3.evaluation import ModelEvaluator

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = ModelEvaluator(
    metrics=['rmse', 'mae', 'jaccard', 'precision', 'recall'],
    confidence_level=0.95
)

# è¯„ä¼°æ¨¡åž‹
results = evaluator.evaluate_model(
    model=trained_model,
    test_loader=test_dataloader,
    return_predictions=True
)

# æŸ¥çœ‹ç»“æžœ
for metric, value in results['metrics'].items():
    ci_lower, ci_upper = results['confidence_intervals'][metric]
    print(f"{metric}: {value:.4f} [{ci_lower:.4f}, {ci_upper:.4f}]")
```

### åŸºå‡†æµ‹è¯•

```python
from neuronal_network_v3.evaluation import BenchmarkEvaluator

# åˆå§‹åŒ–åŸºå‡†æµ‹è¯•
benchmark = BenchmarkEvaluator(
    benchmark_datasets=['standard_test', 'high_density', 'low_snr'],
    reference_methods=['decode_v2', 'thunderstorm', 'rapidstorm']
)

# æ·»åŠ å¾…æµ‹è¯•æ¨¡åž‹
benchmark.add_model('decode_v3_single', single_channel_model)
benchmark.add_model('decode_v3_multi', multi_channel_model)

# è¿è¡ŒåŸºå‡†æµ‹è¯•
benchmark_results = benchmark.run_benchmark(
    save_results=True,
    output_dir='benchmark_results/'
)

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
benchmark.generate_comparison_report(
    results=benchmark_results,
    save_path='benchmark_report.pdf'
)
```

### ä¸ç¡®å®šæ€§è¯„ä¼°

```python
from neuronal_network_v3.evaluation.metrics import UncertaintyMetrics

# åˆå§‹åŒ–ä¸ç¡®å®šæ€§è¯„ä¼°
unc_metrics = UncertaintyMetrics()

# è®¡ç®—æ ¡å‡†æŒ‡æ ‡
calibration_results = unc_metrics.compute_calibration(
    predictions=pred_mean,
    uncertainties=pred_std,
    targets=ground_truth,
    num_bins=10
)

print(f"æœŸæœ›æ ¡å‡†è¯¯å·® (ECE): {calibration_results['ece']:.4f}")
print(f"æœ€å¤§æ ¡å‡†è¯¯å·® (MCE): {calibration_results['mce']:.4f}")
print(f"Brieråˆ†æ•°: {calibration_results['brier_score']:.4f}")

# è®¡ç®—è¦†ç›–çŽ‡
coverage_results = unc_metrics.compute_coverage(
    predictions=pred_mean,
    uncertainties=pred_std,
    targets=ground_truth,
    confidence_levels=[0.68, 0.95, 0.99]
)

for level, coverage in coverage_results.items():
    print(f"{level*100}% ç½®ä¿¡åŒºé—´è¦†ç›–çŽ‡: {coverage:.3f}")
```

### ç»“æžœå¯è§†åŒ–

```python
from neuronal_network_v3.evaluation import ResultVisualizer

# åˆå§‹åŒ–å¯è§†åŒ–å™¨
visualizer = ResultVisualizer(
    style='publication',
    dpi=300,
    save_format='both'  # ä¿å­˜ä¸ºPNGå’ŒPDF
)

# åˆ›å»ºè¯„ä¼°å›¾è¡¨
fig = visualizer.create_evaluation_dashboard(
    metrics=evaluation_metrics,
    predictions=model_predictions,
    ground_truth=test_targets
)

# ä¿å­˜å›¾è¡¨
visualizer.save_figure(fig, 'evaluation_dashboard.png')

# åˆ›å»ºä¸ç¡®å®šæ€§å¯è§†åŒ–
unc_fig = visualizer.plot_uncertainty_analysis(
    predictions=pred_mean,
    uncertainties=pred_std,
    targets=ground_truth
)

# åˆ›å»ºæ¯”ä¾‹é¢„æµ‹å¯è§†åŒ–ï¼ˆå¤šé€šé“ï¼‰
ratio_fig = visualizer.plot_ratio_predictions(
    predicted_ratios=pred_ratios,
    true_ratios=true_ratios,
    uncertainties=ratio_uncertainties
)
```

## ðŸ“Š è¯„ä¼°æŒ‡æ ‡è¯¦è§£

### åŸºç¡€å›žå½’æŒ‡æ ‡

#### å‡æ–¹æ ¹è¯¯å·® (RMSE)
```python
def rmse(predictions, targets):
    return torch.sqrt(torch.mean((predictions - targets) ** 2))
```

#### å¹³å‡ç»å¯¹è¯¯å·® (MAE)
```python
def mae(predictions, targets):
    return torch.mean(torch.abs(predictions - targets))
```

#### çš®å°”é€Šç›¸å…³ç³»æ•°
```python
def pearson_correlation(predictions, targets):
    return torch.corrcoef(torch.stack([predictions.flatten(), targets.flatten()]))[0, 1]
```

### æ£€æµ‹æ€§èƒ½æŒ‡æ ‡

#### ç²¾ç¡®çŽ‡å’Œå¬å›žçŽ‡
```python
def precision_recall(detections, ground_truth, distance_threshold=2.0):
    # åŸºäºŽè·ç¦»é˜ˆå€¼çš„åŒ¹é…
    matches = match_detections(detections, ground_truth, distance_threshold)
    
    precision = len(matches) / len(detections) if len(detections) > 0 else 0
    recall = len(matches) / len(ground_truth) if len(ground_truth) > 0 else 0
    
    return precision, recall
```

#### JaccardæŒ‡æ•°
```python
def jaccard_index(detections, ground_truth, distance_threshold=2.0):
    matches = match_detections(detections, ground_truth, distance_threshold)
    union = len(detections) + len(ground_truth) - len(matches)
    
    return len(matches) / union if union > 0 else 0
```

### å¤šé€šé“ç‰¹å®šæŒ‡æ ‡

#### æ¯”ä¾‹é¢„æµ‹è¯¯å·®
```python
def ratio_prediction_error(pred_ratios, true_ratios):
    return {
        'mae': torch.mean(torch.abs(pred_ratios - true_ratios)),
        'rmse': torch.sqrt(torch.mean((pred_ratios - true_ratios) ** 2)),
        'mape': torch.mean(torch.abs((pred_ratios - true_ratios) / true_ratios)) * 100
    }
```

#### å…‰å­æ•°å®ˆæ’è¯¯å·®
```python
def conservation_error(ch1_photons, ch2_photons, total_photons):
    predicted_total = ch1_photons + ch2_photons
    relative_error = torch.abs(predicted_total - total_photons) / total_photons
    return torch.mean(relative_error)
```

### ä¸ç¡®å®šæ€§é‡åŒ–æŒ‡æ ‡

#### æœŸæœ›æ ¡å‡†è¯¯å·® (ECE)
```python
def expected_calibration_error(predictions, uncertainties, targets, num_bins=10):
    bin_boundaries = torch.linspace(0, 1, num_bins + 1)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]
    
    ece = 0
    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        # è®¡ç®—æ¯ä¸ªbinçš„æ ¡å‡†è¯¯å·®
        in_bin = (uncertainties > bin_lower) & (uncertainties <= bin_upper)
        prop_in_bin = in_bin.float().mean()
        
        if prop_in_bin > 0:
            accuracy_in_bin = (torch.abs(predictions[in_bin] - targets[in_bin]) < uncertainties[in_bin]).float().mean()
            avg_confidence_in_bin = uncertainties[in_bin].mean()
            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
    
    return ece
```

#### è¦†ç›–çŽ‡
```python
def coverage_probability(predictions, uncertainties, targets, confidence_level=0.95):
    z_score = torch.distributions.Normal(0, 1).icdf(torch.tensor((1 + confidence_level) / 2))
    lower_bound = predictions - z_score * uncertainties
    upper_bound = predictions + z_score * uncertainties
    
    coverage = ((targets >= lower_bound) & (targets <= upper_bound)).float().mean()
    return coverage
```

## ðŸ”§ é«˜çº§è¯„ä¼°åŠŸèƒ½

### äº¤å‰éªŒè¯è¯„ä¼°

```python
from sklearn.model_selection import KFold

class CrossValidationEvaluator:
    def __init__(self, n_folds=5, random_state=42):
        self.n_folds = n_folds
        self.kfold = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)
    
    def evaluate_with_cv(self, model_class, dataset, model_params):
        fold_results = []
        
        for fold, (train_idx, val_idx) in enumerate(self.kfold.split(dataset)):
            print(f"è¯„ä¼°ç¬¬ {fold + 1}/{self.n_folds} æŠ˜...")
            
            # åˆ†å‰²æ•°æ®
            train_data = torch.utils.data.Subset(dataset, train_idx)
            val_data = torch.utils.data.Subset(dataset, val_idx)
            
            # è®­ç»ƒæ¨¡åž‹
            model = model_class(**model_params)
            trained_model = self.train_model(model, train_data)
            
            # è¯„ä¼°æ¨¡åž‹
            fold_metrics = self.evaluate_model(trained_model, val_data)
            fold_results.append(fold_metrics)
        
        # è®¡ç®—å¹³å‡æ€§èƒ½å’Œæ ‡å‡†å·®
        avg_metrics = self.aggregate_fold_results(fold_results)
        return avg_metrics
    
    def aggregate_fold_results(self, fold_results):
        aggregated = {}
        for metric in fold_results[0].keys():
            values = [result[metric] for result in fold_results]
            aggregated[metric] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'min': np.min(values),
                'max': np.max(values)
            }
        return aggregated
```

### ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•

```python
from scipy import stats

class StatisticalTester:
    def __init__(self, alpha=0.05):
        self.alpha = alpha
    
    def paired_t_test(self, model1_results, model2_results):
        """é…å¯¹tæ£€éªŒæ¯”è¾ƒä¸¤ä¸ªæ¨¡åž‹"""
        statistic, p_value = stats.ttest_rel(model1_results, model2_results)
        
        return {
            'statistic': statistic,
            'p_value': p_value,
            'significant': p_value < self.alpha,
            'effect_size': self.cohens_d(model1_results, model2_results)
        }
    
    def wilcoxon_test(self, model1_results, model2_results):
        """Wilcoxonç¬¦å·ç§©æ£€éªŒï¼ˆéžå‚æ•°ï¼‰"""
        statistic, p_value = stats.wilcoxon(model1_results, model2_results)
        
        return {
            'statistic': statistic,
            'p_value': p_value,
            'significant': p_value < self.alpha
        }
    
    def cohens_d(self, group1, group2):
        """è®¡ç®—Cohen's dæ•ˆåº”é‡"""
        pooled_std = np.sqrt(((len(group1) - 1) * np.var(group1) + 
                             (len(group2) - 1) * np.var(group2)) / 
                            (len(group1) + len(group2) - 2))
        return (np.mean(group1) - np.mean(group2)) / pooled_std
```

### æ€§èƒ½å‰–æž

```python
import time
import psutil
import torch.profiler

class PerformanceProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, model, test_loader, num_batches=10):
        """æŽ¨ç†æ€§èƒ½å‰–æž"""
        model.eval()
        
        # GPUå†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            initial_memory = torch.cuda.memory_allocated()
        
        # CPUä½¿ç”¨çŽ‡
        cpu_percent_start = psutil.cpu_percent()
        
        # æŽ¨ç†æ—¶é—´æµ‹é‡
        inference_times = []
        
        with torch.profiler.profile(
            activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
            record_shapes=True,
            profile_memory=True
        ) as prof:
            
            for i, batch in enumerate(test_loader):
                if i >= num_batches:
                    break
                
                start_time = time.time()
                
                with torch.no_grad():
                    _ = model(batch[0])
                
                end_time = time.time()
                inference_times.append(end_time - start_time)
        
        # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        self.metrics['inference_time'] = {
            'mean': np.mean(inference_times),
            'std': np.std(inference_times),
            'min': np.min(inference_times),
            'max': np.max(inference_times)
        }
        
        if torch.cuda.is_available():
            peak_memory = torch.cuda.max_memory_allocated()
            self.metrics['gpu_memory'] = {
                'peak_mb': (peak_memory - initial_memory) / 1024**2,
                'peak_gb': (peak_memory - initial_memory) / 1024**3
            }
        
        self.metrics['cpu_usage'] = psutil.cpu_percent() - cpu_percent_start
        
        # ä¿å­˜è¯¦ç»†çš„profileræŠ¥å‘Š
        prof.export_chrome_trace("inference_trace.json")
        
        return self.metrics
```

### é”™è¯¯åˆ†æž

```python
class ErrorAnalyzer:
    def __init__(self):
        self.error_categories = {
            'high_error': [],
            'medium_error': [],
            'low_error': []
        }
    
    def analyze_prediction_errors(self, predictions, targets, metadata):
        """åˆ†æžé¢„æµ‹è¯¯å·®çš„åˆ†å¸ƒå’Œæ¨¡å¼"""
        errors = torch.abs(predictions - targets)
        
        # æŒ‰è¯¯å·®å¤§å°åˆ†ç±»
        high_error_threshold = torch.quantile(errors, 0.9)
        medium_error_threshold = torch.quantile(errors, 0.7)
        
        high_error_mask = errors > high_error_threshold
        medium_error_mask = (errors > medium_error_threshold) & (errors <= high_error_threshold)
        low_error_mask = errors <= medium_error_threshold
        
        # åˆ†æžé«˜è¯¯å·®æ ·æœ¬çš„ç‰¹å¾
        high_error_analysis = self.analyze_error_patterns(
            predictions[high_error_mask],
            targets[high_error_mask],
            metadata[high_error_mask] if metadata is not None else None
        )
        
        return {
            'error_distribution': {
                'high_error_count': high_error_mask.sum().item(),
                'medium_error_count': medium_error_mask.sum().item(),
                'low_error_count': low_error_mask.sum().item()
            },
            'high_error_analysis': high_error_analysis,
            'error_statistics': {
                'mean': errors.mean().item(),
                'std': errors.std().item(),
                'median': errors.median().item(),
                'q95': torch.quantile(errors, 0.95).item()
            }
        }
    
    def analyze_error_patterns(self, predictions, targets, metadata):
        """åˆ†æžè¯¯å·®æ¨¡å¼"""
        analysis = {}
        
        # è¯¯å·®ä¸Žç›®æ ‡å€¼çš„å…³ç³»
        errors = torch.abs(predictions - targets)
        correlation = torch.corrcoef(torch.stack([errors.flatten(), targets.flatten()]))[0, 1]
        analysis['error_target_correlation'] = correlation.item()
        
        # è¯¯å·®çš„ç©ºé—´åˆ†å¸ƒï¼ˆå¦‚æžœæœ‰ä½ç½®ä¿¡æ¯ï¼‰
        if metadata is not None and 'positions' in metadata:
            positions = metadata['positions']
            analysis['spatial_error_pattern'] = self.analyze_spatial_errors(errors, positions)
        
        return analysis
```

## ðŸ“Š å¯è§†åŒ–åŠŸèƒ½

### è¯„ä¼°ä»ªè¡¨æ¿

```python
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.gridspec import GridSpec

class EvaluationDashboard:
    def __init__(self, style='seaborn', figsize=(15, 10)):
        plt.style.use(style)
        self.figsize = figsize
    
    def create_dashboard(self, metrics, predictions, targets):
        """åˆ›å»ºç»¼åˆè¯„ä¼°ä»ªè¡¨æ¿"""
        fig = plt.figure(figsize=self.figsize)
        gs = GridSpec(3, 4, figure=fig)
        
        # 1. è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
        ax1 = fig.add_subplot(gs[0, 0])
        errors = torch.abs(predictions - targets)
        ax1.hist(errors.numpy(), bins=50, alpha=0.7, edgecolor='black')
        ax1.set_title('è¯¯å·®åˆ†å¸ƒ')
        ax1.set_xlabel('ç»å¯¹è¯¯å·®')
        ax1.set_ylabel('é¢‘æ¬¡')
        
        # 2. é¢„æµ‹vsçœŸå®žå€¼æ•£ç‚¹å›¾
        ax2 = fig.add_subplot(gs[0, 1])
        ax2.scatter(targets.numpy(), predictions.numpy(), alpha=0.5)
        min_val = min(targets.min(), predictions.min())
        max_val = max(targets.max(), predictions.max())
        ax2.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)
        ax2.set_title('é¢„æµ‹ vs çœŸå®žå€¼')
        ax2.set_xlabel('çœŸå®žå€¼')
        ax2.set_ylabel('é¢„æµ‹å€¼')
        
        # 3. æ®‹å·®å›¾
        ax3 = fig.add_subplot(gs[0, 2])
        residuals = predictions - targets
        ax3.scatter(targets.numpy(), residuals.numpy(), alpha=0.5)
        ax3.axhline(y=0, color='r', linestyle='--')
        ax3.set_title('æ®‹å·®å›¾')
        ax3.set_xlabel('çœŸå®žå€¼')
        ax3.set_ylabel('æ®‹å·®')
        
        # 4. æŒ‡æ ‡é›·è¾¾å›¾
        ax4 = fig.add_subplot(gs[0, 3], projection='polar')
        self.plot_metrics_radar(ax4, metrics)
        
        # 5. è¯¯å·®éšæ—¶é—´å˜åŒ–ï¼ˆå¦‚æžœæœ‰æ—¶é—´ä¿¡æ¯ï¼‰
        ax5 = fig.add_subplot(gs[1, :])
        ax5.plot(errors.numpy())
        ax5.set_title('è¯¯å·®éšæ ·æœ¬å˜åŒ–')
        ax5.set_xlabel('æ ·æœ¬ç´¢å¼•')
        ax5.set_ylabel('ç»å¯¹è¯¯å·®')
        
        # 6. æŒ‡æ ‡æ±‡æ€»è¡¨
        ax6 = fig.add_subplot(gs[2, :])
        ax6.axis('tight')
        ax6.axis('off')
        self.create_metrics_table(ax6, metrics)
        
        plt.tight_layout()
        return fig
    
    def plot_metrics_radar(self, ax, metrics):
        """ç»˜åˆ¶æŒ‡æ ‡é›·è¾¾å›¾"""
        # æ ‡å‡†åŒ–æŒ‡æ ‡åˆ°0-1èŒƒå›´
        metric_names = list(metrics.keys())
        metric_values = [metrics[name] for name in metric_names]
        
        # å½’ä¸€åŒ–ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æŒ‡æ ‡è°ƒæ•´ï¼‰
        normalized_values = [(1 - val) if 'error' in name.lower() else val 
                           for name, val in zip(metric_names, metric_values)]
        
        angles = np.linspace(0, 2 * np.pi, len(metric_names), endpoint=False)
        angles = np.concatenate((angles, [angles[0]]))
        normalized_values = normalized_values + [normalized_values[0]]
        
        ax.plot(angles, normalized_values, 'o-', linewidth=2)
        ax.fill(angles, normalized_values, alpha=0.25)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metric_names)
        ax.set_title('æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾')
    
    def create_metrics_table(self, ax, metrics):
        """åˆ›å»ºæŒ‡æ ‡æ±‡æ€»è¡¨"""
        table_data = []
        for metric_name, value in metrics.items():
            if isinstance(value, dict):
                for sub_metric, sub_value in value.items():
                    table_data.append([f"{metric_name}_{sub_metric}", f"{sub_value:.4f}"])
            else:
                table_data.append([metric_name, f"{value:.4f}"])
        
        table = ax.table(cellText=table_data,
                        colLabels=['æŒ‡æ ‡', 'æ•°å€¼'],
                        cellLoc='center',
                        loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1.2, 1.5)
        ax.set_title('è¯„ä¼°æŒ‡æ ‡æ±‡æ€»')
```

## ðŸ› å¸¸è§é—®é¢˜

### Q: è¯„ä¼°æŒ‡æ ‡ä¸ç¨³å®šæ€Žä¹ˆåŠžï¼Ÿ
A: å»ºè®®æŽªæ–½ï¼š
- å¢žåŠ æµ‹è¯•æ ·æœ¬æ•°é‡
- ä½¿ç”¨äº¤å‰éªŒè¯
- è®¡ç®—ç½®ä¿¡åŒºé—´
- æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
- ä½¿ç”¨å¤šä¸ªéšæœºç§å­

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡ï¼Ÿ
A: é€‰æ‹©åŽŸåˆ™ï¼š
- æ ¹æ®ä»»åŠ¡ç±»åž‹é€‰æ‹©
- è€ƒè™‘ä¸šåŠ¡éœ€æ±‚
- å¹³è¡¡å¤šä¸ªæŒ‡æ ‡
- åŒ…å«ä¸ç¡®å®šæ€§è¯„ä¼°
- è€ƒè™‘è®¡ç®—æˆæœ¬

### Q: è¯„ä¼°é€Ÿåº¦æ…¢æ€Žä¹ˆåŠžï¼Ÿ
A: ä¼˜åŒ–æ–¹æ³•ï¼š
- å¹¶è¡Œè®¡ç®—
- é‡‡æ ·è¯„ä¼°
- ç®€åŒ–æŒ‡æ ‡
- ç¼“å­˜ä¸­é—´ç»“æžœ
- ä½¿ç”¨GPUåŠ é€Ÿ

### Q: å¦‚ä½•è§£é‡Šè¯„ä¼°ç»“æžœï¼Ÿ
A: åˆ†æžç­–ç•¥ï¼š
- å¯¹æ¯”åŸºå‡†æ–¹æ³•
- åˆ†æžè¯¯å·®åˆ†å¸ƒ
- è€ƒè™‘åº”ç”¨åœºæ™¯
- æŸ¥çœ‹å¯è§†åŒ–ç»“æžœ
- è¿›è¡Œç»Ÿè®¡æ£€éªŒ

## ðŸ“š ç›¸å…³æ–‡æ¡£

- [æ¨¡åž‹æ–‡æ¡£](../models/README.md)
- [æŽ¨ç†æ¨¡å—æ–‡æ¡£](../inference/README.md)
- [è®­ç»ƒæ¨¡å—æ–‡æ¡£](../training/README.md)
- [æ•°æ®å¤„ç†æ–‡æ¡£](../data/README.md)
- [å¤šé€šé“è®­ç»ƒæŒ‡å—](../README_MultiChannel.md)
- [è¯„ä¼°æŒ‡æ ‡å‚è€ƒ](./METRICS_REFERENCE.md)