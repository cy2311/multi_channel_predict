# Utils å·¥å…·æ¨¡å—

æœ¬æ¨¡å—åŒ…å«DECODEç¥ç»ç½‘ç»œv3çš„é€šç”¨å·¥å…·å‡½æ•°å’Œå®ç”¨ç¨‹åºï¼Œä¸ºæ•´ä¸ªé¡¹ç›®æä¾›åŸºç¡€æ”¯æŒåŠŸèƒ½ã€‚

## ğŸ“‹ æ¨¡å—æ¦‚è§ˆ

### æ ¸å¿ƒç»„ä»¶

#### ğŸ”¹ ConfigManager (`config_utils.py`)
- **åŠŸèƒ½**: é…ç½®æ–‡ä»¶ç®¡ç†
- **ç‰¹ç‚¹**:
  - YAML/JSONé…ç½®è§£æ
  - é…ç½®éªŒè¯å’Œåˆå¹¶
  - ç¯å¢ƒå˜é‡æ”¯æŒ
  - é…ç½®æ¨¡æ¿ç”Ÿæˆ
- **ç”¨é€”**: ç»Ÿä¸€çš„é…ç½®ç®¡ç†ç³»ç»Ÿ

#### ğŸ”¹ FileUtils (`file_utils.py`)
- **åŠŸèƒ½**: æ–‡ä»¶æ“ä½œå·¥å…·
- **ç‰¹ç‚¹**:
  - å®‰å…¨æ–‡ä»¶æ“ä½œ
  - æ‰¹é‡æ–‡ä»¶å¤„ç†
  - è·¯å¾„ç®¡ç†
  - æ–‡ä»¶æ ¼å¼è½¬æ¢
- **ç”¨é€”**: æ–‡ä»¶ç³»ç»Ÿç›¸å…³æ“ä½œ

#### ğŸ”¹ DataUtils (`data_utils.py`)
- **åŠŸèƒ½**: æ•°æ®å¤„ç†å·¥å…·
- **ç‰¹ç‚¹**:
  - æ•°æ®æ ¼å¼è½¬æ¢
  - æ•°ç»„æ“ä½œ
  - ç»Ÿè®¡è®¡ç®—
  - æ•°æ®éªŒè¯
- **ç”¨é€”**: é€šç”¨æ•°æ®å¤„ç†åŠŸèƒ½

#### ğŸ”¹ PlotUtils (`plot_utils.py`)
- **åŠŸèƒ½**: å¯è§†åŒ–å·¥å…·
- **ç‰¹ç‚¹**:
  - æ ‡å‡†åŒ–ç»˜å›¾
  - å¤šç§å›¾è¡¨ç±»å‹
  - è‡ªå®šä¹‰æ ·å¼
  - æ‰¹é‡ç»˜å›¾
- **ç”¨é€”**: æ•°æ®å¯è§†åŒ–å’Œç»“æœå±•ç¤º

#### ğŸ”¹ MathUtils (`math_utils.py`)
- **åŠŸèƒ½**: æ•°å­¦è®¡ç®—å·¥å…·
- **ç‰¹ç‚¹**:
  - æ•°å€¼è®¡ç®—
  - ç»Ÿè®¡å‡½æ•°
  - å‡ ä½•å˜æ¢
  - ä¿¡å·å¤„ç†
- **ç”¨é€”**: æ•°å­¦è¿ç®—å’Œç®—æ³•æ”¯æŒ

#### ğŸ”¹ LogUtils (`log_utils.py`)
- **åŠŸèƒ½**: æ—¥å¿—ç®¡ç†
- **ç‰¹ç‚¹**:
  - ç»“æ„åŒ–æ—¥å¿—
  - å¤šçº§åˆ«æ—¥å¿—
  - æ–‡ä»¶å’Œæ§åˆ¶å°è¾“å‡º
  - æ—¥å¿—è½®è½¬
- **ç”¨é€”**: ç³»ç»Ÿæ—¥å¿—è®°å½•å’Œè°ƒè¯•

#### ğŸ”¹ DeviceUtils (`device_utils.py`)
- **åŠŸèƒ½**: è®¾å¤‡ç®¡ç†å·¥å…·
- **ç‰¹ç‚¹**:
  - GPU/CPUæ£€æµ‹
  - å†…å­˜ç›‘æ§
  - è®¾å¤‡ä¼˜åŒ–
  - èµ„æºç®¡ç†
- **ç”¨é€”**: ç¡¬ä»¶èµ„æºç®¡ç†å’Œä¼˜åŒ–

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### é…ç½®ç®¡ç†

```python
from neuronal_network_v3.utils import ConfigManager

# åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
config_manager = ConfigManager()

# åŠ è½½é…ç½®æ–‡ä»¶
config = config_manager.load_config('configs/training_config.yaml')

# åˆå¹¶å¤šä¸ªé…ç½®
base_config = config_manager.load_config('configs/base_config.yaml')
training_config = config_manager.load_config('configs/training_config.yaml')
merged_config = config_manager.merge_configs(base_config, training_config)

# éªŒè¯é…ç½®
schema = {
    'model': {'type': str, 'required': True},
    'learning_rate': {'type': float, 'min': 1e-6, 'max': 1.0},
    'batch_size': {'type': int, 'min': 1, 'max': 1024}
}

is_valid, errors = config_manager.validate_config(config, schema)
if not is_valid:
    print(f"é…ç½®éªŒè¯å¤±è´¥: {errors}")

# ä¿å­˜é…ç½®
config_manager.save_config(config, 'configs/current_config.yaml')

# ä»ç¯å¢ƒå˜é‡æ›´æ–°é…ç½®
config_with_env = config_manager.update_from_env(
    config, 
    env_mapping={
        'LEARNING_RATE': 'training.learning_rate',
        'BATCH_SIZE': 'training.batch_size',
        'GPU_ID': 'device.gpu_id'
    }
)

print(f"æœ€ç»ˆé…ç½®: {config_with_env}")
```

### æ–‡ä»¶æ“ä½œå·¥å…·

```python
from neuronal_network_v3.utils import FileUtils

# åˆå§‹åŒ–æ–‡ä»¶å·¥å…·
file_utils = FileUtils()

# å®‰å…¨åˆ›å»ºç›®å½•
file_utils.ensure_dir('results/experiment_001/')

# æ‰¹é‡æ–‡ä»¶æ“ä½œ
data_files = file_utils.find_files(
    directory='data/',
    pattern='*.h5',
    recursive=True
)

print(f"æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")

# æ–‡ä»¶æ ¼å¼è½¬æ¢
file_utils.convert_format(
    input_file='data/raw_data.csv',
    output_file='data/processed_data.h5',
    input_format='csv',
    output_format='hdf5'
)

# å®‰å…¨æ–‡ä»¶å¤åˆ¶
file_utils.safe_copy(
    src='models/best_model.pth',
    dst='backup/best_model_backup.pth',
    overwrite=False
)

# æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥
checksum = file_utils.calculate_checksum('data/important_data.h5')
file_utils.save_checksum(checksum, 'data/important_data.h5.md5')

# éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
is_valid = file_utils.verify_checksum(
    'data/important_data.h5',
    'data/important_data.h5.md5'
)

if is_valid:
    print("æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡")
else:
    print("è­¦å‘Š: æ–‡ä»¶å¯èƒ½å·²æŸå")

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
file_utils.cleanup_temp_files(
    directory='temp/',
    max_age_hours=24,
    pattern='*.tmp'
)
```

### æ•°æ®å¤„ç†å·¥å…·

```python
from neuronal_network_v3.utils import DataUtils
import numpy as np
import torch

# åˆå§‹åŒ–æ•°æ®å·¥å…·
data_utils = DataUtils()

# æ•°æ®æ ¼å¼è½¬æ¢
numpy_array = np.random.randn(100, 100)
torch_tensor = data_utils.numpy_to_torch(numpy_array, device='cuda')
back_to_numpy = data_utils.torch_to_numpy(torch_tensor)

# æ•°æ®æ ‡å‡†åŒ–
data = np.random.randn(1000, 5)
normalized_data, stats = data_utils.normalize_data(
    data, 
    method='zscore',
    return_stats=True
)

# ä½¿ç”¨ä¿å­˜çš„ç»Ÿè®¡ä¿¡æ¯æ ‡å‡†åŒ–æ–°æ•°æ®
new_data = np.random.randn(100, 5)
normalized_new_data = data_utils.apply_normalization(new_data, stats)

# æ•°æ®åˆ†å‰²
train_data, val_data, test_data = data_utils.split_data(
    data,
    train_ratio=0.7,
    val_ratio=0.15,
    test_ratio=0.15,
    random_state=42
)

# æ•°æ®éªŒè¯
validation_results = data_utils.validate_data(
    data,
    checks={
        'no_nan': True,
        'no_inf': True,
        'range_check': (-10, 10),
        'shape_check': (None, 5)  # ä»»æ„è¡Œæ•°ï¼Œ5åˆ—
    }
)

if not validation_results['is_valid']:
    print(f"æ•°æ®éªŒè¯å¤±è´¥: {validation_results['errors']}")

# æ•°æ®ç»Ÿè®¡
stats = data_utils.compute_statistics(data)
print(f"æ•°æ®ç»Ÿè®¡: {stats}")

# å¼‚å¸¸å€¼æ£€æµ‹
outliers = data_utils.detect_outliers(
    data,
    method='iqr',
    threshold=1.5
)

print(f"æ£€æµ‹åˆ° {len(outliers)} ä¸ªå¼‚å¸¸å€¼")

# æ•°æ®æ’å€¼
data_with_missing = data.copy()
data_with_missing[np.random.choice(1000, 50, replace=False)] = np.nan

interpolated_data = data_utils.interpolate_missing(
    data_with_missing,
    method='linear'
)
```

### å¯è§†åŒ–å·¥å…·

```python
from neuronal_network_v3.utils import PlotUtils
import matplotlib.pyplot as plt

# åˆå§‹åŒ–ç»˜å›¾å·¥å…·
plot_utils = PlotUtils(
    style='publication',
    dpi=300,
    figsize=(10, 8)
)

# è®¾ç½®ç»˜å›¾æ ·å¼
plot_utils.set_style({
    'font.size': 12,
    'axes.linewidth': 1.5,
    'grid.alpha': 0.3
})

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
training_history = {
    'train_loss': [1.0, 0.8, 0.6, 0.4, 0.3],
    'val_loss': [1.1, 0.9, 0.7, 0.5, 0.4],
    'train_acc': [0.6, 0.7, 0.8, 0.85, 0.9],
    'val_acc': [0.55, 0.65, 0.75, 0.8, 0.85]
}

fig = plot_utils.plot_training_curves(
    training_history,
    save_path='plots/training_curves.png'
)

# ç»˜åˆ¶é¢„æµ‹ç»“æœå¯¹æ¯”
predictions = np.random.randn(100)
targets = predictions + np.random.randn(100) * 0.1

fig = plot_utils.plot_prediction_comparison(
    predictions=predictions,
    targets=targets,
    title='é¢„æµ‹ç»“æœå¯¹æ¯”',
    save_path='plots/prediction_comparison.png'
)

# ç»˜åˆ¶åˆ†å¸ƒå›¾
data = np.random.randn(1000)
fig = plot_utils.plot_distribution(
    data,
    bins=50,
    title='æ•°æ®åˆ†å¸ƒ',
    xlabel='æ•°å€¼',
    ylabel='é¢‘æ¬¡'
)

# ç»˜åˆ¶çƒ­åŠ›å›¾
correlation_matrix = np.corrcoef(np.random.randn(10, 100))
fig = plot_utils.plot_heatmap(
    correlation_matrix,
    title='ç›¸å…³æ€§çŸ©é˜µ',
    cmap='coolwarm',
    center=0
)

# æ‰¹é‡ç»˜å›¾
data_dict = {
    'experiment_1': np.random.randn(100),
    'experiment_2': np.random.randn(100) + 1,
    'experiment_3': np.random.randn(100) - 1
}

plot_utils.plot_multiple_distributions(
    data_dict,
    save_dir='plots/distributions/',
    format='both'  # ä¿å­˜PNGå’ŒPDF
)

# åˆ›å»ºå­å›¾ç½‘æ ¼
fig, axes = plot_utils.create_subplot_grid(
    nrows=2, ncols=2,
    figsize=(12, 10),
    titles=['å›¾1', 'å›¾2', 'å›¾3', 'å›¾4']
)

# åœ¨å­å›¾ä¸­ç»˜åˆ¶
for i, ax in enumerate(axes.flat):
    ax.plot(np.random.randn(100).cumsum())
    ax.set_title(f'å­å›¾ {i+1}')

plot_utils.save_figure(fig, 'plots/subplot_example.png')
```

### æ•°å­¦å·¥å…·

```python
from neuronal_network_v3.utils import MathUtils
import numpy as np

# åˆå§‹åŒ–æ•°å­¦å·¥å…·
math_utils = MathUtils()

# å‡ ä½•å˜æ¢
points = np.random.randn(100, 2)

# æ—‹è½¬å˜æ¢
rotated_points = math_utils.rotate_points(
    points, 
    angle=np.pi/4,  # 45åº¦
    center=(0, 0)
)

# ç¼©æ”¾å˜æ¢
scaled_points = math_utils.scale_points(
    points,
    scale_x=2.0,
    scale_y=1.5
)

# å¹³ç§»å˜æ¢
translated_points = math_utils.translate_points(
    points,
    dx=5.0,
    dy=3.0
)

# ç»Ÿè®¡è®¡ç®—
data = np.random.randn(1000)

# è®¡ç®—ç½®ä¿¡åŒºé—´
ci_lower, ci_upper = math_utils.confidence_interval(
    data,
    confidence=0.95
)

print(f"95% ç½®ä¿¡åŒºé—´: [{ci_lower:.3f}, {ci_upper:.3f}]")

# è®¡ç®—æ•ˆåº”é‡
group1 = np.random.randn(100)
group2 = np.random.randn(100) + 0.5

cohens_d = math_utils.cohens_d(group1, group2)
print(f"Cohen's d: {cohens_d:.3f}")

# ä¿¡å·å¤„ç†
signal = np.sin(2 * np.pi * 5 * np.linspace(0, 1, 1000))  # 5Hzä¿¡å·
noise = np.random.randn(1000) * 0.1
noisy_signal = signal + noise

# æ»¤æ³¢
filtered_signal = math_utils.lowpass_filter(
    noisy_signal,
    cutoff_freq=10,
    sampling_rate=1000
)

# å³°å€¼æ£€æµ‹
peaks = math_utils.find_peaks(
    signal,
    height=0.5,
    distance=50
)

print(f"æ£€æµ‹åˆ° {len(peaks)} ä¸ªå³°å€¼")

# æ’å€¼
x = np.linspace(0, 10, 11)
y = np.sin(x)
x_new = np.linspace(0, 10, 101)

y_interp = math_utils.interpolate(
    x, y, x_new,
    method='cubic'
)

# æ•°å€¼ç§¯åˆ†
integral = math_utils.integrate(
    lambda x: x**2,
    a=0, b=1,
    method='simpson'
)

print(f"âˆ«â‚€Â¹ xÂ² dx = {integral:.6f}")

# ä¼˜åŒ–
result = math_utils.minimize(
    lambda x: (x[0] - 1)**2 + (x[1] - 2)**2,
    x0=[0, 0],
    method='BFGS'
)

print(f"ä¼˜åŒ–ç»“æœ: x = {result.x}, f(x) = {result.fun}")
```

### æ—¥å¿—ç®¡ç†

```python
from neuronal_network_v3.utils import LogUtils
import logging

# åˆå§‹åŒ–æ—¥å¿—å·¥å…·
log_utils = LogUtils()

# è®¾ç½®æ—¥å¿—é…ç½®
logger = log_utils.setup_logger(
    name='decode_v3',
    log_file='logs/training.log',
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    max_bytes=10*1024*1024,  # 10MB
    backup_count=5
)

# ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—
log_utils.log_structured(
    logger,
    level='info',
    message='è®­ç»ƒå¼€å§‹',
    extra={
        'epoch': 1,
        'batch_size': 32,
        'learning_rate': 0.001,
        'model': 'DoubleMUNet'
    }
)

# è®°å½•æ€§èƒ½æŒ‡æ ‡
log_utils.log_metrics(
    logger,
    metrics={
        'train_loss': 0.456,
        'val_loss': 0.523,
        'train_acc': 0.89,
        'val_acc': 0.85
    },
    step=100
)

# è®°å½•å¼‚å¸¸
try:
    # ä¸€äº›å¯èƒ½å‡ºé”™çš„ä»£ç 
    result = 1 / 0
except Exception as e:
    log_utils.log_exception(
        logger,
        e,
        context={
            'function': 'train_step',
            'batch_idx': 42,
            'epoch': 5
        }
    )

# åˆ›å»ºä¸Šä¸‹æ–‡æ—¥å¿—è®°å½•å™¨
with log_utils.log_context(logger, 'model_training'):
    logger.info('å¼€å§‹æ¨¡å‹è®­ç»ƒ')
    # è®­ç»ƒä»£ç 
    logger.info('æ¨¡å‹è®­ç»ƒå®Œæˆ')

# æ—¥å¿—åˆ†æ
log_stats = log_utils.analyze_logs(
    log_file='logs/training.log',
    start_time='2024-01-01 00:00:00',
    end_time='2024-01-02 00:00:00'
)

print(f"æ—¥å¿—ç»Ÿè®¡: {log_stats}")
```

### è®¾å¤‡ç®¡ç†

```python
from neuronal_network_v3.utils import DeviceUtils
import torch

# åˆå§‹åŒ–è®¾å¤‡å·¥å…·
device_utils = DeviceUtils()

# è·å–æœ€ä½³è®¾å¤‡
device = device_utils.get_best_device()
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# æ£€æŸ¥GPUä¿¡æ¯
if torch.cuda.is_available():
    gpu_info = device_utils.get_gpu_info()
    print(f"GPUä¿¡æ¯: {gpu_info}")
    
    # é€‰æ‹©æœ€ä½³GPU
    best_gpu = device_utils.select_best_gpu()
    print(f"æœ€ä½³GPU: {best_gpu}")

# å†…å­˜ç›‘æ§
memory_info = device_utils.get_memory_info(device)
print(f"å†…å­˜ä¿¡æ¯: {memory_info}")

# è®¾ç½®å†…å­˜ç®¡ç†
device_utils.setup_memory_management(
    device=device,
    memory_fraction=0.8,  # ä½¿ç”¨80%çš„GPUå†…å­˜
    allow_growth=True
)

# ç›‘æ§èµ„æºä½¿ç”¨
with device_utils.resource_monitor(device) as monitor:
    # æ‰§è¡Œä¸€äº›è®¡ç®—å¯†é›†çš„æ“ä½œ
    data = torch.randn(1000, 1000, device=device)
    result = torch.matmul(data, data.T)
    
    # è·å–èµ„æºä½¿ç”¨æƒ…å†µ
    usage = monitor.get_usage()
    print(f"èµ„æºä½¿ç”¨: {usage}")

# ä¼˜åŒ–è®¾å¤‡è®¾ç½®
device_utils.optimize_device_settings(
    device=device,
    benchmark=True,
    deterministic=False
)

# æ¸…ç†GPUå†…å­˜
device_utils.cleanup_memory(device)

# è®¾å¤‡å…¼å®¹æ€§æ£€æŸ¥
compatibility = device_utils.check_compatibility(
    required_cuda_version='11.0',
    required_memory_gb=8
)

if not compatibility['is_compatible']:
    print(f"è®¾å¤‡å…¼å®¹æ€§é—®é¢˜: {compatibility['issues']}")
```

## ğŸ”§ é«˜çº§å·¥å…·åŠŸèƒ½

### æ€§èƒ½åˆ†æå™¨

```python
class PerformanceProfiler:
    def __init__(self):
        self.timers = {}
        self.counters = {}
        self.memory_tracker = {}
    
    def timer(self, name):
        """è®¡æ—¶å™¨è£…é¥°å™¨"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                result = func(*args, **kwargs)
                end_time = time.time()
                
                if name not in self.timers:
                    self.timers[name] = []
                self.timers[name].append(end_time - start_time)
                
                return result
            return wrapper
        return decorator
    
    def count(self, name, increment=1):
        """è®¡æ•°å™¨"""
        if name not in self.counters:
            self.counters[name] = 0
        self.counters[name] += increment
    
    def track_memory(self, name, device='cuda'):
        """å†…å­˜è¿½è¸ª"""
        if device == 'cuda' and torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated(device) / 1024**2  # MB
        else:
            import psutil
            memory_used = psutil.virtual_memory().used / 1024**2  # MB
        
        if name not in self.memory_tracker:
            self.memory_tracker[name] = []
        self.memory_tracker[name].append(memory_used)
    
    def get_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = {
            'timers': {},
            'counters': self.counters,
            'memory': {}
        }
        
        # è®¡æ—¶å™¨ç»Ÿè®¡
        for name, times in self.timers.items():
            report['timers'][name] = {
                'count': len(times),
                'total': sum(times),
                'mean': np.mean(times),
                'std': np.std(times),
                'min': min(times),
                'max': max(times)
            }
        
        # å†…å­˜ç»Ÿè®¡
        for name, memories in self.memory_tracker.items():
            report['memory'][name] = {
                'count': len(memories),
                'mean': np.mean(memories),
                'max': max(memories),
                'min': min(memories)
            }
        
        return report

# ä½¿ç”¨ç¤ºä¾‹
profiler = PerformanceProfiler()

@profiler.timer('data_loading')
def load_data():
    # æ¨¡æ‹Ÿæ•°æ®åŠ è½½
    time.sleep(0.1)
    return torch.randn(100, 100)

@profiler.timer('model_forward')
def model_forward(data):
    # æ¨¡æ‹Ÿæ¨¡å‹å‰å‘ä¼ æ’­
    time.sleep(0.05)
    return data * 2

# æ‰§è¡Œæ“ä½œ
for i in range(10):
    profiler.track_memory('before_loading')
    data = load_data()
    profiler.count('data_loaded')
    
    profiler.track_memory('after_loading')
    result = model_forward(data)
    profiler.count('forward_pass')
    
    profiler.track_memory('after_forward')

# ç”ŸæˆæŠ¥å‘Š
report = profiler.get_report()
print(json.dumps(report, indent=2))
```

### ç¼“å­˜ç®¡ç†å™¨

```python
class CacheManager:
    def __init__(self, cache_dir='cache/', max_size_gb=10):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.max_size_bytes = max_size_gb * 1024**3
        self.cache_index = self._load_cache_index()
    
    def _load_cache_index(self):
        index_file = self.cache_dir / 'cache_index.json'
        if index_file.exists():
            with open(index_file, 'r') as f:
                return json.load(f)
        return {}
    
    def _save_cache_index(self):
        index_file = self.cache_dir / 'cache_index.json'
        with open(index_file, 'w') as f:
            json.dump(self.cache_index, f, indent=2)
    
    def _get_cache_key(self, *args, **kwargs):
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = {'args': args, 'kwargs': kwargs}
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, key):
        """è·å–ç¼“å­˜"""
        if key in self.cache_index:
            cache_file = self.cache_dir / f"{key}.pkl"
            if cache_file.exists():
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                
                # æ›´æ–°è®¿é—®æ—¶é—´
                self.cache_index[key]['last_accessed'] = time.time()
                self._save_cache_index()
                
                return data
        return None
    
    def set(self, key, data):
        """è®¾ç½®ç¼“å­˜"""
        cache_file = self.cache_dir / f"{key}.pkl"
        
        # ä¿å­˜æ•°æ®
        with open(cache_file, 'wb') as f:
            pickle.dump(data, f)
        
        # æ›´æ–°ç´¢å¼•
        file_size = cache_file.stat().st_size
        self.cache_index[key] = {
            'size': file_size,
            'created': time.time(),
            'last_accessed': time.time()
        }
        
        # æ£€æŸ¥ç¼“å­˜å¤§å°
        self._cleanup_if_needed()
        self._save_cache_index()
    
    def _cleanup_if_needed(self):
        """æ¸…ç†ç¼“å­˜"""
        total_size = sum(item['size'] for item in self.cache_index.values())
        
        if total_size > self.max_size_bytes:
            # æŒ‰æœ€åè®¿é—®æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„
            sorted_items = sorted(
                self.cache_index.items(),
                key=lambda x: x[1]['last_accessed']
            )
            
            for key, info in sorted_items:
                cache_file = self.cache_dir / f"{key}.pkl"
                if cache_file.exists():
                    cache_file.unlink()
                
                del self.cache_index[key]
                total_size -= info['size']
                
                if total_size <= self.max_size_bytes * 0.8:  # æ¸…ç†åˆ°80%
                    break
    
    def cached(self, func):
        """ç¼“å­˜è£…é¥°å™¨"""
        def wrapper(*args, **kwargs):
            key = self._get_cache_key(func.__name__, *args, **kwargs)
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = self.get(key)
            if cached_result is not None:
                return cached_result
            
            # è®¡ç®—ç»“æœå¹¶ç¼“å­˜
            result = func(*args, **kwargs)
            self.set(key, result)
            
            return result
        return wrapper

# ä½¿ç”¨ç¤ºä¾‹
cache_manager = CacheManager()

@cache_manager.cached
def expensive_computation(n):
    """æ¨¡æ‹Ÿè€—æ—¶è®¡ç®—"""
    time.sleep(1)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
    return sum(i**2 for i in range(n))

# ç¬¬ä¸€æ¬¡è°ƒç”¨ä¼šè®¡ç®—å¹¶ç¼“å­˜
start_time = time.time()
result1 = expensive_computation(1000)
print(f"ç¬¬ä¸€æ¬¡è°ƒç”¨è€—æ—¶: {time.time() - start_time:.2f}ç§’")

# ç¬¬äºŒæ¬¡è°ƒç”¨ä¼šä»ç¼“å­˜è·å–
start_time = time.time()
result2 = expensive_computation(1000)
print(f"ç¬¬äºŒæ¬¡è°ƒç”¨è€—æ—¶: {time.time() - start_time:.2f}ç§’")

assert result1 == result2
```

### å¹¶è¡Œå¤„ç†å·¥å…·

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from multiprocessing import cpu_count

class ParallelProcessor:
    def __init__(self, max_workers=None, use_processes=False):
        self.max_workers = max_workers or cpu_count()
        self.use_processes = use_processes
    
    def map(self, func, iterable, chunksize=1):
        """å¹¶è¡Œæ˜ å°„"""
        executor_class = ProcessPoolExecutor if self.use_processes else ThreadPoolExecutor
        
        with executor_class(max_workers=self.max_workers) as executor:
            if self.use_processes:
                results = list(executor.map(func, iterable, chunksize=chunksize))
            else:
                results = list(executor.map(func, iterable))
        
        return results
    
    def map_async(self, func, iterable, callback=None):
        """å¼‚æ­¥å¹¶è¡Œæ˜ å°„"""
        executor_class = ProcessPoolExecutor if self.use_processes else ThreadPoolExecutor
        
        with executor_class(max_workers=self.max_workers) as executor:
            futures = [executor.submit(func, item) for item in iterable]
            
            results = []
            for future in futures:
                result = future.result()
                results.append(result)
                
                if callback:
                    callback(result)
        
        return results
    
    def batch_process(self, func, data, batch_size=100):
        """æ‰¹é‡å¤„ç†"""
        results = []
        
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            batch_results = self.map(func, batch)
            results.extend(batch_results)
            
            # è¿›åº¦æŠ¥å‘Š
            progress = min((i + batch_size) / len(data), 1.0)
            print(f"å¤„ç†è¿›åº¦: {progress:.1%}")
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
processor = ParallelProcessor(max_workers=4)

def process_item(item):
    # æ¨¡æ‹Ÿå¤„ç†
    time.sleep(0.1)
    return item ** 2

# å¹¶è¡Œå¤„ç†
data = list(range(100))
results = processor.map(process_item, data)
print(f"å¤„ç†äº† {len(results)} ä¸ªé¡¹ç›®")

# æ‰¹é‡å¤„ç†å¤§æ•°æ®
large_data = list(range(10000))
batch_results = processor.batch_process(process_item, large_data, batch_size=500)
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: é…ç½®æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®æ€ä¹ˆåŠï¼Ÿ
A: è§£å†³æ–¹æ³•ï¼š
1. ä½¿ç”¨é…ç½®éªŒè¯åŠŸèƒ½
2. æ£€æŸ¥YAML/JSONè¯­æ³•
3. å‚è€ƒé…ç½®æ¨¡æ¿
4. ä½¿ç”¨é…ç½®ç”Ÿæˆå·¥å…·

### Q: æ–‡ä»¶æ“ä½œæƒé™é—®é¢˜ï¼Ÿ
A: è§£å†³æ–¹æ¡ˆï¼š
1. æ£€æŸ¥æ–‡ä»¶æƒé™
2. ä½¿ç”¨å®‰å…¨æ–‡ä»¶æ“ä½œ
3. åˆ›å»ºå¿…è¦çš„ç›®å½•
4. å¤„ç†æƒé™å¼‚å¸¸

### Q: å†…å­˜ä½¿ç”¨è¿‡å¤šæ€ä¹ˆåŠï¼Ÿ
A: ä¼˜åŒ–ç­–ç•¥ï¼š
1. ä½¿ç”¨å†…å­˜ç›‘æ§
2. åŠæ—¶æ¸…ç†å˜é‡
3. ä½¿ç”¨ç”Ÿæˆå™¨
4. åˆ†æ‰¹å¤„ç†æ•°æ®

### Q: æ—¥å¿—æ–‡ä»¶è¿‡å¤§ï¼Ÿ
A: ç®¡ç†æ–¹æ³•ï¼š
1. è®¾ç½®æ—¥å¿—è½®è½¬
2. è°ƒæ•´æ—¥å¿—çº§åˆ«
3. å®šæœŸæ¸…ç†æ—§æ—¥å¿—
4. ä½¿ç”¨å‹ç¼©å­˜å‚¨

### Q: å¹¶è¡Œå¤„ç†æ•ˆç‡ä½ï¼Ÿ
A: ä¼˜åŒ–å»ºè®®ï¼š
1. é€‰æ‹©åˆé€‚çš„å¹¶è¡Œæ–¹å¼
2. è°ƒæ•´å·¥ä½œè¿›ç¨‹æ•°
3. ä¼˜åŒ–ä»»åŠ¡åˆ†å‰²
4. å‡å°‘è¿›ç¨‹é—´é€šä¿¡

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [é…ç½®ç®¡ç†æŒ‡å—](./CONFIG_GUIDE.md)
- [æ€§èƒ½ä¼˜åŒ–æŒ‡å—](./PERFORMANCE_GUIDE.md)
- [è°ƒè¯•å·¥å…·ä½¿ç”¨](./DEBUG_TOOLS.md)
- [æœ€ä½³å®è·µ](./BEST_PRACTICES.md)
- [APIå‚è€ƒ](./API_REFERENCE.md)