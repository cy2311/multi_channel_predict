# Inference æ¨ç†æ¨¡å—

æœ¬æ¨¡å—åŒ…å«DECODEç¥ç»ç½‘ç»œv3çš„æ¨ç†å¼•æ“ï¼Œæ”¯æŒå•é€šé“å’Œå¤šé€šé“SMLMæ•°æ®çš„é«˜æ•ˆæ¨ç†ã€åå¤„ç†å’Œç»“æœè§£æã€‚

## ğŸ“‹ æ¨¡å—æ¦‚è§ˆ

### æ ¸å¿ƒç»„ä»¶

#### ğŸ”¹ MultiChannelInfer (`multi_channel_infer.py`)
- **åŠŸèƒ½**: å¤šé€šé“è”åˆæ¨ç†å¼•æ“
- **ç‰¹ç‚¹**:
  - åŒé€šé“ååŒæ¨ç†
  - æ¯”ä¾‹é¢„æµ‹å’Œä¸ç¡®å®šæ€§é‡åŒ–
  - ç‰©ç†çº¦æŸé›†æˆ
  - æ‰¹é‡å¤„ç†ä¼˜åŒ–
- **ç”¨é€”**: å¤šé€šé“SMLMæ•°æ®çš„ä¸»è¦æ¨ç†æ¥å£

#### ğŸ”¹ BaseInfer (`infer.py`)
- **åŠŸèƒ½**: åŸºç¡€æ¨ç†å™¨
- **ç‰¹ç‚¹**:
  - æ ‡å‡†åŒ–æ¨ç†æµç¨‹
  - å†…å­˜é«˜æ•ˆå¤„ç†
  - å¯é…ç½®çš„åå¤„ç†
  - å¤šç§è¾“å‡ºæ ¼å¼
- **ç”¨é€”**: å•é€šé“æ¨ç†å’ŒåŸºç¡€æ¨ç†æ“ä½œ

#### ğŸ”¹ PostProcessing (`post_processing.py`)
- **åŠŸèƒ½**: æ¨ç†ç»“æœåå¤„ç†
- **ç‰¹ç‚¹**:
  - å³°å€¼æ£€æµ‹å’Œèšç±»
  - åæ ‡ç²¾åŒ–
  - è´¨é‡è¿‡æ»¤
  - ç»“æœä¼˜åŒ–
- **ç”¨é€”**: åŸå§‹æ¨ç†ç»“æœçš„ç²¾åŒ–å’Œä¼˜åŒ–

#### ğŸ”¹ ResultParser (`result_parser.py`)
- **åŠŸèƒ½**: ç»“æœè§£æå’Œæ ¼å¼è½¬æ¢
- **ç‰¹ç‚¹**:
  - å¤šç§è¾“å‡ºæ ¼å¼æ”¯æŒ
  - å…ƒæ•°æ®ç®¡ç†
  - ç»“æœéªŒè¯
  - ç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆ
- **ç”¨é€”**: æ¨ç†ç»“æœçš„æ ‡å‡†åŒ–è¾“å‡º

#### ğŸ”¹ InferenceUtils (`utils.py`)
- **åŠŸèƒ½**: æ¨ç†å·¥å…·å‡½æ•°
- **ç‰¹ç‚¹**:
  - å›¾åƒé¢„å¤„ç†
  - æ‰¹å¤„ç†ä¼˜åŒ–
  - å†…å­˜ç®¡ç†
  - æ€§èƒ½ç›‘æ§
- **ç”¨é€”**: æ¨ç†è¿‡ç¨‹çš„è¾…åŠ©åŠŸèƒ½

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### å¤šé€šé“æ¨ç†

```python
from neuronal_network_v3.inference import MultiChannelInfer
from neuronal_network_v3.models import SigmaMUNet, RatioNet
import torch

# åŠ è½½æ¨¡å‹
channel1_net = SigmaMUNet(n_inp=1, n_out=10)
channel2_net = SigmaMUNet(n_inp=1, n_out=10)
ratio_net = RatioNet(input_channels=20, hidden_dim=128)

# åŠ è½½æƒé‡
checkpoint = torch.load('models/multi_channel_model.pth')
channel1_net.load_state_dict(checkpoint['channel1_net'])
channel2_net.load_state_dict(checkpoint['channel2_net'])
ratio_net.load_state_dict(checkpoint['ratio_net'])

# åˆå§‹åŒ–æ¨ç†å™¨
inferrer = MultiChannelInfer(
    channel1_net=channel1_net,
    channel2_net=channel2_net,
    ratio_net=ratio_net,
    device='cuda',
    apply_conservation=True,      # åº”ç”¨å…‰å­æ•°å®ˆæ’
    apply_consistency=True,       # åº”ç”¨æ¯”ä¾‹ä¸€è‡´æ€§
    batch_size=16                 # æ‰¹å¤„ç†å¤§å°
)

# æ¨ç†
results = inferrer.predict(
    channel1_images=ch1_images,   # [N, 1, H, W]
    channel2_images=ch2_images,   # [N, 1, H, W]
    return_uncertainty=True,      # è¿”å›ä¸ç¡®å®šæ€§
    apply_postprocessing=True     # åº”ç”¨åå¤„ç†
)

# ç»“æœåŒ…å«
print(f"é€šé“1é¢„æµ‹: {results['channel1']['predictions'].shape}")
print(f"é€šé“2é¢„æµ‹: {results['channel2']['predictions'].shape}")
print(f"æ¯”ä¾‹é¢„æµ‹: {results['ratio']['mean'].shape}")
print(f"æ¯”ä¾‹ä¸ç¡®å®šæ€§: {results['ratio']['std'].shape}")
print(f"æ£€æµ‹åˆ°çš„å‘å°„ä½“: {len(results['detections'])}")
```

### å•é€šé“æ¨ç†

```python
from neuronal_network_v3.inference import BaseInfer
from neuronal_network_v3.models import SigmaMUNet

# åŠ è½½æ¨¡å‹
model = SigmaMUNet(n_inp=1, n_out=10)
model.load_state_dict(torch.load('models/single_channel_model.pth'))

# åˆå§‹åŒ–æ¨ç†å™¨
inferrer = BaseInfer(
    model=model,
    device='cuda',
    batch_size=32
)

# æ¨ç†
results = inferrer.predict(
    images=images,                # [N, 1, H, W]
    return_raw=False,             # ä¸è¿”å›åŸå§‹è¾“å‡º
    apply_postprocessing=True     # åº”ç”¨åå¤„ç†
)

# ç»“æœå¤„ç†
detections = results['detections']
for detection in detections:
    x, y, z = detection['position']
    intensity = detection['intensity']
    uncertainty = detection['uncertainty']
    print(f"ä½ç½®: ({x:.2f}, {y:.2f}, {z:.2f}), å¼ºåº¦: {intensity:.2f}")
```

### æ‰¹é‡æ¨ç†

```python
from neuronal_network_v3.inference import MultiChannelBatchInfer

# å¤§è§„æ¨¡æ•°æ®æ¨ç†
batch_inferrer = MultiChannelBatchInfer(
    channel1_net=channel1_net,
    channel2_net=channel2_net,
    ratio_net=ratio_net,
    auto_batch_size=True,         # è‡ªåŠ¨æ‰¹å¤§å°
    max_memory_gb=8.0,            # æœ€å¤§å†…å­˜ä½¿ç”¨
    progress_bar=True             # æ˜¾ç¤ºè¿›åº¦æ¡
)

# å¤„ç†å¤§å‹æ•°æ®é›†
results = batch_inferrer.predict_dataset(
    dataset_path='data/large_test_set.h5',
    output_path='results/predictions.h5',
    chunk_size=1000               # åˆ†å—å¤„ç†
)
```

### åå¤„ç†é…ç½®

```python
from neuronal_network_v3.inference.post_processing import PostProcessor

# é…ç½®åå¤„ç†å‚æ•°
post_processor = PostProcessor(
    detection_threshold=0.5,      # æ£€æµ‹é˜ˆå€¼
    nms_threshold=0.3,            # éæå¤§å€¼æŠ‘åˆ¶é˜ˆå€¼
    min_distance=2.0,             # æœ€å°è·ç¦»ï¼ˆåƒç´ ï¼‰
    quality_threshold=0.7,        # è´¨é‡é˜ˆå€¼
    coordinate_refinement=True,   # åæ ‡ç²¾åŒ–
    uncertainty_filtering=True    # ä¸ç¡®å®šæ€§è¿‡æ»¤
)

# åº”ç”¨åå¤„ç†
processed_results = post_processor.process(
    raw_predictions=raw_results,
    metadata=image_metadata
)
```

## âš™ï¸ æ¨ç†é…ç½®

### MultiChannelInferå‚æ•°

```python
infer_config = {
    'device': 'cuda',                    # è®¡ç®—è®¾å¤‡
    'batch_size': 16,                    # æ‰¹å¤„ç†å¤§å°
    'apply_conservation': True,          # å…‰å­æ•°å®ˆæ’
    'apply_consistency': True,           # æ¯”ä¾‹ä¸€è‡´æ€§
    'conservation_weight': 1.0,          # å®ˆæ’çº¦æŸæƒé‡
    'consistency_weight': 0.5,           # ä¸€è‡´æ€§çº¦æŸæƒé‡
    'uncertainty_threshold': 0.1,        # ä¸ç¡®å®šæ€§é˜ˆå€¼
    'output_format': 'dict',             # è¾“å‡ºæ ¼å¼
    'return_intermediate': False,        # è¿”å›ä¸­é—´ç»“æœ
    'memory_efficient': True             # å†…å­˜é«˜æ•ˆæ¨¡å¼
}
```

### åå¤„ç†å‚æ•°

```python
postprocess_config = {
    'detection_threshold': 0.5,          # æ£€æµ‹æ¦‚ç‡é˜ˆå€¼
    'nms_threshold': 0.3,                # NMS IoUé˜ˆå€¼
    'min_distance': 2.0,                 # æœ€å°æ£€æµ‹è·ç¦»
    'max_detections': 1000,              # æœ€å¤§æ£€æµ‹æ•°é‡
    'quality_threshold': 0.7,            # è´¨é‡åˆ†æ•°é˜ˆå€¼
    'coordinate_refinement': True,       # äºšåƒç´ ç²¾åŒ–
    'uncertainty_filtering': True,       # åŸºäºä¸ç¡®å®šæ€§è¿‡æ»¤
    'clustering_method': 'dbscan',       # èšç±»æ–¹æ³•
    'cluster_eps': 1.5,                  # èšç±»å‚æ•°
    'filter_edge_detections': True       # è¿‡æ»¤è¾¹ç¼˜æ£€æµ‹
}
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### ç‰©ç†çº¦æŸæ¨ç†

```python
class PhysicsConstrainedInfer(MultiChannelInfer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.constraint_optimizer = ConstraintOptimizer()
    
    def apply_physics_constraints(self, predictions):
        """åº”ç”¨ç‰©ç†çº¦æŸä¼˜åŒ–é¢„æµ‹ç»“æœ"""
        # å…‰å­æ•°å®ˆæ’
        total_photons = predictions['total_photons']
        ch1_photons = predictions['channel1']['photons']
        ch2_photons = predictions['channel2']['photons']
        
        # çº¦æŸä¼˜åŒ–
        optimized_predictions = self.constraint_optimizer.optimize(
            ch1_photons, ch2_photons, total_photons
        )
        
        return optimized_predictions
```

### ä¸ç¡®å®šæ€§é‡åŒ–

```python
class UncertaintyQuantifier:
    def __init__(self, num_samples=100):
        self.num_samples = num_samples
    
    def monte_carlo_inference(self, model, inputs):
        """è’™ç‰¹å¡æ´›æ¨ç†è·å–ä¸ç¡®å®šæ€§"""
        model.train()  # å¯ç”¨dropout
        predictions = []
        
        for _ in range(self.num_samples):
            with torch.no_grad():
                pred = model(inputs)
                predictions.append(pred)
        
        predictions = torch.stack(predictions)
        mean = predictions.mean(dim=0)
        std = predictions.std(dim=0)
        
        return mean, std
    
    def epistemic_uncertainty(self, predictions):
        """è®¡ç®—è®¤çŸ¥ä¸ç¡®å®šæ€§"""
        return predictions.var(dim=0)
    
    def aleatoric_uncertainty(self, model_variance):
        """è®¡ç®—å¶ç„¶ä¸ç¡®å®šæ€§"""
        return model_variance
```

### è‡ªé€‚åº”æ‰¹å¤„ç†

```python
class AdaptiveBatchInfer:
    def __init__(self, model, max_memory_gb=8.0):
        self.model = model
        self.max_memory = max_memory_gb * 1024**3  # è½¬æ¢ä¸ºå­—èŠ‚
        self.optimal_batch_size = self.find_optimal_batch_size()
    
    def find_optimal_batch_size(self):
        """è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜æ‰¹å¤§å°"""
        batch_size = 1
        while True:
            try:
                # æµ‹è¯•å½“å‰æ‰¹å¤§å°
                dummy_input = torch.randn(batch_size, 1, 64, 64)
                memory_before = torch.cuda.memory_allocated()
                
                with torch.no_grad():
                    _ = self.model(dummy_input)
                
                memory_after = torch.cuda.memory_allocated()
                memory_used = memory_after - memory_before
                
                if memory_used > self.max_memory:
                    return batch_size - 1
                
                batch_size *= 2
                
            except RuntimeError:  # OOM
                return batch_size // 2
    
    def predict(self, inputs):
        """ä½¿ç”¨æœ€ä¼˜æ‰¹å¤§å°è¿›è¡Œæ¨ç†"""
        results = []
        for i in range(0, len(inputs), self.optimal_batch_size):
            batch = inputs[i:i + self.optimal_batch_size]
            with torch.no_grad():
                batch_results = self.model(batch)
            results.append(batch_results)
        
        return torch.cat(results, dim=0)
```

### ç»“æœéªŒè¯

```python
class ResultValidator:
    def __init__(self, validation_config):
        self.config = validation_config
    
    def validate_detections(self, detections, metadata):
        """éªŒè¯æ£€æµ‹ç»“æœçš„åˆç†æ€§"""
        validation_report = {
            'total_detections': len(detections),
            'valid_detections': 0,
            'warnings': [],
            'errors': []
        }
        
        for detection in detections:
            # ä½ç½®åˆç†æ€§æ£€æŸ¥
            if self.is_position_valid(detection['position'], metadata):
                validation_report['valid_detections'] += 1
            else:
                validation_report['warnings'].append(
                    f"Invalid position: {detection['position']}"
                )
            
            # å¼ºåº¦åˆç†æ€§æ£€æŸ¥
            if not self.is_intensity_valid(detection['intensity']):
                validation_report['warnings'].append(
                    f"Unusual intensity: {detection['intensity']}"
                )
        
        return validation_report
    
    def is_position_valid(self, position, metadata):
        """æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…"""
        x, y, z = position
        image_shape = metadata['image_shape']
        
        return (0 <= x < image_shape[1] and 
                0 <= y < image_shape[0] and 
                -1000 <= z <= 1000)  # ZèŒƒå›´æ£€æŸ¥
    
    def is_intensity_valid(self, intensity):
        """æ£€æŸ¥å¼ºåº¦æ˜¯å¦åˆç†"""
        return 0 < intensity < 1e6  # åˆç†çš„å…‰å­æ•°èŒƒå›´
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### GPUå†…å­˜ä¼˜åŒ–

```python
# å†…å­˜é«˜æ•ˆæ¨ç†
with torch.cuda.amp.autocast():  # æ··åˆç²¾åº¦
    with torch.no_grad():         # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        predictions = model(inputs)

# æ¸…ç†GPUç¼“å­˜
torch.cuda.empty_cache()
```

### å¹¶è¡Œæ¨ç†

```python
from torch.nn import DataParallel
from concurrent.futures import ThreadPoolExecutor

# å¤šGPUæ¨ç†
if torch.cuda.device_count() > 1:
    model = DataParallel(model)

# CPUå¹¶è¡Œåå¤„ç†
def parallel_postprocess(predictions, num_workers=4):
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = []
        for pred in predictions:
            future = executor.submit(postprocess_single, pred)
            futures.append(future)
        
        results = [future.result() for future in futures]
    return results
```

### æµå¼æ¨ç†

```python
class StreamingInfer:
    def __init__(self, model, buffer_size=10):
        self.model = model
        self.buffer_size = buffer_size
        self.input_queue = queue.Queue(maxsize=buffer_size)
        self.output_queue = queue.Queue()
    
    def inference_worker(self):
        """æ¨ç†å·¥ä½œçº¿ç¨‹"""
        while True:
            batch = self.input_queue.get()
            if batch is None:  # åœæ­¢ä¿¡å·
                break
            
            with torch.no_grad():
                results = self.model(batch)
            
            self.output_queue.put(results)
    
    def stream_predict(self, data_stream):
        """æµå¼æ¨ç†"""
        # å¯åŠ¨æ¨ç†çº¿ç¨‹
        worker_thread = threading.Thread(target=self.inference_worker)
        worker_thread.start()
        
        # å¤„ç†æ•°æ®æµ
        for batch in data_stream:
            self.input_queue.put(batch)
            
            # è·å–ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                result = self.output_queue.get_nowait()
                yield result
            except queue.Empty:
                continue
        
        # åœæ­¢æ¨ç†çº¿ç¨‹
        self.input_queue.put(None)
        worker_thread.join()
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: æ¨ç†é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ
A: ä¼˜åŒ–å»ºè®®ï¼š
- å¢åŠ æ‰¹å¤§å°
- ä½¿ç”¨æ··åˆç²¾åº¦
- å¯ç”¨æ¨¡å‹ç¼–è¯‘
- ä½¿ç”¨å¤šGPUå¹¶è¡Œ
- ä¼˜åŒ–æ•°æ®åŠ è½½

### Q: GPUå†…å­˜ä¸è¶³ï¼Ÿ
A: è§£å†³æ–¹æ¡ˆï¼š
- å‡å°‘æ‰¹å¤§å°
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- å¯ç”¨å†…å­˜é«˜æ•ˆæ¨¡å¼
- åˆ†å—å¤„ç†å¤§å›¾åƒ
- ä½¿ç”¨CPUæ¨ç†

### Q: æ¨ç†ç»“æœä¸å‡†ç¡®ï¼Ÿ
A: æ£€æŸ¥é¡¹ç›®ï¼š
- æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½
- è¾“å…¥æ•°æ®é¢„å¤„ç†
- åå¤„ç†å‚æ•°è®¾ç½®
- ç‰©ç†çº¦æŸæ˜¯å¦åˆç†
- æ¨¡å‹è®­ç»ƒè´¨é‡

### Q: å¦‚ä½•å¤„ç†å¤§è§„æ¨¡æ•°æ®ï¼Ÿ
A: ç­–ç•¥ï¼š
- ä½¿ç”¨æ‰¹é‡æ¨ç†
- å¯ç”¨æµå¼å¤„ç†
- åˆ†å—å¤„ç†
- å¹¶è¡Œè®¡ç®—
- ç»“æœç¼“å­˜

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [æ¨¡å‹æ–‡æ¡£](../models/README.md)
- [è®­ç»ƒæ¨¡å—æ–‡æ¡£](../training/README.md)
- [è¯„ä¼°æ¨¡å—æ–‡æ¡£](../evaluation/README.md)
- [æ•°æ®å¤„ç†æ–‡æ¡£](../data/README.md)
- [å¤šé€šé“è®­ç»ƒæŒ‡å—](../README_MultiChannel.md)