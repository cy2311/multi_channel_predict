# Training è®­ç»ƒæ¨¡å—

æœ¬æ¨¡å—åŒ…å«DECODEç¥ç»ç½‘ç»œv3çš„å®Œæ•´è®­ç»ƒç³»ç»Ÿï¼Œæ”¯æŒå•é€šé“å’Œå¤šé€šé“SMLMæ•°æ®çš„æ¨¡å‹è®­ç»ƒã€ä¼˜åŒ–å’Œç›‘æ§ã€‚

## ğŸ“‹ æ¨¡å—æ¦‚è§ˆ

### æ ¸å¿ƒç»„ä»¶

#### ğŸ”¹ MultiChannelTrainer (`multi_channel_trainer.py`)
- **åŠŸèƒ½**: å¤šé€šé“ç³»ç»Ÿä¸“ç”¨è®­ç»ƒå™¨
- **ç‰¹ç‚¹**:
  - åŒé€šé“è”åˆè®­ç»ƒ
  - æ¯”ä¾‹é¢„æµ‹è®­ç»ƒ
  - ç‰©ç†çº¦æŸé›†æˆ
  - ä¸ç¡®å®šæ€§è®­ç»ƒ
- **ç”¨é€”**: å¤šé€šé“æ¨¡å‹çš„ç«¯åˆ°ç«¯è®­ç»ƒ

#### ğŸ”¹ BaseTrainer (`trainer.py`)
- **åŠŸèƒ½**: åŸºç¡€è®­ç»ƒå™¨æ¡†æ¶
- **ç‰¹ç‚¹**:
  - çµæ´»çš„è®­ç»ƒæµç¨‹
  - å¤šç§ä¼˜åŒ–å™¨æ”¯æŒ
  - å­¦ä¹ ç‡è°ƒåº¦
  - æ—©åœæœºåˆ¶
- **ç”¨é€”**: å•é€šé“æ¨¡å‹å’ŒåŸºç¡€è®­ç»ƒä»»åŠ¡

#### ğŸ”¹ TrainingConfig (`config.py`)
- **åŠŸèƒ½**: è®­ç»ƒé…ç½®ç®¡ç†
- **ç‰¹ç‚¹**:
  - å‚æ•°éªŒè¯
  - é…ç½®ç»§æ‰¿
  - åŠ¨æ€é…ç½®
  - é…ç½®ä¿å­˜/åŠ è½½
- **ç”¨é€”**: ç»Ÿä¸€çš„è®­ç»ƒå‚æ•°ç®¡ç†

#### ğŸ”¹ TrainingMonitor (`monitor.py`)
- **åŠŸèƒ½**: è®­ç»ƒè¿‡ç¨‹ç›‘æ§
- **ç‰¹ç‚¹**:
  - å®æ—¶æŒ‡æ ‡è¿½è¸ª
  - å¯è§†åŒ–ç›‘æ§
  - å¼‚å¸¸æ£€æµ‹
  - æ€§èƒ½åˆ†æ
- **ç”¨é€”**: è®­ç»ƒè¿‡ç¨‹çš„å®æ—¶ç›‘æ§å’Œåˆ†æ

#### ğŸ”¹ CheckpointManager (`checkpoint.py`)
- **åŠŸèƒ½**: æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†
- **ç‰¹ç‚¹**:
  - è‡ªåŠ¨ä¿å­˜
  - ç‰ˆæœ¬ç®¡ç†
  - æœ€ä½³æ¨¡å‹è¿½è¸ª
  - æ–­ç‚¹ç»­è®­
- **ç”¨é€”**: è®­ç»ƒçŠ¶æ€çš„ä¿å­˜å’Œæ¢å¤

#### ğŸ”¹ LossScheduler (`scheduler.py`)
- **åŠŸèƒ½**: æŸå¤±å‡½æ•°è°ƒåº¦
- **ç‰¹ç‚¹**:
  - åŠ¨æ€æƒé‡è°ƒæ•´
  - å¤šé˜¶æ®µè®­ç»ƒ
  - è‡ªé€‚åº”è°ƒåº¦
  - æŸå¤±å¹³è¡¡
- **ç”¨é€”**: å¤æ‚æŸå¤±å‡½æ•°çš„åŠ¨æ€è°ƒæ•´

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### å¤šé€šé“è®­ç»ƒ

```python
from neuronal_network_v3.training import MultiChannelTrainer
from neuronal_network_v3.models import DoubleMUNet
from neuronal_network_v3.loss import UnifiedLoss
from neuronal_network_v3.data import create_multi_channel_dataloader

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = create_multi_channel_dataloader(
    data_path='data/train_multi_channel.h5',
    config={
        'batch_size': 16,
        'shuffle': True,
        'num_workers': 4,
        'pin_memory': True
    },
    mode='train'
)

val_loader = create_multi_channel_dataloader(
    data_path='data/val_multi_channel.h5',
    config={'batch_size': 32, 'shuffle': False},
    mode='val'
)

# åˆå§‹åŒ–æ¨¡å‹
model = DoubleMUNet(
    in_channels=1,
    out_channels_ch1=5,  # x, y, z, photons, bg
    out_channels_ch2=5,
    out_channels_ratio=1,
    uncertainty=True
)

# åˆå§‹åŒ–æŸå¤±å‡½æ•°
loss_fn = UnifiedLoss(
    ch1_weight=1.0,
    ch2_weight=1.0,
    ratio_weight=0.5,
    conservation_weight=0.1,
    uncertainty_weight=0.1
)

# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = MultiChannelTrainer(
    model=model,
    loss_fn=loss_fn,
    optimizer_config={
        'type': 'AdamW',
        'lr': 1e-3,
        'weight_decay': 1e-4
    },
    scheduler_config={
        'type': 'CosineAnnealingLR',
        'T_max': 100,
        'eta_min': 1e-6
    },
    device='cuda',
    mixed_precision=True
)

# å¼€å§‹è®­ç»ƒ
training_history = trainer.train(
    train_loader=train_loader,
    val_loader=val_loader,
    epochs=100,
    save_dir='checkpoints/multi_channel/',
    save_every=10,
    validate_every=5,
    early_stopping_patience=15
)

# æŸ¥çœ‹è®­ç»ƒå†å²
print("=== è®­ç»ƒå®Œæˆ ===")
print(f"æœ€ä½³éªŒè¯æŸå¤±: {training_history['best_val_loss']:.6f}")
print(f"æœ€ä½³æ¨¡å‹epoch: {training_history['best_epoch']}")
print(f"è®­ç»ƒæ€»æ—¶é•¿: {training_history['total_time']:.2f}ç§’")
```

### å•é€šé“è®­ç»ƒ

```python
from neuronal_network_v3.training import BaseTrainer
from neuronal_network_v3.models import SigmaMUNet
from neuronal_network_v3.loss import RatioGaussianNLLLoss

# åˆå§‹åŒ–æ¨¡å‹
model = SigmaMUNet(
    in_channels=1,
    out_channels=5,  # x, y, z, photons, bg
    uncertainty=True
)

# åˆå§‹åŒ–æŸå¤±å‡½æ•°
loss_fn = RatioGaussianNLLLoss()

# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = BaseTrainer(
    model=model,
    loss_fn=loss_fn,
    optimizer_config={
        'type': 'Adam',
        'lr': 2e-3,
        'betas': (0.9, 0.999)
    },
    scheduler_config={
        'type': 'StepLR',
        'step_size': 30,
        'gamma': 0.5
    }
)

# è®­ç»ƒé…ç½®
training_config = {
    'epochs': 80,
    'gradient_clip_val': 1.0,
    'accumulate_grad_batches': 2,
    'log_every': 100,
    'validate_every': 5
}

# å¼€å§‹è®­ç»ƒ
results = trainer.train(
    train_loader=train_loader,
    val_loader=val_loader,
    **training_config
)
```

### é«˜çº§è®­ç»ƒé…ç½®

```python
from neuronal_network_v3.training import TrainingConfig, LossScheduler

# åˆ›å»ºè®­ç»ƒé…ç½®
config = TrainingConfig(
    # åŸºç¡€é…ç½®
    epochs=100,
    batch_size=16,
    learning_rate=1e-3,
    
    # ä¼˜åŒ–å™¨é…ç½®
    optimizer={
        'type': 'AdamW',
        'weight_decay': 1e-4,
        'amsgrad': True
    },
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler={
        'type': 'OneCycleLR',
        'max_lr': 1e-2,
        'pct_start': 0.3,
        'anneal_strategy': 'cos'
    },
    
    # æ­£åˆ™åŒ–
    regularization={
        'dropout': 0.1,
        'batch_norm': True,
        'weight_decay': 1e-4
    },
    
    # è®­ç»ƒç­–ç•¥
    training_strategy={
        'mixed_precision': True,
        'gradient_clipping': 1.0,
        'accumulate_grad_batches': 1,
        'early_stopping_patience': 20
    },
    
    # æŸå¤±å‡½æ•°æƒé‡è°ƒåº¦
    loss_scheduling={
        'enable': True,
        'schedule_type': 'linear',
        'warmup_epochs': 10,
        'final_weights': {
            'ch1_weight': 1.0,
            'ch2_weight': 1.0,
            'ratio_weight': 0.8,
            'conservation_weight': 0.2
        }
    },
    
    # ç›‘æ§é…ç½®
    monitoring={
        'log_every': 50,
        'validate_every': 5,
        'save_every': 10,
        'plot_every': 20
    }
)

# ä½¿ç”¨é…ç½®åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = MultiChannelTrainer.from_config(config)
```

### æŸå¤±å‡½æ•°è°ƒåº¦

```python
from neuronal_network_v3.training import LossScheduler

# åˆ›å»ºæŸå¤±è°ƒåº¦å™¨
loss_scheduler = LossScheduler(
    total_epochs=100,
    schedule_config={
        'ch1_weight': {
            'type': 'constant',
            'value': 1.0
        },
        'ch2_weight': {
            'type': 'constant', 
            'value': 1.0
        },
        'ratio_weight': {
            'type': 'linear',
            'start_value': 0.1,
            'end_value': 0.8,
            'start_epoch': 10,
            'end_epoch': 50
        },
        'conservation_weight': {
            'type': 'exponential',
            'start_value': 0.01,
            'end_value': 0.2,
            'decay_rate': 0.1
        },
        'uncertainty_weight': {
            'type': 'cosine',
            'min_value': 0.05,
            'max_value': 0.15,
            'period': 20
        }
    }
)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for epoch in range(100):
    # æ›´æ–°æŸå¤±æƒé‡
    current_weights = loss_scheduler.get_weights(epoch)
    loss_fn.update_weights(**current_weights)
    
    # è®­ç»ƒä¸€ä¸ªepoch
    train_loss = trainer.train_epoch(train_loader)
    
    print(f"Epoch {epoch}: Loss={train_loss:.6f}, Weights={current_weights}")
```

### è®­ç»ƒç›‘æ§

```python
from neuronal_network_v3.training import TrainingMonitor
import wandb

# åˆå§‹åŒ–ç›‘æ§å™¨
monitor = TrainingMonitor(
    log_dir='logs/training/',
    use_tensorboard=True,
    use_wandb=True,
    wandb_project='decode_v3',
    wandb_config={
        'model': 'DoubleMUNet',
        'dataset': 'multi_channel_smlm',
        'experiment': 'baseline_training'
    }
)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­è®°å½•æŒ‡æ ‡
for epoch in range(epochs):
    # è®­ç»ƒé˜¶æ®µ
    model.train()
    train_metrics = {'loss': 0, 'ch1_loss': 0, 'ch2_loss': 0, 'ratio_loss': 0}
    
    for batch_idx, batch in enumerate(train_loader):
        # ... è®­ç»ƒä»£ç  ...
        
        # è®°å½•æ‰¹æ¬¡æŒ‡æ ‡
        monitor.log_batch_metrics(
            epoch=epoch,
            batch_idx=batch_idx,
            metrics={
                'train_loss': loss.item(),
                'train_ch1_loss': ch1_loss.item(),
                'train_ch2_loss': ch2_loss.item(),
                'train_ratio_loss': ratio_loss.item(),
                'learning_rate': optimizer.param_groups[0]['lr']
            }
        )
    
    # éªŒè¯é˜¶æ®µ
    val_metrics = trainer.validate(val_loader)
    
    # è®°å½•epochæŒ‡æ ‡
    monitor.log_epoch_metrics(
        epoch=epoch,
        train_metrics=train_metrics,
        val_metrics=val_metrics,
        model=model
    )
    
    # ç”Ÿæˆå¯è§†åŒ–
    if epoch % 10 == 0:
        monitor.plot_training_curves()
        monitor.plot_model_predictions(model, val_loader)
```

### æ£€æŸ¥ç‚¹ç®¡ç†

```python
from neuronal_network_v3.training import CheckpointManager

# åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
checkpoint_manager = CheckpointManager(
    save_dir='checkpoints/multi_channel/',
    save_top_k=3,  # ä¿å­˜æœ€å¥½çš„3ä¸ªæ¨¡å‹
    monitor_metric='val_loss',
    mode='min',
    save_every_n_epochs=5,
    save_last=True
)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for epoch in range(epochs):
    # ... è®­ç»ƒä»£ç  ...
    
    # éªŒè¯
    val_metrics = trainer.validate(val_loader)
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint_manager.save_checkpoint(
        epoch=epoch,
        model=model,
        optimizer=optimizer,
        scheduler=scheduler,
        metrics=val_metrics,
        config=training_config
    )
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœ
    if checkpoint_manager.should_stop_early(patience=15):
        print(f"Early stopping at epoch {epoch}")
        break

# åŠ è½½æœ€ä½³æ¨¡å‹
best_checkpoint = checkpoint_manager.load_best_checkpoint()
model.load_state_dict(best_checkpoint['model_state_dict'])

# æ–­ç‚¹ç»­è®­
if checkpoint_manager.has_checkpoint():
    checkpoint = checkpoint_manager.load_last_checkpoint()
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
    print(f"ä»epoch {start_epoch}ç»§ç»­è®­ç»ƒ")
```

## ğŸ”§ é«˜çº§è®­ç»ƒæŠ€æœ¯

### æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import GradScaler, autocast

class MixedPrecisionTrainer(BaseTrainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scaler = GradScaler()
        self.use_amp = True
    
    def train_step(self, batch):
        inputs, targets = batch
        
        with autocast(enabled=self.use_amp):
            outputs = self.model(inputs)
            loss = self.loss_fn(outputs, targets)
        
        # åå‘ä¼ æ’­
        self.scaler.scale(loss).backward()
        
        # æ¢¯åº¦è£å‰ª
        if self.gradient_clip_val > 0:
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
        
        # ä¼˜åŒ–å™¨æ­¥éª¤
        self.scaler.step(self.optimizer)
        self.scaler.update()
        self.optimizer.zero_grad()
        
        return loss.item()
```

### æ¢¯åº¦ç´¯ç§¯

```python
class GradientAccumulationTrainer(BaseTrainer):
    def __init__(self, accumulate_grad_batches=1, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.accumulate_grad_batches = accumulate_grad_batches
    
    def train_epoch(self, train_loader):
        self.model.train()
        total_loss = 0
        
        for batch_idx, batch in enumerate(train_loader):
            loss = self.train_step(batch)
            
            # æ ‡å‡†åŒ–æŸå¤±
            loss = loss / self.accumulate_grad_batches
            loss.backward()
            
            # æ¯accumulate_grad_batchesæ­¥æ›´æ–°ä¸€æ¬¡
            if (batch_idx + 1) % self.accumulate_grad_batches == 0:
                if self.gradient_clip_val > 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                
                self.optimizer.step()
                self.optimizer.zero_grad()
            
            total_loss += loss.item() * self.accumulate_grad_batches
        
        return total_loss / len(train_loader)
```

### è‡ªé€‚åº”å­¦ä¹ ç‡

```python
class AdaptiveLRTrainer(BaseTrainer):
    def __init__(self, patience=10, factor=0.5, min_lr=1e-7, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.patience = patience
        self.factor = factor
        self.min_lr = min_lr
        self.best_val_loss = float('inf')
        self.patience_counter = 0
    
    def adjust_learning_rate(self, val_loss):
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            self.patience_counter = 0
        else:
            self.patience_counter += 1
            
            if self.patience_counter >= self.patience:
                current_lr = self.optimizer.param_groups[0]['lr']
                new_lr = max(current_lr * self.factor, self.min_lr)
                
                if new_lr < current_lr:
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = new_lr
                    print(f"å­¦ä¹ ç‡é™ä½åˆ°: {new_lr:.2e}")
                    self.patience_counter = 0
```

### å¤šGPUè®­ç»ƒ

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

class DistributedTrainer(BaseTrainer):
    def __init__(self, local_rank, world_size, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.local_rank = local_rank
        self.world_size = world_size
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
        dist.init_process_group(backend='nccl')
        torch.cuda.set_device(local_rank)
        
        # åŒ…è£…æ¨¡å‹
        self.model = DDP(self.model, device_ids=[local_rank])
    
    def create_distributed_dataloader(self, dataset, batch_size, shuffle=True):
        sampler = DistributedSampler(
            dataset,
            num_replicas=self.world_size,
            rank=self.local_rank,
            shuffle=shuffle
        )
        
        return DataLoader(
            dataset,
            batch_size=batch_size,
            sampler=sampler,
            num_workers=4,
            pin_memory=True
        )
    
    def train_epoch(self, train_loader):
        # è®¾ç½®epochä»¥ç¡®ä¿æ•°æ®shuffle
        train_loader.sampler.set_epoch(self.current_epoch)
        
        return super().train_epoch(train_loader)
```

### è¯¾ç¨‹å­¦ä¹ 

```python
class CurriculumTrainer(BaseTrainer):
    def __init__(self, curriculum_config, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.curriculum_config = curriculum_config
        self.current_stage = 0
    
    def get_current_curriculum(self, epoch):
        """æ ¹æ®epochè·å–å½“å‰è¯¾ç¨‹è®¾ç½®"""
        for stage, config in enumerate(self.curriculum_config):
            if epoch < config['end_epoch']:
                return stage, config
        return len(self.curriculum_config) - 1, self.curriculum_config[-1]
    
    def train_epoch(self, train_loader):
        stage, config = self.get_current_curriculum(self.current_epoch)
        
        if stage != self.current_stage:
            print(f"åˆ‡æ¢åˆ°è¯¾ç¨‹é˜¶æ®µ {stage}: {config['description']}")
            self.current_stage = stage
            
            # è°ƒæ•´å­¦ä¹ ç‡
            if 'learning_rate' in config:
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = config['learning_rate']
            
            # è°ƒæ•´æŸå¤±æƒé‡
            if 'loss_weights' in config:
                self.loss_fn.update_weights(**config['loss_weights'])
        
        return super().train_epoch(train_loader)
```

### çŸ¥è¯†è’¸é¦

```python
class DistillationTrainer(BaseTrainer):
    def __init__(self, teacher_model, temperature=4.0, alpha=0.7, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.teacher_model = teacher_model
        self.teacher_model.eval()
        self.temperature = temperature
        self.alpha = alpha
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
    
    def distillation_loss(self, student_outputs, teacher_outputs, targets):
        # ç¡¬ç›®æ ‡æŸå¤±
        hard_loss = self.loss_fn(student_outputs, targets)
        
        # è½¯ç›®æ ‡æŸå¤±
        student_soft = F.log_softmax(student_outputs / self.temperature, dim=1)
        teacher_soft = F.softmax(teacher_outputs / self.temperature, dim=1)
        soft_loss = self.kl_loss(student_soft, teacher_soft) * (self.temperature ** 2)
        
        # ç»„åˆæŸå¤±
        total_loss = self.alpha * soft_loss + (1 - self.alpha) * hard_loss
        return total_loss
    
    def train_step(self, batch):
        inputs, targets = batch
        
        # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
        student_outputs = self.model(inputs)
        
        # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­
        with torch.no_grad():
            teacher_outputs = self.teacher_model(inputs)
        
        # è®¡ç®—è’¸é¦æŸå¤±
        loss = self.distillation_loss(student_outputs, teacher_outputs, targets)
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        if self.gradient_clip_val > 0:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
        
        self.optimizer.step()
        self.optimizer.zero_grad()
        
        return loss.item()
```

## ğŸ“Š è®­ç»ƒç­–ç•¥

### å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

```python
# 1. ä½™å¼¦é€€ç«
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=100, eta_min=1e-6
)

# 2. ä¸€å‘¨æœŸå­¦ä¹ ç‡
scheduler = torch.optim.lr_scheduler.OneCycleLR(
    optimizer, max_lr=1e-2, steps_per_epoch=len(train_loader), epochs=100
)

# 3. æŒ‡æ•°è¡°å‡
scheduler = torch.optim.lr_scheduler.ExponentialLR(
    optimizer, gamma=0.95
)

# 4. è‡ªé€‚åº”è°ƒæ•´
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-7
)

# 5. åˆ†æ®µå¸¸æ•°
scheduler = torch.optim.lr_scheduler.MultiStepLR(
    optimizer, milestones=[30, 60, 90], gamma=0.1
)
```

### æ•°æ®å¢å¼ºç­–ç•¥

```python
from neuronal_network_v3.data.transforms import *

# è®­ç»ƒæ—¶æ•°æ®å¢å¼º
train_transforms = Compose([
    RandomRotation(angle_range=(-180, 180)),
    RandomFlip(p=0.5),
    GaussianNoise(noise_std=0.1),
    RandomBrightness(factor_range=(0.8, 1.2)),
    RandomContrast(factor_range=(0.8, 1.2)),
    Normalize(mean=0.5, std=0.5)
])

# éªŒè¯æ—¶åªåšæ ‡å‡†åŒ–
val_transforms = Compose([
    Normalize(mean=0.5, std=0.5)
])
```

### æ­£åˆ™åŒ–æŠ€æœ¯

```python
# 1. Dropout
model = SigmaMUNet(
    in_channels=1,
    out_channels=5,
    dropout=0.1
)

# 2. æƒé‡è¡°å‡
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-3,
    weight_decay=1e-4
)

# 3. æ ‡ç­¾å¹³æ»‘
class LabelSmoothingLoss(nn.Module):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
    
    def forward(self, pred, target):
        # å®ç°æ ‡ç­¾å¹³æ»‘
        pass

# 4. æ¢¯åº¦æƒ©ç½š
def gradient_penalty(model, real_data, fake_data):
    # å®ç°æ¢¯åº¦æƒ©ç½š
    pass
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: è®­ç»ƒè¿‡ç¨‹ä¸­æŸå¤±ä¸ä¸‹é™æ€ä¹ˆåŠï¼Ÿ
A: æ’æŸ¥æ­¥éª¤ï¼š
1. æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦åˆé€‚
2. éªŒè¯æ•°æ®é¢„å¤„ç†
3. æ£€æŸ¥æ¨¡å‹æ¶æ„
4. ç¡®è®¤æŸå¤±å‡½æ•°å®ç°
5. æ£€æŸ¥æ¢¯åº¦æµåŠ¨
6. å°è¯•ä¸åŒçš„ä¼˜åŒ–å™¨

### Q: è®­ç»ƒé€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ
A: ä¼˜åŒ–æ–¹æ³•ï¼š
1. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
2. å¢åŠ batch size
3. ä½¿ç”¨å¤šGPUè®­ç»ƒ
4. ä¼˜åŒ–æ•°æ®åŠ è½½
5. ä½¿ç”¨æ›´å¿«çš„å­˜å‚¨
6. å‡å°‘ä¸å¿…è¦çš„è®¡ç®—

### Q: æ¨¡å‹è¿‡æ‹Ÿåˆæ€ä¹ˆåŠï¼Ÿ
A: è§£å†³æ–¹æ¡ˆï¼š
1. å¢åŠ æ•°æ®å¢å¼º
2. ä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯
3. å‡å°‘æ¨¡å‹å¤æ‚åº¦
4. æ—©åœè®­ç»ƒ
5. ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®
6. äº¤å‰éªŒè¯

### Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: è§£å†³æ–¹æ³•ï¼š
1. å‡å°‘batch size
2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
3. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
4. ä¼˜åŒ–æ¨¡å‹æ¶æ„
5. ä½¿ç”¨æ•°æ®å¹¶è¡Œ
6. æ¸…ç†ä¸å¿…è¦çš„å˜é‡

### Q: å¦‚ä½•è°ƒè¯•è®­ç»ƒè¿‡ç¨‹ï¼Ÿ
A: è°ƒè¯•æŠ€å·§ï¼š
1. å¯è§†åŒ–æŸå¤±æ›²çº¿
2. ç›‘æ§æ¢¯åº¦èŒƒæ•°
3. æ£€æŸ¥å­¦ä¹ ç‡å˜åŒ–
4. åˆ†æéªŒè¯æŒ‡æ ‡
5. ä½¿ç”¨tensorboard
6. ä¿å­˜ä¸­é—´ç»“æœ

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [æ¨¡å‹æ–‡æ¡£](../models/README.md)
- [æŸå¤±å‡½æ•°æ–‡æ¡£](../loss/README.md)
- [æ•°æ®å¤„ç†æ–‡æ¡£](../data/README.md)
- [è¯„ä¼°æ¨¡å—æ–‡æ¡£](../evaluation/README.md)
- [æ¨ç†æ¨¡å—æ–‡æ¡£](../inference/README.md)
- [å¤šé€šé“è®­ç»ƒæŒ‡å—](../README_MultiChannel.md)
- [è®­ç»ƒæœ€ä½³å®è·µ](./TRAINING_BEST_PRACTICES.md)