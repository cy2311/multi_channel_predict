# DECODEç¥ç»ç½‘ç»œv3 - å¤šé€šé“æ‰©å±•

åŸºäº`DECODE_Network_Analysis.md`æ–‡æ¡£å®ç°çš„å¤šé€šé“DECODEç½‘ç»œæ‰©å±•ï¼Œæ”¯æŒåŒé€šé“è”åˆæ¨ç†ã€ä¸ç¡®å®šæ€§é‡åŒ–å’Œç‰©ç†çº¦æŸã€‚

## ğŸš€ æ–°åŠŸèƒ½ç‰¹æ€§

### 1. å¤šé€šé“æ¶æ„
- **åŒé€šé“ç‹¬ç«‹ç½‘ç»œ**: ä¸¤ä¸ªç‹¬ç«‹çš„SigmaMUNetå¤„ç†ä¸åŒé€šé“æ•°æ®
- **æ¯”ä¾‹é¢„æµ‹ç½‘ç»œ**: RatioNeté¢„æµ‹å…‰å­æ•°åœ¨ä¸¤é€šé“é—´çš„åˆ†é…æ¯”ä¾‹
- **ä¸ç¡®å®šæ€§é‡åŒ–**: é¢„æµ‹æ¯”ä¾‹çš„å‡å€¼å’Œæ–¹å·®ï¼Œæä¾›ç½®ä¿¡åº¦ä¼°è®¡
- **ç‰¹å¾æå–å™¨**: ä»SigmaMUNetè¾“å‡ºä¸­æå–ç”¨äºæ¯”ä¾‹é¢„æµ‹çš„ç‰¹å¾

### 2. ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥
- **é˜¶æ®µ1**: åŒé€šé“ç‹¬ç«‹è®­ç»ƒ - åˆ†åˆ«è®­ç»ƒä¸¤ä¸ªé€šé“çš„SigmaMUNet
- **é˜¶æ®µ2**: æ¯”ä¾‹ç½‘ç»œè®­ç»ƒ - å†»ç»“é€šé“ç½‘ç»œï¼Œè®­ç»ƒRatioNet
- **é˜¶æ®µ3**: è”åˆå¾®è°ƒ - ç«¯åˆ°ç«¯ä¼˜åŒ–æ‰€æœ‰ç»„ä»¶

### 3. ç‰©ç†çº¦æŸ
- **å…‰å­æ•°å®ˆæ’**: ç¡®ä¿ä¸¤é€šé“å…‰å­æ•°ä¹‹å’Œç­‰äºæ€»å…‰å­æ•°
- **æ¯”ä¾‹ä¸€è‡´æ€§**: ä¿è¯ç›´æ¥é¢„æµ‹çš„æ¯”ä¾‹ä¸ä»å…‰å­æ•°è®¡ç®—çš„æ¯”ä¾‹ä¸€è‡´
- **çº¦æŸæŸå¤±**: åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­å¼ºåˆ¶æ‰§è¡Œç‰©ç†çº¦æŸ

### 4. é«˜çº§æŸå¤±å‡½æ•°
- **RatioGaussianNLLLoss**: åŸºäºé«˜æ–¯è´Ÿå¯¹æ•°ä¼¼ç„¶çš„æ¯”ä¾‹é¢„æµ‹æŸå¤±
- **MultiChannelLossWithGaussianRatio**: é›†æˆå¤šé€šé“æŸå¤±å’Œç‰©ç†çº¦æŸ
- **ä¸ç¡®å®šæ€§æ­£åˆ™åŒ–**: é˜²æ­¢è¿‡åº¦è‡ªä¿¡çš„é¢„æµ‹

### 5. å…¨é¢è¯„ä¼°ç³»ç»Ÿ
- **å¤šç»´åº¦è¯„ä¼°**: å•é€šé“æ€§èƒ½ã€æ¯”ä¾‹é¢„æµ‹ã€ç‰©ç†çº¦æŸè¯„ä¼°
- **ä¸ç¡®å®šæ€§è¯„ä¼°**: æ ¡å‡†æ€§ã€è¦†ç›–ç‡ã€é”åº¦åˆ†æ
- **å¯è§†åŒ–åˆ†æ**: è‡ªåŠ¨ç”Ÿæˆè¯„ä¼°å›¾è¡¨å’Œåˆ†ææŠ¥å‘Š

## ğŸ“ é¡¹ç›®ç»“æ„

```
neuronal_network_v3/
â”œâ”€â”€ ğŸ“ models/                     # ç¥ç»ç½‘ç»œæ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ ratio_net.py              # æ¯”ä¾‹é¢„æµ‹ç½‘ç»œ
â”‚   â”œâ”€â”€ sigma_munet.py            # Sigma MUNetæ¨¡å‹
â”‚   â”œâ”€â”€ double_munet.py           # åŒé€šé“MUNet
â”‚   â”œâ”€â”€ simple_smlm_net.py        # ç®€å•SMLMç½‘ç»œ
â”‚   â”œâ”€â”€ unet2d.py                 # 2D UNetåŸºç¡€æ¨¡å‹
â”‚   â””â”€â”€ __init__.py               # æ¨¡å‹æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ ğŸ“ loss/                       # æŸå¤±å‡½æ•°å®šä¹‰
â”‚   â”œâ”€â”€ ratio_loss.py             # æ¯”ä¾‹é¢„æµ‹æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ gaussian_mm_loss.py       # é«˜æ–¯æ··åˆæ¨¡å‹æŸå¤±
â”‚   â”œâ”€â”€ ppxyzb_loss.py            # PPXYZBæŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ unified_loss.py           # ç»Ÿä¸€æŸå¤±å‡½æ•°
â”‚   â””â”€â”€ __init__.py               # æŸå¤±å‡½æ•°æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ ğŸ“ trainer/                    # è®­ç»ƒå™¨æ¨¡å—
â”‚   â”œâ”€â”€ multi_channel_trainer.py  # å¤šé€šé“è®­ç»ƒå™¨
â”‚   â””â”€â”€ __init__.py               # è®­ç»ƒå™¨æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ ğŸ“ training/                   # è®­ç»ƒç›¸å…³å·¥å…·
â”‚   â”œâ”€â”€ trainer.py                # åŸºç¡€è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ dataset.py                # è®­ç»ƒæ•°æ®é›†
â”‚   â”œâ”€â”€ target_generator.py       # ç›®æ ‡ç”Ÿæˆå™¨
â”‚   â”œâ”€â”€ callbacks.py              # è®­ç»ƒå›è°ƒå‡½æ•°
â”‚   â”œâ”€â”€ utils.py                  # è®­ç»ƒå·¥å…·å‡½æ•°
â”‚   â””â”€â”€ __init__.py               # è®­ç»ƒæ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ ğŸ“ inference/                  # æ¨ç†å¼•æ“
â”‚   â”œâ”€â”€ multi_channel_infer.py    # å¤šé€šé“æ¨ç†å¼•æ“
â”‚   â”œâ”€â”€ infer.py                  # åŸºç¡€æ¨ç†å™¨
â”‚   â”œâ”€â”€ post_processing.py        # åå¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ result_parser.py          # ç»“æœè§£æå™¨
â”‚   â”œâ”€â”€ utils.py                  # æ¨ç†å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ __init__.py               # æ¨ç†æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ ğŸ“ evaluation/                 # è¯„ä¼°ç³»ç»Ÿ
â”‚   â”œâ”€â”€ multi_channel_evaluation.py # å¤šé€šé“è¯„ä¼°ç³»ç»Ÿ
â”‚   â”œâ”€â”€ evaluator.py              # åŸºç¡€è¯„ä¼°å™¨
â”‚   â”œâ”€â”€ analyzer.py               # ç»“æœåˆ†æå™¨
â”‚   â”œâ”€â”€ benchmark.py              # åŸºå‡†æµ‹è¯•
â”‚   â”œâ”€â”€ metrics.py                # è¯„ä¼°æŒ‡æ ‡
â”‚   â”œâ”€â”€ visualizer.py             # å¯è§†åŒ–å·¥å…·
â”‚   â””â”€â”€ __init__.py               # è¯„ä¼°æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ ğŸ“ data/                       # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ multi_channel_dataset.py  # å¤šé€šé“æ•°æ®é›†
â”‚   â”œâ”€â”€ dataset.py                # åŸºç¡€æ•°æ®é›†
â”‚   â”œâ”€â”€ transforms.py             # æ•°æ®å˜æ¢
â”‚   â””â”€â”€ __init__.py               # æ•°æ®æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ ğŸ“ utils/                      # å·¥å…·å‡½æ•°åº“ [ğŸ“– è¯¦ç»†æ–‡æ¡£](utils/README.md)
â”‚   â”œâ”€â”€ config_utils.py           # é…ç½®ç®¡ç†å·¥å…·
â”‚   â”œâ”€â”€ file_utils.py             # æ–‡ä»¶æ“ä½œå·¥å…·
â”‚   â”œâ”€â”€ data_utils.py             # æ•°æ®å¤„ç†å·¥å…·
â”‚   â”œâ”€â”€ plot_utils.py             # å¯è§†åŒ–å·¥å…·
â”‚   â”œâ”€â”€ math_utils.py             # æ•°å­¦è®¡ç®—å·¥å…·
â”‚   â”œâ”€â”€ log_utils.py              # æ—¥å¿—ç®¡ç†å·¥å…·
â”‚   â”œâ”€â”€ device_utils.py           # è®¾å¤‡ç®¡ç†å·¥å…·
â”‚   â””â”€â”€ __init__.py               # å·¥å…·æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ ğŸ“ checkpoints/                # æ¨¡å‹æ£€æŸ¥ç‚¹å­˜å‚¨
â”œâ”€â”€ ğŸ“ logs/                       # è®­ç»ƒæ—¥å¿—
â”œâ”€â”€ ğŸ“ results/                    # ç»“æœè¾“å‡º
â”œâ”€â”€ ğŸ“„ multi_channel_config.yaml   # å¤šé€šé“è®­ç»ƒé…ç½®
â”œâ”€â”€ ğŸ“„ training_config.yaml        # åŸºç¡€è®­ç»ƒé…ç½®
â”œâ”€â”€ ğŸ“„ train_multi_channel.py      # å¤šé€šé“è®­ç»ƒè„šæœ¬
â”œâ”€â”€ ğŸ“„ infer_multi_channel.py      # å¤šé€šé“æ¨ç†è„šæœ¬
â”œâ”€â”€ ğŸ“„ example_multi_channel.py    # å¤šé€šé“ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ ğŸ“„ test_multi_channel.py       # å¤šé€šé“æµ‹è¯•è„šæœ¬
â”œâ”€â”€ ğŸ“„ train.py                    # åŸºç¡€è®­ç»ƒè„šæœ¬
â”œâ”€â”€ ğŸ“„ submit_training.sh          # SLURMè®­ç»ƒæäº¤è„šæœ¬
â”œâ”€â”€ ğŸ“„ submit_training_resume.sh   # SLURMæ¢å¤è®­ç»ƒè„šæœ¬
â”œâ”€â”€ ğŸ“„ DECODE_Network_Analysis.md  # ç½‘ç»œæ¶æ„åˆ†ææ–‡æ¡£
â”œâ”€â”€ ğŸ“„ neuronal_network_architecture.md # ç¥ç»ç½‘ç»œæ¶æ„æ–‡æ¡£
â”œâ”€â”€ ğŸ“„ README_MultiChannel.md      # å¤šé€šé“æ‰©å±•æ–‡æ¡£ï¼ˆæœ¬æ–‡æ¡£ï¼‰
â””â”€â”€ ğŸ“„ __init__.py                 # ä¸»æ¨¡å—åˆå§‹åŒ–
```

## ğŸ“š æ¨¡å—æ–‡æ¡£

æœ¬é¡¹ç›®åŒ…å«å¤šä¸ªä¸“ä¸šæ¨¡å—ï¼Œæ¯ä¸ªæ¨¡å—éƒ½æœ‰è¯¦ç»†çš„æ–‡æ¡£è¯´æ˜ï¼š

- **[ğŸ§  Models æ¨¡å‹æ¨¡å—](models/README.md)** - DECODEç¥ç»ç½‘ç»œæ¨¡å‹å®šä¹‰å’Œæ¶æ„
- **[ğŸ“Š Loss æŸå¤±å‡½æ•°æ¨¡å—](loss/README.md)** - å¤šé€šé“æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç›®æ ‡
- **[ğŸ“ Data æ•°æ®æ¨¡å—](data/README.md)** - æ•°æ®åŠ è½½ã€å¤„ç†å’Œå˜æ¢
- **[ğŸ” Inference æ¨ç†æ¨¡å—](inference/README.md)** - æ¨¡å‹æ¨ç†å’Œåå¤„ç†
- **[ğŸ“ˆ Evaluation è¯„ä¼°æ¨¡å—](evaluation/README.md)** - æ€§èƒ½è¯„ä¼°å’ŒæŒ‡æ ‡è®¡ç®—
- **[ğŸ¯ Training è®­ç»ƒæ¨¡å—](training/README.md)** - æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–ç­–ç•¥
- **[ğŸ› ï¸ Utils å·¥å…·æ¨¡å—](utils/README.md)** - é€šç”¨å·¥å…·å’Œå®ç”¨ç¨‹åº

## ğŸ› ï¸ å®‰è£…å’Œç¯å¢ƒ

### ä¾èµ–è¦æ±‚
```bash
# åŸºç¡€ä¾èµ–
pip install torch torchvision
pip install numpy scipy matplotlib
pip install h5py pyyaml tqdm

# å¯é€‰ä¾èµ–ï¼ˆç”¨äºé«˜çº§åŠŸèƒ½ï¼‰
pip install tensorboard wandb
pip install scikit-learn seaborn
```

### ç¯å¢ƒé…ç½®
```python
# è®¾ç½®PYTHONPATH
export PYTHONPATH="/path/to/DECODE_rewrite:$PYTHONPATH"

# æˆ–åœ¨Pythonä¸­
import sys
sys.path.append('/path/to/DECODE_rewrite')
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ•°æ®å‡†å¤‡

å¤šé€šé“æ•°æ®åº”åŒ…å«ä»¥ä¸‹ç»“æ„ï¼š

```python
# HDF5æ ¼å¼
data_file.h5:
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ channel1/
â”‚   â”‚   â”œâ”€â”€ images      # é€šé“1è¾“å…¥å›¾åƒ [N, H, W]
â”‚   â”‚   â”œâ”€â”€ targets     # é€šé“1ç›®æ ‡ [N, C, H, W]
â”‚   â”‚   â””â”€â”€ photons     # é€šé“1å…‰å­æ•° [N]
â”‚   â”œâ”€â”€ channel2/
â”‚   â”‚   â”œâ”€â”€ images      # é€šé“2è¾“å…¥å›¾åƒ [N, H, W]
â”‚   â”‚   â”œâ”€â”€ targets     # é€šé“2ç›®æ ‡ [N, C, H, W]
â”‚   â”‚   â””â”€â”€ photons     # é€šé“2å…‰å­æ•° [N]
â”‚   â””â”€â”€ metadata        # å…ƒæ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
â”œâ”€â”€ val/
â”‚   â””â”€â”€ ...
â””â”€â”€ test/
    â””â”€â”€ ...
```

### 2. é…ç½®æ–‡ä»¶

å¤åˆ¶å¹¶ä¿®æ”¹`multi_channel_config.yaml`ï¼š

```yaml
# åŸºæœ¬é…ç½®ç¤ºä¾‹
model:
  n_inp: 1
  n_out: 10
  sigma_munet_params:
    depth: 3
    initial_features: 64
  ratio_net:
    hidden_dim: 128
    num_layers: 3

data:
  train_path: "data/train_multi_channel.h5"
  val_path: "data/val_multi_channel.h5"
  patch_size: 64
  
training:
  stage1_epochs: 100
  stage2_epochs: 50
  stage3_epochs: 30
  batch_size: 16
```

### 3. è®­ç»ƒæ¨¡å‹

```bash
# å®Œæ•´ä¸‰é˜¶æ®µè®­ç»ƒ
python train_multi_channel.py --config multi_channel_config.yaml

# å•ç‹¬è®­ç»ƒæŸä¸ªé˜¶æ®µ
python train_multi_channel.py --config multi_channel_config.yaml --stage 1

# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
python train_multi_channel.py --config multi_channel_config.yaml --resume outputs/stage2_models.pth
```

### 4. æ¨¡å‹æ¨ç†

```bash
# åŸºæœ¬æ¨ç†
python infer_multi_channel.py \
    --model outputs/stage3_models.pth \
    --input data/test_images.h5 \
    --output results/

# å¸¦è¯„ä¼°å’Œå¯è§†åŒ–çš„æ¨ç†
python infer_multi_channel.py \
    --model outputs/stage3_models.pth \
    --input data/test_images.h5 \
    --output results/ \
    --apply-constraints \
    --evaluate \
    --visualize
```

## ğŸ“Š APIä½¿ç”¨ç¤ºä¾‹

### 1. å¤šé€šé“è®­ç»ƒ

```python
from neuronal_network_v3.trainer.multi_channel_trainer import MultiChannelTrainer
from neuronal_network_v3.data.multi_channel_dataset import MultiChannelDataModule

# åŠ è½½é…ç½®
with open('multi_channel_config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# åˆå§‹åŒ–æ•°æ®æ¨¡å—
data_module = MultiChannelDataModule(config)
data_module.setup()

# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = MultiChannelTrainer(config, device='cuda')

# æ‰§è¡Œä¸‰é˜¶æ®µè®­ç»ƒ
results = trainer.train_full_pipeline(
    train_loader=data_module.train_loader,
    val_loader=data_module.val_loader,
    save_dir='outputs/'
)
```

### 2. å¤šé€šé“æ¨ç†

```python
from neuronal_network_v3.inference.multi_channel_infer import MultiChannelInfer
from neuronal_network_v3.models.sigma_munet import SigmaMUNet
from neuronal_network_v3.models.ratio_net import RatioNet

# åŠ è½½æ¨¡å‹
channel1_net = SigmaMUNet(n_inp=1, n_out=10)
channel2_net = SigmaMUNet(n_inp=1, n_out=10)
ratio_net = RatioNet(input_channels=20, hidden_dim=128)

# åŠ è½½æƒé‡
checkpoint = torch.load('outputs/stage3_models.pth')
channel1_net.load_state_dict(checkpoint['channel1_net'])
channel2_net.load_state_dict(checkpoint['channel2_net'])
ratio_net.load_state_dict(checkpoint['ratio_net'])

# åˆå§‹åŒ–æ¨ç†å™¨
inferrer = MultiChannelInfer(
    channel1_net=channel1_net,
    channel2_net=channel2_net,
    ratio_net=ratio_net,
    apply_conservation=True,
    apply_consistency=True
)

# æ¨ç†
results = inferrer.predict(channel1_images, channel2_images)
print(f"Predicted ratios: {results['ratio_mean']}")
print(f"Uncertainties: {results['ratio_std']}")
```

### 3. è¯„ä¼°å’Œå¯è§†åŒ–

```python
from neuronal_network_v3.evaluation.multi_channel_evaluation import MultiChannelEvaluation

# åˆå§‹åŒ–è¯„ä¼°å™¨
evaluator = MultiChannelEvaluation(device='cuda')

# è¯„ä¼°
metrics = evaluator.evaluate(pred_results, ground_truth)
print(f"Ratio MAE: {metrics['ratio']['mae']:.4f}")
print(f"Coverage 95%: {metrics['ratio']['coverage_95']:.4f}")
print(f"Conservation error: {metrics['conservation']['conservation_error']:.4f}")

# å¯è§†åŒ–
fig = evaluator.visualize_results(
    pred_results, ground_truth,
    save_path='evaluation_plots.png'
)
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰æŸå¤±å‡½æ•°

```python
from neuronal_network_v3.loss.ratio_loss import RatioGaussianNLLLoss

# åˆ›å»ºè‡ªå®šä¹‰æ¯”ä¾‹æŸå¤±
ratio_loss = RatioGaussianNLLLoss(
    photon_conservation_weight=1.0,
    ratio_consistency_weight=0.5,
    uncertainty_regularization=0.1
)

# è®¡ç®—æŸå¤±
loss = ratio_loss(ratio_mean, ratio_std, true_ratio)
```

### 2. æ‰¹é‡æ¨ç†ä¼˜åŒ–

```python
from neuronal_network_v3.inference.multi_channel_infer import MultiChannelBatchInfer

# è‡ªåŠ¨æ‰¹å¤§å°ä¼˜åŒ–
batch_inferrer = MultiChannelBatchInfer(
    channel1_net=channel1_net,
    channel2_net=channel2_net,
    ratio_net=ratio_net,
    auto_batch_size=True,
    max_memory_gb=8.0
)

# å¤§è§„æ¨¡æ•°æ®æ¨ç†
results = batch_inferrer.predict_large_dataset(large_dataset)
```

### 3. ä¸ç¡®å®šæ€§åˆ†æ

```python
from neuronal_network_v3.evaluation.multi_channel_evaluation import RatioEvaluationMetrics

# æ ¡å‡†æ€§åˆ†æ
calibration_metrics = RatioEvaluationMetrics.compute_calibration_metrics(
    pred_mean, pred_std, true_values
)
print(f"Expected Calibration Error: {calibration_metrics['ece']:.4f}")

# é”åº¦åˆ†æ
sharpness_metrics = RatioEvaluationMetrics.compute_sharpness(pred_std)
print(f"Mean uncertainty: {sharpness_metrics['mean_uncertainty']:.4f}")
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜ä½¿ç”¨
- è‡ªåŠ¨æ‰¹å¤§å°è°ƒæ•´
- æ··åˆç²¾åº¦è®­ç»ƒ

### 2. è®¡ç®—ä¼˜åŒ–
- GPUå¹¶è¡Œå¤„ç†
- å¼‚æ­¥æ•°æ®åŠ è½½
- æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–

### 3. è®­ç»ƒæŠ€å·§
- å­¦ä¹ ç‡è°ƒåº¦
- æ—©åœæœºåˆ¶
- æ¨¡å‹é›†æˆ

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**
   ```python
   # å‡å°‘æ‰¹å¤§å°
   config['training']['batch_size'] = 8
   
   # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
   config['hardware']['gradient_checkpointing'] = True
   ```

2. **è®­ç»ƒä¸ç¨³å®š**
   ```python
   # é™ä½å­¦ä¹ ç‡
   config['training']['optimizer']['lr'] = 1e-4
   
   # å¢åŠ æ¢¯åº¦è£å‰ª
   config['hardware']['gradient_clipping'] = 0.5
   ```

3. **ç‰©ç†çº¦æŸè¿å**
   ```python
   # å¢åŠ çº¦æŸæƒé‡
   config['loss']['joint_params']['conservation_weight'] = 1.0
   config['loss']['joint_params']['consistency_weight'] = 0.5
   ```

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.getLogger().setLevel(logging.DEBUG)

# æ£€æŸ¥æ•°æ®ç»Ÿè®¡
data_stats = dataset.get_statistics()
print(f"Data statistics: {data_stats}")

# éªŒè¯ç‰©ç†çº¦æŸ
conservation_error = torch.abs(pred_ch1_photons + pred_ch2_photons - total_photons)
print(f"Conservation error: {conservation_error.mean():.4f}")
```

## ğŸ¯ æœ€ä½³å®è·µ

### æ•°æ®ç®¡ç†
- ä½¿ç”¨HDF5æ ¼å¼å­˜å‚¨å¤§å‹æ•°æ®é›†
- å®æ–½æ•°æ®ç‰ˆæœ¬æ§åˆ¶å’Œæ ¡éªŒ
- å®šæœŸå¤‡ä»½é‡è¦æ•°æ®å’Œæ¨¡å‹
- ä½¿ç”¨æ•°æ®ç¼“å­˜æé«˜è®­ç»ƒæ•ˆç‡

### æ¨¡å‹å¼€å‘
- éµå¾ªæ¨¡å—åŒ–è®¾è®¡åŸåˆ™
- ä½¿ç”¨é…ç½®æ–‡ä»¶ç®¡ç†è¶…å‚æ•°
- å®æ–½å…¨é¢çš„å•å…ƒæµ‹è¯•
- è®°å½•æ¨¡å‹æ¶æ„å’Œè®­ç»ƒè¿‡ç¨‹

### è®­ç»ƒç­–ç•¥
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒèŠ‚çœå†…å­˜
- å®æ–½æ—©åœå’Œå­¦ä¹ ç‡è°ƒåº¦
- å®šæœŸéªŒè¯å’Œä¿å­˜æ£€æŸ¥ç‚¹
- ç›‘æ§è®­ç»ƒæŒ‡æ ‡å’Œèµ„æºä½¿ç”¨

### ä»£ç è´¨é‡
- éµå¾ªPEP 8ä»£ç è§„èŒƒ
- ç¼–å†™æ¸…æ™°çš„æ–‡æ¡£å­—ç¬¦ä¸²
- ä½¿ç”¨ç±»å‹æç¤ºæé«˜ä»£ç å¯è¯»æ€§
- å®æ–½ä»£ç å®¡æŸ¥æµç¨‹

## ğŸ”§ é¡¹ç›®ç®¡ç†

### ç›®å½•ç»“æ„å»ºè®®
```
project_workspace/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/              # å¤„ç†åæ•°æ®
â”‚   â””â”€â”€ cache/                  # æ•°æ®ç¼“å­˜
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ exp_001_baseline/       # å®éªŒ1ï¼šåŸºçº¿æ¨¡å‹
â”‚   â”œâ”€â”€ exp_002_multi_channel/  # å®éªŒ2ï¼šå¤šé€šé“æ¨¡å‹
â”‚   â””â”€â”€ exp_003_optimization/   # å®éªŒ3ï¼šä¼˜åŒ–ç‰ˆæœ¬
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ checkpoints/            # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ final/                  # æœ€ç»ˆæ¨¡å‹
â”‚   â””â”€â”€ pretrained/             # é¢„è®­ç»ƒæ¨¡å‹
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/                # ç»“æœå›¾è¡¨
â”‚   â”œâ”€â”€ reports/                # åˆ†ææŠ¥å‘Š
â”‚   â””â”€â”€ metrics/                # è¯„ä¼°æŒ‡æ ‡
â””â”€â”€ logs/
    â”œâ”€â”€ training/               # è®­ç»ƒæ—¥å¿—
    â”œâ”€â”€ inference/              # æ¨ç†æ—¥å¿—
    â””â”€â”€ system/                 # ç³»ç»Ÿæ—¥å¿—
```

### å®éªŒç®¡ç†
- ä¸ºæ¯ä¸ªå®éªŒåˆ›å»ºç‹¬ç«‹ç›®å½•
- è®°å½•å®éªŒé…ç½®å’Œç»“æœ
- ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶è·Ÿè¸ªä»£ç å˜æ›´
- å»ºç«‹å®éªŒç»“æœå¯¹æ¯”æœºåˆ¶

### æ€§èƒ½ç›‘æ§
- ç›‘æ§GPU/CPUä½¿ç”¨ç‡
- è·Ÿè¸ªå†…å­˜æ¶ˆè€—æƒ…å†µ
- è®°å½•è®­ç»ƒæ—¶é—´å’Œæ”¶æ•›æ€§
- å»ºç«‹æ€§èƒ½åŸºå‡†æµ‹è¯•

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. DECODEåŸå§‹è®ºæ–‡: [Deep learning enables fast and dense single-molecule localization with high accuracy](https://www.nature.com/articles/s41592-021-01236-x)
2. ä¸ç¡®å®šæ€§é‡åŒ–: [What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?](https://arxiv.org/abs/1703.04977)
3. ç‰©ç†çº¦æŸå­¦ä¹ : [Physics-informed neural networks](https://www.sciencedirect.com/science/article/pii/S0021999118307125)
4. å¤šä»»åŠ¡å­¦ä¹ : [Multi-Task Learning Using Uncertainty to Weigh Losses](https://arxiv.org/abs/1705.07115)
5. æ·±åº¦å­¦ä¹ æœ€ä½³å®è·µ: [Deep Learning Best Practices](https://www.deeplearningbook.org/)

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ”¹è¿›å»ºè®®ï¼

### è´¡çŒ®æµç¨‹
1. Forké¡¹ç›®åˆ°ä¸ªäººä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. åˆ›å»ºPull Request

### ä»£ç è´¡çŒ®è§„èŒƒ
- éµå¾ªç°æœ‰ä»£ç é£æ ¼
- æ·»åŠ é€‚å½“çš„æµ‹è¯•ç”¨ä¾‹
- æ›´æ–°ç›¸å…³æ–‡æ¡£
- ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡

### é—®é¢˜æŠ¥å‘Š
- ä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜æè¿°é—®é¢˜
- æä¾›è¯¦ç»†çš„é‡ç°æ­¥éª¤
- åŒ…å«ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
- é™„ä¸Šç›¸å…³çš„é”™è¯¯æ—¥å¿—

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªåŸDECODEé¡¹ç›®çš„è®¸å¯è¯æ¡æ¬¾ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒLICENSEæ–‡ä»¶ã€‚

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- åˆ›å»ºGitHub Issueï¼ˆæ¨èï¼‰
- å‘é€é‚®ä»¶è‡³é¡¹ç›®ç»´æŠ¤è€…
- å‚ä¸é¡¹ç›®è®¨è®ºåŒº

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹è´¡çŒ®è€…å’Œé¡¹ç›®ï¼š
- åŸDECODEé¡¹ç›®å›¢é˜Ÿ
- PyTorchç¤¾åŒº
- ç§‘å­¦è®¡ç®—Pythonç”Ÿæ€ç³»ç»Ÿ
- æ‰€æœ‰æµ‹è¯•ç”¨æˆ·å’Œåé¦ˆæä¾›è€…

---

**æ³¨æ„**: è¿™æ˜¯åŸºäº`DECODE_Network_Analysis.md`æ–‡æ¡£å®ç°çš„å¤šé€šé“æ‰©å±•ã€‚ä½¿ç”¨å‰è¯·ç¡®ä¿ç†è§£ç›¸å…³çš„ç†è®ºåŸºç¡€å’Œå®ç°ç»†èŠ‚ã€‚å»ºè®®å…ˆé˜…è¯»å„æ¨¡å—çš„è¯¦ç»†æ–‡æ¡£ï¼Œç„¶åæ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„åŠŸèƒ½ç»„ä»¶ã€‚