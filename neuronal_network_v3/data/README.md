# Data æ•°æ®å¤„ç†æ¨¡å—

æœ¬æ¨¡å—åŒ…å«DECODEç¥ç»ç½‘ç»œv3çš„æ•°æ®å¤„ç†ã€åŠ è½½å’Œå˜æ¢åŠŸèƒ½ï¼Œæ”¯æŒå•é€šé“å’Œå¤šé€šé“SMLMæ•°æ®çš„é«˜æ•ˆå¤„ç†ã€‚

## ğŸ“‹ æ¨¡å—æ¦‚è§ˆ

### æ ¸å¿ƒç»„ä»¶

#### ğŸ”¹ MultiChannelSMLMDataset (`multi_channel_dataset.py`)
- **åŠŸèƒ½**: å¤šé€šé“SMLMæ•°æ®é›†å¤„ç†
- **ç‰¹ç‚¹**:
  - æ”¯æŒHDF5æ ¼å¼æ•°æ®
  - åŒé€šé“æ•°æ®åŒæ­¥åŠ è½½
  - è‡ªåŠ¨æ•°æ®éªŒè¯å’Œæ¸…ç†
  - çµæ´»çš„æ•°æ®åˆ†å‰²ç­–ç•¥
- **ç”¨é€”**: å¤šé€šé“è®­ç»ƒå’Œæ¨ç†çš„ä¸»è¦æ•°æ®æ¥å£

#### ğŸ”¹ SMLMDataset (`dataset.py`)
- **åŠŸèƒ½**: åŸºç¡€SMLMæ•°æ®é›†ç±»
- **ç‰¹ç‚¹**:
  - æ ‡å‡†åŒ–çš„æ•°æ®æ¥å£
  - å†…å­˜é«˜æ•ˆçš„æ•°æ®åŠ è½½
  - æ”¯æŒå¤šç§æ•°æ®æ ¼å¼
  - å¯é…ç½®çš„é¢„å¤„ç†æµæ°´çº¿
- **ç”¨é€”**: å•é€šé“æ•°æ®å¤„ç†å’ŒåŸºç¡€æ•°æ®æ“ä½œ

#### ğŸ”¹ æ•°æ®å˜æ¢ (`transforms.py`)
- **åŠŸèƒ½**: æ•°æ®å¢å¼ºå’Œé¢„å¤„ç†å˜æ¢
- **ç‰¹ç‚¹**:
  - ä¸“ä¸ºSMLMæ•°æ®è®¾è®¡çš„å˜æ¢
  - æ”¯æŒå‡ ä½•å’Œå¼ºåº¦å˜æ¢
  - ä¿æŒç‰©ç†ä¸€è‡´æ€§
  - å¯ç»„åˆçš„å˜æ¢æµæ°´çº¿
- **ç”¨é€”**: æ•°æ®å¢å¼ºã€å½’ä¸€åŒ–å’Œé¢„å¤„ç†

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### å¤šé€šé“æ•°æ®é›†ä½¿ç”¨

```python
from neuronal_network_v3.data import MultiChannelSMLMDataset, create_multi_channel_dataloader
from neuronal_network_v3.data.transforms import get_default_transforms

# åˆ›å»ºæ•°æ®é›†
dataset = MultiChannelSMLMDataset(
    data_path='data/multi_channel_train.h5',
    mode='train',
    patch_size=64,
    transform=get_default_transforms(mode='train')
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
dataloader = create_multi_channel_dataloader(
    data_path='data/multi_channel_train.h5',
    config={
        'batch_size': 16,
        'num_workers': 4,
        'patch_size': 64
    },
    mode='train'
)

# æ•°æ®è¿­ä»£
for batch in dataloader:
    ch1_images = batch['channel1']['images']    # [B, 1, H, W]
    ch2_images = batch['channel2']['images']    # [B, 1, H, W]
    ch1_targets = batch['channel1']['targets']  # [B, C, H, W]
    ch2_targets = batch['channel2']['targets']  # [B, C, H, W]
    metadata = batch['metadata']                # å…ƒæ•°æ®ä¿¡æ¯
```

### å•é€šé“æ•°æ®é›†ä½¿ç”¨

```python
from neuronal_network_v3.data import SMLMDataset
from torch.utils.data import DataLoader

# åˆ›å»ºæ•°æ®é›†
dataset = SMLMDataset(
    data_path='data/single_channel_train.h5',
    mode='train',
    patch_size=64,
    augment=True
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
dataloader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    pin_memory=True
)

# æ•°æ®è¿­ä»£
for images, targets in dataloader:
    # images: [B, 1, H, W]
    # targets: [B, C, H, W]
    pass
```

### æ•°æ®å˜æ¢ä½¿ç”¨

```python
from neuronal_network_v3.data.transforms import (
    RandomRotation, RandomFlip, GaussianNoise, 
    Normalize, Compose
)

# åˆ›å»ºå˜æ¢æµæ°´çº¿
transforms = Compose([
    RandomRotation(angle_range=(-180, 180)),  # éšæœºæ—‹è½¬
    RandomFlip(p=0.5),                        # éšæœºç¿»è½¬
    GaussianNoise(noise_std=0.1),             # é«˜æ–¯å™ªå£°
    Normalize(mean=0.5, std=0.2)              # å½’ä¸€åŒ–
])

# åº”ç”¨å˜æ¢
transformed_data = transforms({
    'image': image,
    'target': target,
    'metadata': metadata
})
```

## ğŸ“Š æ•°æ®æ ¼å¼è§„èŒƒ

### HDF5æ•°æ®ç»“æ„

```
data_file.h5
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ channel1/
â”‚   â”‚   â”œâ”€â”€ images          # [N, H, W] è¾“å…¥å›¾åƒ
â”‚   â”‚   â”œâ”€â”€ targets         # [N, C, H, W] ç›®æ ‡æ•°æ®
â”‚   â”‚   â”œâ”€â”€ photons         # [N] å…‰å­æ•°ä¿¡æ¯
â”‚   â”‚   â””â”€â”€ positions       # [N, M, 3] å‘å°„ä½“ä½ç½® (x, y, z)
â”‚   â”œâ”€â”€ channel2/
â”‚   â”‚   â”œâ”€â”€ images          # [N, H, W] è¾“å…¥å›¾åƒ
â”‚   â”‚   â”œâ”€â”€ targets         # [N, C, H, W] ç›®æ ‡æ•°æ®
â”‚   â”‚   â”œâ”€â”€ photons         # [N] å…‰å­æ•°ä¿¡æ¯
â”‚   â”‚   â””â”€â”€ positions       # [N, M, 3] å‘å°„ä½“ä½ç½®
â”‚   â””â”€â”€ metadata
â”‚       â”œâ”€â”€ pixel_size      # åƒç´ å°ºå¯¸ (nm)
â”‚       â”œâ”€â”€ wavelength      # æ³¢é•¿ä¿¡æ¯
â”‚       â”œâ”€â”€ na              # æ•°å€¼å­”å¾„
â”‚       â””â”€â”€ acquisition_params  # é‡‡é›†å‚æ•°
â”œâ”€â”€ val/
â”‚   â””â”€â”€ ... (åŒtrainç»“æ„)
â””â”€â”€ test/
    â””â”€â”€ ... (åŒtrainç»“æ„)
```

### ç›®æ ‡æ•°æ®æ ¼å¼

ç›®æ ‡æ•°æ®é€šå¸¸åŒ…å«ä»¥ä¸‹é€šé“ï¼š
- **é€šé“0**: æ£€æµ‹æ¦‚ç‡å›¾
- **é€šé“1-2**: X, Yä½ç½®åç§»
- **é€šé“3**: Zä½ç½®ä¿¡æ¯
- **é€šé“4**: äº®åº¦ä¿¡æ¯
- **é€šé“5-6**: X, Yä½ç½®ä¸ç¡®å®šæ€§
- **é€šé“7**: Zä½ç½®ä¸ç¡®å®šæ€§
- **é€šé“8**: äº®åº¦ä¸ç¡®å®šæ€§
- **é€šé“9**: èƒŒæ™¯ä¿¡æ¯

## âš™ï¸ æ•°æ®é…ç½®

### æ•°æ®é›†å‚æ•°

```python
# MultiChannelSMLMDataseté…ç½®
dataset_config = {
    'data_path': 'path/to/data.h5',
    'mode': 'train',           # 'train', 'val', 'test'
    'patch_size': 64,          # å›¾åƒå—å¤§å°
    'overlap': 0.1,            # å›¾åƒå—é‡å æ¯”ä¾‹
    'min_photons': 100,        # æœ€å°å…‰å­æ•°é˜ˆå€¼
    'max_photons': 10000,      # æœ€å¤§å…‰å­æ•°é˜ˆå€¼
    'normalize': True,         # æ˜¯å¦å½’ä¸€åŒ–
    'augment': True,           # æ˜¯å¦æ•°æ®å¢å¼º
    'cache_size': 1000         # ç¼“å­˜å¤§å°
}
```

### æ•°æ®åŠ è½½å™¨å‚æ•°

```python
# DataLoaderé…ç½®
loader_config = {
    'batch_size': 16,          # æ‰¹å¤§å°
    'num_workers': 4,          # å·¥ä½œè¿›ç¨‹æ•°
    'pin_memory': True,        # æ˜¯å¦å›ºå®šå†…å­˜
    'drop_last': True,         # æ˜¯å¦ä¸¢å¼ƒæœ€åä¸å®Œæ•´æ‰¹æ¬¡
    'persistent_workers': True, # æ˜¯å¦ä¿æŒå·¥ä½œè¿›ç¨‹
    'prefetch_factor': 2       # é¢„å–å› å­
}
```

## ğŸ”§ æ•°æ®å˜æ¢è¯¦è§£

### å‡ ä½•å˜æ¢

#### RandomRotation
```python
# éšæœºæ—‹è½¬å˜æ¢
rotation = RandomRotation(
    angle_range=(-180, 180),   # æ—‹è½¬è§’åº¦èŒƒå›´
    p=0.8,                     # åº”ç”¨æ¦‚ç‡
    interpolation='bilinear'   # æ’å€¼æ–¹æ³•
)
```

#### RandomFlip
```python
# éšæœºç¿»è½¬å˜æ¢
flip = RandomFlip(
    horizontal=True,           # æ°´å¹³ç¿»è½¬
    vertical=True,             # å‚ç›´ç¿»è½¬
    p=0.5                      # åº”ç”¨æ¦‚ç‡
)
```

#### RandomCrop
```python
# éšæœºè£å‰ªå˜æ¢
crop = RandomCrop(
    size=(64, 64),             # è£å‰ªå°ºå¯¸
    padding=4,                 # å¡«å……å¤§å°
    padding_mode='reflect'     # å¡«å……æ¨¡å¼
)
```

### å¼ºåº¦å˜æ¢

#### GaussianNoise
```python
# é«˜æ–¯å™ªå£°å˜æ¢
noise = GaussianNoise(
    noise_std=0.1,             # å™ªå£°æ ‡å‡†å·®
    p=0.7                      # åº”ç”¨æ¦‚ç‡
)
```

#### RandomBrightness
```python
# éšæœºäº®åº¦å˜æ¢
brightness = RandomBrightness(
    factor_range=(0.8, 1.2),   # äº®åº¦å› å­èŒƒå›´
    p=0.6                      # åº”ç”¨æ¦‚ç‡
)
```

#### PoissonNoise
```python
# æ³Šæ¾å™ªå£°å˜æ¢ï¼ˆæ¨¡æ‹Ÿå…‰å­å™ªå£°ï¼‰
poisson = PoissonNoise(
    scale_factor=1.0,          # ç¼©æ”¾å› å­
    p=0.5                      # åº”ç”¨æ¦‚ç‡
)
```

### å½’ä¸€åŒ–å˜æ¢

#### Normalize
```python
# æ ‡å‡†åŒ–å˜æ¢
normalize = Normalize(
    mean=0.5,                  # å‡å€¼
    std=0.2,                   # æ ‡å‡†å·®
    per_channel=True           # æ˜¯å¦æŒ‰é€šé“å½’ä¸€åŒ–
)
```

#### ZScoreNormalize
```python
# Z-scoreå½’ä¸€åŒ–
zscore = ZScoreNormalize(
    eps=1e-8                   # æ•°å€¼ç¨³å®šæ€§å‚æ•°
)
```

## ğŸš€ é«˜çº§åŠŸèƒ½

### æ•°æ®ç¼“å­˜

```python
# å¯ç”¨æ•°æ®ç¼“å­˜ä»¥æé«˜è®­ç»ƒé€Ÿåº¦
dataset = MultiChannelSMLMDataset(
    data_path='data/large_dataset.h5',
    cache_size=5000,           # ç¼“å­˜æ ·æœ¬æ•°
    cache_mode='memory',       # 'memory' æˆ– 'disk'
    cache_dir='/tmp/cache'     # ç£ç›˜ç¼“å­˜ç›®å½•
)
```

### æ•°æ®éªŒè¯

```python
# æ•°æ®å®Œæ•´æ€§éªŒè¯
from neuronal_network_v3.data.validation import DataValidator

validator = DataValidator()
validation_report = validator.validate_dataset(
    data_path='data/dataset.h5',
    check_consistency=True,    # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
    check_completeness=True,   # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    check_quality=True         # æ£€æŸ¥æ•°æ®è´¨é‡
)

print(validation_report)
```

### æ•°æ®ç»Ÿè®¡

```python
# è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
stats = dataset.get_statistics()
print(f"æ•°æ®é›†å¤§å°: {stats['size']}")
print(f"å›¾åƒå°ºå¯¸: {stats['image_shape']}")
print(f"å…‰å­æ•°èŒƒå›´: {stats['photon_range']}")
print(f"å‘å°„ä½“å¯†åº¦: {stats['emitter_density']}")
```

### è‡ªå®šä¹‰æ•°æ®å˜æ¢

```python
from neuronal_network_v3.data.transforms import BaseTransform

class CustomTransform(BaseTransform):
    def __init__(self, param1, param2):
        super().__init__()
        self.param1 = param1
        self.param2 = param2
    
    def __call__(self, data):
        image = data['image']
        target = data['target']
        
        # è‡ªå®šä¹‰å˜æ¢é€»è¾‘
        transformed_image = self.apply_transform(image)
        transformed_target = self.apply_transform(target)
        
        return {
            'image': transformed_image,
            'target': transformed_target,
            'metadata': data['metadata']
        }
    
    def apply_transform(self, tensor):
        # å®ç°å…·ä½“çš„å˜æ¢
        return tensor
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### æ•°æ®åŠ è½½ä¼˜åŒ–

```python
# ä¼˜åŒ–æ•°æ®åŠ è½½æ€§èƒ½
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=8,             # å¢åŠ å·¥ä½œè¿›ç¨‹
    pin_memory=True,           # GPUè®­ç»ƒæ—¶å¯ç”¨
    persistent_workers=True,   # ä¿æŒå·¥ä½œè¿›ç¨‹
    prefetch_factor=4          # å¢åŠ é¢„å–
)
```

### å†…å­˜ä¼˜åŒ–

```python
# å†…å­˜é«˜æ•ˆçš„æ•°æ®å¤„ç†
dataset = MultiChannelSMLMDataset(
    data_path='data/large_dataset.h5',
    lazy_loading=True,         # å»¶è¿ŸåŠ è½½
    memory_map=True,           # å†…å­˜æ˜ å°„
    chunk_size=1000            # åˆ†å—å¤„ç†
)
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: æ•°æ®åŠ è½½é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ
A: ä¼˜åŒ–å»ºè®®ï¼š
- å¢åŠ `num_workers`
- å¯ç”¨`pin_memory`
- ä½¿ç”¨SSDå­˜å‚¨
- å¯ç”¨æ•°æ®ç¼“å­˜
- ä¼˜åŒ–HDF5æ–‡ä»¶ç»“æ„

### Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: è§£å†³æ–¹æ¡ˆï¼š
- å‡å°‘`batch_size`
- å¯ç”¨`lazy_loading`
- ä½¿ç”¨å†…å­˜æ˜ å°„
- å‡å°‘ç¼“å­˜å¤§å°
- ä½¿ç”¨æ•°æ®æµå¼å¤„ç†

### Q: å¦‚ä½•å¤„ç†ä¸å¹³è¡¡æ•°æ®ï¼Ÿ
A: ç­–ç•¥ï¼š
- ä½¿ç”¨åŠ æƒé‡‡æ ·
- æ•°æ®å¢å¼º
- æŸå¤±å‡½æ•°åŠ æƒ
- åˆ†å±‚é‡‡æ ·

### Q: æ•°æ®å˜æ¢åç›®æ ‡ä¸åŒ¹é…ï¼Ÿ
A: æ£€æŸ¥ï¼š
- å˜æ¢æ˜¯å¦åŒæ—¶åº”ç”¨äºå›¾åƒå’Œç›®æ ‡
- åæ ‡å˜æ¢æ˜¯å¦æ­£ç¡®
- æ’å€¼æ–¹æ³•æ˜¯å¦åˆé€‚
- è¾¹ç•Œå¤„ç†æ˜¯å¦æ­£ç¡®

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [æ¨¡å‹æ–‡æ¡£](../models/README.md)
- [è®­ç»ƒæ¨¡å—æ–‡æ¡£](../training/README.md)
- [æ¨ç†æ¨¡å—æ–‡æ¡£](../inference/README.md)
- [å¤šé€šé“è®­ç»ƒæŒ‡å—](../README_MultiChannel.md)
- [æ•°æ®æ ¼å¼è§„èŒƒ](./DATA_FORMAT.md)